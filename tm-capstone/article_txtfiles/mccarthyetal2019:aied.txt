Checking It Twice: Does Adding Spelling and Grammar Checkers Improve Essay Quality in an Automated Writing Tutor?. 

Abstract. This study investigated the effect of incorporating spelling and grammar checking tools within an automated writing tutoring system, Writing Pal. High school students (n = 119) wrote and revised six persuasive essays. After initial drafts, all students received formative feedback about writing strategies. Half of the participants were also given access to spelling and grammar checking tools during the writing and revision periods. Linear mixed effects models revealed that essay quality for students in both conditions improved from initial draft to revision in terms of all aspects except essay unity. The availability of spelling and grammar checking yielded added improvements from initial draft to revision for several aspects of essay quality (i.e., conclusion, organization, voice, grammar/mechanics, and word choice), but other aspects were unaffected (i.e., introduction, body, unity, and sentence structure). The availability of spelling and grammar checking tools had no effect on holistic essay scores. These results indicate that automated spelling and grammar feedback contribute to modest, incremental improvements in writing quality that may complement automated strategy feedback. 
Keywords: automated writing evaluation; feedback; natural language processing; spelling and grammar checking; writing strategies

1	Introduction 
National standards report that a majority of students are not proficient writers [1]. One challenge is that the development of writing skills requires repeated, deliberate practice with formative feedback [2], yet providing opportunities for students to compose essays and receive extensive feedback is time-consuming for instructors. Although the U.S. Department of Education recommends at least one hour of writing each day, writing instruction is often overlooked compared to other skills [3,4]. However, sophisticated natural language processing (NLP) tools have made it possible for automated writing evaluation systems (AWEs) to partially address these challenges. AWEs can provide (a) rapid and accurate evaluations and (b) formative feedback that are otherwise unfeasible in classroom settings constrained by time and resources. Students who receive AWE support have also demonstrated improved writing skills [59].
	As one example, the Writing Pal (WPal [1013]) is an AWE and intelligent tutoring system (ITS) that targets persuasive writing. Learners are typically given 25 minutes to write an essay in response to an SAT-like persuasive writing prompt. WPal employs NLP-driven algorithms to evaluate the essays [14]. WPal algorithms deliver both summative feedback (i.e., a holistic score on a 6point scale) and formative feedback about writing strategies and improving essay quality. This strategy feedback is related to eight different aspects of writing: freewriting, planning, introduction, body, conclusion, unity, paraphrasing, and revision (see [13]). WPal lessons and practice improve students’ writing quality and strategy knowledge, and both students and teachers find WPal instruction to be valuable and informative [13,15,16].

1.1	 What about Spelling and Grammar?
The development of technologies that deliver valid automated formative feedback has required innovations in NLP, modeling, and artificial intelligence. Nonetheless, a common question from educators is “Can it check my students’ spelling and grammar?” Research suggests that classroom teachers devote extensive time and attention to grammar and mechanics, often at the expense of other writing instruction [17,18]. Although spelling and grammar checkers have proven useful in ITSs for second language learning (e.g., [19]), the benefits of these tools for writing composition may be questionable. Research suggests that spelling and grammar instruction has little effect on writing quality after early elementary school [20], and spelling and grammar errors are only weakly correlated to expert judgments of essay quality [21]. More effective instruction focuses on writing strategies, such as drafting, organizing, and revising in combination with individualized feedback to improve these strategies (see [22]). Indeed, strategy instruction interventions have yielded large effect sizes [23].
Although spelling and grammar instruction alone may be insufficient for improving students’ overall writing skill, this type of feedback may still be valuable. Struggling with spelling can exhaust cognitive resources that would be better put toward other aspects of the writing process [24,25]. Moreover, in contrast to writing experts, nonexpert readers may base their evaluations of quality on spelling and grammar [2628]. For instance, nonexperts use spelling and grammar mistakes to make judgments about a writers’ intelligence and other personality traits [29,30]. Johnson and colleagues [30] asked college students to read writing samples that contained no errors, low-level errors (spelling and grammar mistakes), high-level errors (structural and conceptual errors), or both types of errors. Participants rated the quality of the essays as well as their perceptions of the authors. Low-level errors resulted in more harsh judgments, both in terms of the essay quality and negative personality traits to the writers. In other words, spelling and grammar are still important considerations for writing quality, assessment, and audience perceptions.
1.2	The Current Study
The current study examines the effect of incorporating spelling and grammar checking (SGC) tools into an automated writing evaluation and tutoring system for writing (i.e., WPal). WPal already provides strategy training and formative feedback for more complex aspects of the writing process (e.g., planning, elaboration, and unity). Thus, this study poses the research question: In an AWE system, what is the added value of spelling and grammar feedback when combined with higher-level strategy feedback? In this experiment, all students write essays, receive strategy feedback, and then revise their essays. However, half of the students are also given access to (and reminders to use) spelling and grammar checking tools while writing and revising. 
 Spelling and grammar feedback could benefit students’ overall essay quality by helping them focus on content and structure based writing and revision [24,25]. Alternatively, spelling and grammar information may have no effect on performance. This prediction is driven by the research that indicates essay quality is a function of deeper aspects of the content and style (e.g., [21]). Finally, spelling and grammar feedback could reduce essay quality. This outcome might arise because too much prompting and feedback can be distracting rather than helpful (e.g., [31]). Students also often default to “unproductive” superficial revisions [32,33], and the inclusion of spelling and grammar feedback may misdirect students away from substantive revisions. 
2	Method
2.1	Participants
High school students (n = 143) were recruited from a large metropolitan area in the southwestern United States and received financial compensation for their participation. Twenty-four participants were omitted due to incomplete data as a result of either technical or experimenter errors, resulting in a total of 119 participants.
One student completed the study but did not provide demographic information. Thus, the sample (Mage = 17.19, SD = 1.28, Range: 1319) was 61.3% female and 37.5% male; 53.8% Caucasian, 21% Hispanic (Latin American), 10.1% Asian, 7.6% African American, and 6.7% reported as other. Finally, 85.7% of participants were native English speakers.

2.2	Design and Procedure
Students were randomly assigned one of two conditions in which they received either writing strategy feedback (Strategy Condition, n = 60) or writing strategy feedback along with spelling and grammar checking tools (Strategy + SGC Condition, n = 59). 
	The study included four sessions. In the first session, students provided demographic information and then completed the Gates-MacGinitie Reading Test (GMRT; [33]) as a measure of reading skill. In each of the subsequent three sessions, the students wrote and revised two essays on different persuasive writing prompts (Table 1). Each of the prompts had a brief introduction to frame the issue and ended in a question. For example, the prompt about images and impressions read:

All around us appearances are mistaken for reality. Clever advertisements create 
favorable impressions but say little or nothing about the products they promote. In stores, colorful packages are often better than their contents. In the media, how certain entertainers, politicians, and other public figures appear is sometimes considered more important than their abilities. All too often, what we think we see becomes far more important than what really is. Do images and impressions have a positive or negative effect on people?

Participants were allotted 25 minutes to write an initial draft. After this time elapsed, WPal assigned a holistic score from “Poor” (1) to “Great” (6) along with individual strategy feedback aligned with WPal strategy lessons (e.g., planning, paragraph quality, unity, and paraphrasing). After viewing this feedback, participants were given 10 minutes to revise. Feedback messages provide actionable steps to help the participant improve their essay. For example, if the algorithm determined that the essay was too short, the participant might receive the following strategy feedback message:

This essay may not have enough paragraphs to fully support the main argument. If you need help developing support for future drafts or essays, it may be helpful to freewrite.
•	Write down possible arguments that may relate to your thesis
•	Brainstorm as many relevant facts and examples as you can
•	Try to think of details from school classes, news stories, and your own life that may relate to the arguments!
For participants in the Strategy + SGC Condition, “Check Spelling” and “Check Grammar” buttons appeared at the bottom of the interface during the writing and revision periods (Figure 1). Participants could access either function at any time while writing or revising. Errors were detected using the open source API LanguageTool [35,36]. When these tools were selected, errors were underlined similar to common word processors. Clicking on the error opened a small popup window with potential corrections. A reminder about the SGC tools appeared when there were 5 minutes remaining in the writing session, but students were not forced to use the tools.
2.3	Writing Assessment and Scoring
In the study, WPal algorithms determined immediate scores and feedback. For later analyses, human raters (following the same criteria underlying the algorithm) assigned holistic scores and nine subscores (i.e., specific writing traits) on both drafts. Raters were four graduate students of English. Rater pairs were trained to a high level of reliability (all kappas > .80) on all metrics. Holistic scores (16 scale) were based on the standard SAT rubric and subscores (16 scale) aligned with WPal lessons: 


•	Grammar, Style, & Mechanics: essay conveys strong control of the standard conventions of writing; avoiding errors in grammar, syntax, and mechanics
•	Introduction: writer demonstrates mastery in meeting the goals of an introduction (e.g., presenting a topic, providing a purpose, clearly stating a thesis, and previewing arguments
•	Body: writer demonstrates mastery in meeting the goals of body arguments (e.g., transition between arguments, using topic sentences, supporting arguments with evidence, and maintaining a flow throughout the arguments) 
•	Conclusion: writer demonstrates mastery in meeting the goals of a conclusion (e.g., summarizing the essay, reestablishing the significance of discussion, capturing the reader’s attention, and effectively closing the essay)
•	Organization: essay follows a logical structure (e.g., introduction, body arguments and evidence, conclusion)
•	Unity: details support the thesis and do not stray from the prompt and the main ideas and organizing principles presented in the introduction
•	Voice: writer is expressive, engaging, and sincere; a strong sense of audience
•	Word Choice: writer is precise and effective in word choice
•	Sentence Structure: sentence patterns are varied effectively, enhancing the quality of the essay
Note that holistic scores was not calculated from the subscales (e.g., an average)—holistic scores and subscores were distinct judgments although likely to be correlated.
Each essay (N = 1435) was scored by two raters. Across raters, the reliability of ratings for scores ranged from ICC = .79 to .90. The final scores for each essay reflect the average score of the two raters.
3	Results
Table 2 presents average scores as a function of draft and feedback condition. Overall, participants exhibited modest yet significant increases in average essay scores from initial draft (M = 3.63, SD = .55) to revision (M = 3.76, SD = .53), F(1, 117) = 25.44, p < .000, η2 = .18. Holistic writing scores were strongly correlated with all writing subscores (r = .73 to .93), supporting the concurrent validity of the assessment. Consistent with existing research [37], reading skill (i.e., GMRT scores) was also positively correlated with holistic score and subscores (r = .47 to .68). 
Table 2. Means and standard deviations of essay scores as a function of experimental condition and essay draft
3.1	Analysis of Essay Improvement and Condition 
Linear mixed effects (LME) models were conducted to detect the effects of the spelling and grammar checker tools on both initial draft and revision while also accounting for potential influences of the essay prompt or reading skill. 
	Holistic writing score and all nine writing subscores were fit with the same series of LME models using the lme4 package [38] in R [39]. Simple slopes were estimated using the reghelper package [40]. Table 3 shows the variables entered into each model. For the Baseline model (M0), only GMRT scores and a prompt factor were included. Second, a Draft model (M1) included a draft factor (i.e., initial vs. revised draft) to investigate how scores changes as a function of revising. Finally, a Condition model (M2) added a condition factor (i.e., Strategy vs. Strategy + SG) along with two interaction terms to assess the effect of condition. The interaction terms included “Draft × Condition” and “GMRT × Condition.” Likelihood ratio tests were used to compare model fit. Significant chi-square (χ2) tests indicate that adding the additional variable(s) improved fit as compared to the previous model (Table 4). These analyses revealed mixed results for the added value of SGC tools in an AWE. Benefits were observed for several subscores but not holistic quality.
3.2	Effects on Holistic Writing Quality and Grammar, Style, and Mechanics
One overarching question was whether the accessibility of SGC tools influenced overall writing quality. The LME models suggest that this was not the case. With regards to Holistic Score, the Draft model demonstrated improved model fit compared to the Baseline model. However, the Condition model did not further improve model fit. Although students improved their essays through revising, the availability of spelling and grammar checking tools did not appear to significantly contribute to holistic gains.
	If students receive valid and useful feedback and writing mechanics, these benefits should influence Grammar, Style, and Mechanics subscores, even if holistic quality was not affected. Indeed, this was the observed pattern. The Draft model exhibited improved model fit compared to Baseline. Further, the Condition model demonstrated further improved model fit. Simple slopes revealed that Strategy + SGC Condition participants improved from initial draft to revision, Estimate = 0.26, SE = 0.04, t(1283) = 4.86, p < 0.001, whereas participants in the Strategy Condition did not, Estimate = 0.02, SE = 0.04, t(1283) = 0.50, p = 0.62. The availability of spelling and grammar feedback had little effect on initial drafts, but facilitated revising with respect to grammar, style, and mechanics, as should be expected. In broader terms, spelling and grammar tools contributed to incremental improvements in writing quality that were not necessarily reflected in holistic essay quality.

3.3	Additional Benefits of Spelling and Grammar Feedback
Several other subscores provided evidence that the availability of spelling and grammar tools facilitated incremental gains in specific aspects of writing. The quality of Conclusion, Organization, Voice, and Word Choice significantly improved from the initial draft to the revision for students in the Strategy + SGC Condition, whereas there was no improvement for students in the Strategy Condition. These findings are reflected within the LME by improved model fit compared to baseline in the Draft model, and improved fit of the Condition model, specifically driven by the Draft × Condition interaction term. Simple slopes—with draft as the focal predictor and condition as the moderator—revealed that participants who had access to spelling and grammar tools tended to increase in subscores from draft to revision (Conclusion Estimate = 0.38 SE = 0.06, t(1283) = 6.07, p < 0.001; Organization Estimate = 0.23, SE = 0.05, t(1283) = 4.86, p < 0.001; Voice Estimate = 0.29, SE = 0.05, t(1283) = 5.97, p < 0.001; Word Choice Estimate = 0.22, SE = 0.04, t(1283) = 5.54, p < 0.001), whereas there was no observed improvements for those in the Strategy Condition (all Estimates < .09; t < 2).

3.4	Writing Subscores Unaffected by Spelling and Grammar Feedback
As hinted by the lack of effects on holistic writing quality, the benefits of SGC tools were not universal—several writing subscores showed no effect of feedback condition. These included Introduction, Body, and Sentence Structure. Participants improved on these traits from initial to revised draft (i.e., revising improved the essay) but condition (i.e., adding spelling and grammar feedback) had no influence (see Table 4). 
	Finally, contrary to other subscores, participants did not show any change in Essay Unity scores across drafts; the Draft model did not improve model fit compared to Baseline. The Condition model also did not improve model fit.
4	Discussion and Future Work
Computer based educational tools can provide a wealth of feedback to student writers, including summative scores on writing quality and individualized, formative feedback on strategies and ways to improve [41]. Similar to word processing programs, these tools might also incorporate spelling and grammar checking. However, one question is whether having access to SGC tools benefits students above and beyond strategy feedback. Prior research suggests that an overemphasis on writing mechanics can be useless or detrimental [21], whereas strategy instruction and feedback are beneficial [22,23]. 
	To explore these questions, the current study examined how the inclusion of spelling and grammar feedback in an AWE system affected the quality of essays and revisions. This study focused on high school students, a critical target population for writing instruction and intervention. Analyses examined gains in holistic essay scores and nine subscores (rated by writing experts) as a function of feedback condition.

4.1	Incremental Improvements in Essay Quality
Linear mixed effect models indicated that essay quality improved holistically and along all subscores (except unity) as a function of revising with feedback. When students wrote essays in WPal, received feedback, and revised, their essays improved. This finding replicates previous results showing that strategy feedback results in improvements from initial draft to the revision [4245].
	As one might expect, the availability of checking tools improved the grammar and mechanics in the essays. However, spelling and grammar feedback, in conjunction with strategy feedback, also improved essays from initial draft to revision on the dimensions of conclusion paragraphs, organization, voice, and word choice. This finding is important because it suggests that providing students with tools to check their spelling and grammar might (a) free up resources to consider other aspects of writing when writing and/or (b) inspire a greater willingness to revise. Grammar checkers have been shown to increase students’ motivation and confidence in their writing [46,47]. Thus, spelling and grammar checkers may have direct benefits on mechanics that then afford indirect benefits elsewhere. Future research should consider students’ subjective reactions to these tools, such as whether they indeed perceive writing to be less burdensome or more engaging when they have a “safety net” of checking tools in an AWE system.
	Importantly, these tools did not appear to benefit holistic essay scores. This is consistent with prior work showing that expert evaluations of essay quality rely on deeper features of text [21]. Likewise, there were no apparent benefits of spelling and grammar feedback on introduction quality, body quality, unity, or sentence structure. Thus, although the tools were moderately useful, they were not universally beneficial. 
	Finally, it is worth noting that this feedback did not reduce performance on any subscores. Overall, these findings indicate that adding spelling and grammar checkers in conjunction with strategy feedback is moderately beneficial for AWEs.

4.2	 Directions for Future Research
In the current study, participants did not improve the unity of their essays, regardless of feedback or checking tools. One explanation may be that unity requires the writer to step back and evaluate the connectivity, or cohesion, of the essay as a whole. A 10minute window for revision may not provide sufficient time to conceptualize and implement such revisions. An alternative explanation could be that participants received fewer messages about essay unity than other feedback topics. Further analyses of the types of feedback that participants received, and their viewing of that feedback, may shed further light on how we might help students to improve the unity of their essays.
	In addition to assessing the specific feedback messages received and viewed, future work might also analyze how students approached the revising task (i.e., the frequency and type of edits made). One possibility is that students in the strategy feedback condition used their 10 minutes of revision time to address mechanical errors—a typical bias toward proofreading over substantive revising. In contrast, because students in the spelling and grammar condition had already completed this task (at least partially), they spent more time during revising on substantive edits. Given that spelling and grammar feedback did not impact the subscores equally, these findings may shed light on which aspects of writing, or which writing strategies, are prioritized by students. Eliciting students’ self reported rationale for revising or using AWE tools could help to understand how they navigate AWE functions and uptake feedback. For instance, students might focus on the “easiest” or “fastest” edits, or they may prioritize the most “critical” flaws.
	Finally, having these tools available might change the dynamics of the writing process [48,49]—students may write more or faster when they feel they can rely on SGC tools to make the task easier. To explore these plausible changes in writing production, the use of logfiles, key strokes, and similar data may help to elucidate how students use the spelling and grammar functions, and the extent that tool use—as opposed to mere availability or accessibility—influences drafting and revising activities. The benefits of using spelling and grammar tools are likely to be more nuanced than a simple “more is better” assumption. Students might rely on the tools to mechanically and mindlessly fix typos instead of reflecting on the meaning of their writing or the possible reactions of their audience.

4.3	Conclusion
Questions about optimal feedback—including feedback content, timing, methods, and effects on performance—are among the most critical challenges facing educators, researchers, or others who develop and implement adaptive educational technologies. A number of nuanced factors influence feedback quality and students’ feedback uptake. 
This study explored feedback in the context of essay writing with AWE support. Research has previously established that formative feedback on writing strategies is effective whereas feedback that exclusively targets grammar and mechanics is not effective. Nonetheless, teachers, students, and writers intuitively crave feedback and automated corrections on these writing features. This study provides compelling evidence that students benefit from both types of feedback, and that guidance on writing mechanics does not inhibit writing quality or deter from revising. 
Several questions remain concerning the source, loci, and dynamics of these effects. Do benefits stem from helping students manage their resources and focus on rhetorical aspects of writing? Do benefits stem from a sense of writer empowerment? How do varying feedback tools influence real-time writing and revising behaviors? Ultimately, the objective is to enhance students’ ability to improve their writing, and automated feedback affords multiple resources for accomplishing that goal.
