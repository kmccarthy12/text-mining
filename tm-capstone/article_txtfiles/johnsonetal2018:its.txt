ADAPTIVE LITERACY INSTRUCTION IN ISTART AND W-PAL: IMPLEMENTING THE INNER AND OUTER LOOP

ABSTRACT

Though the affordances of intelligent tutoring systems (ITSs) are particularly useful for literacy instruction, developing accurate automated feedback in reading and writing present challenges for developers. This chapter outlines some of these challenges in the context of providing both inner-loop and outer-loop feedback (VanLehn, 2006) and describes how we have addressed these challenges in our systems, iSTART and W-Pal. We report on the theoretical and practical development of student and instructional models that are sensitive to students’ evolving literacy skills. The chapter offers an explanation of how we posit the implementation of outer-loop adaptivity can enhance learning of literacy skills in ITSs more broadly.

Keywords: inner and outer loop adaptivity, ill-defined domains, natural language processing, literacy instruction, pedagogical design

INTRODUCTION

Theories of skill acquisition suggest that students need extended, deliberate practice coupled with feedback in order to achieve mastery (e.g. Ericsson, Krampe, & Tesch-Römer, 1993; Healy et al., 1995). However, limited time and other resources in the classroom restrict teachers’ ability to provide these kinds of practice opportunities. Thus, intelligent tutoring systems (ITSs) provide a unique opportunity to give students immediate, individualized practice that may otherwise be impossible in the classroom. One domain that can greatly benefit from ITSs is literacy. Though reading and writing skills are essential, research indicates that less than half of adolescents have the literacy skills needed to meet the demands of school and the workplace (NCES, 2011; 2015). 
ITSs are not designed to replace literacy instruction, but instead supplement classroom instruction. As a result of classroom constraints, teachers often employ multiple-choice reading comprehension measures and cannot realistically provide extensive feedback on individuals’ writing compositions. It may take hours for a Language Arts teacher to read all twenty of her students’ essays and then compose suggestions for improvement for each student. The students would then need to revise their essays and the teacher would need to spend more time evaluating those revisions. An ITS that automatically evaluates an essay and provides feedback in a matter of seconds can speed up this process. As a result, students could have more opportunities for practice and the teacher would have more time to target instruction. However, providing accurate automated feedback for ill-defined domains such as reading and writing is far more complicated than for well-defined domains (e.g., physics, mathematics). Two examples of ITSs for literacy instruction are the Interactive Strategy Training for Active Reading and Thinking (iSTART), for reading comprehension instruction, and Writing Pal (W-Pal), for writing instruction (see adaptiveliteracy.com).
iSTART provides reading strategy instruction and opportunities for extended practice of the strategies. Specifically, iSTART provides instruction intended to improve students’ ability to self-explain texts, by applying reading comprehension strategies during the active process of self-explanation (McNamara, 2004). iSTART uses animated lesson videos that explain and demonstrate the self-explanation strategies and practice modules that provide immediate feedback on self-explanations using natural language processing (NLP). W-Pal similarly uses lesson videos and generative essay practice modules to develop students’ application of writing strategies related to eight stages of the writing process. During W-Pal practice, students receive immediate feedback on essay components and have the opportunity to revise essays on the basis of this feedback.
Although the NLP assessment used in ITSs developed to improve literacy skills afford the potential to provide immediate feedback to develop literacy skills, adapting instruction and practice in systems for less structured topics can prove more difficult than systems designed for well-defined domains. This chapter describes the unique challenges that literacy instruction poses for ITS development. We discuss how our two ITSs, iSTART and Writing Pal, have addressed these challenges and then describe recent efforts to enhance adaptivity to better match students’ skills as they evolve. 

Intelligent Tutoring Systems - Instructional Components

ITSs often employ three elements to deduce a student’s capabilities and select appropriate tasks that can improve the students’ skills: the expert model, the student model, and the instructional model (Shute & Psotka, 1996; VanLehn, 2006; VanLehn, Polson & Richardson, 1988; Woolf, 2010). The expert model represents the ITSs’ “knowledge” of the domain and is created using detailed analyses of the knowledge reported from subject matter experts (SMEs). The student model represents an ever-evolving estimation of students’ understanding of the subject matter. This representation of student knowledge is constructed by examining student task performance in comparison to the complex expert model, and by examining student responses in relation to pre-defined misconceptions (or ‘bugs’). This process of updating the student model using performance indicators and the expert model is referred to as student diagnosis. An ITS uses the instructional model to select which instructional content or task is selected at any given time given inferences about student knowledge and skills. A student model is often constructed using overlay models that represent a subset of an expert model, indicating the differences between novice and expert knowledge and skills (Woolf, 2010). Selection of subsequent instruction and practice tasks uses the overlay of student model and expert model, in conjunction with the instructional model that is grounded on educational theory and empirical results. 
Described in more detail later, iSTART and W-Pal use video lessons, guided demonstrations, and game-based practice to deliver strategy training for reading comprehension and persuasive writing, respectively. NLP algorithms evaluate a wide variety of linguistic, semantic, and rhetorical indices to assess the quality of responses (Graesser, Chipman, Haynes, & Olney, 2005; Jackson & McNamara, 2013; McNamara, Boonthum, Levinstein, & Millis, 2007; McNamara, Crossley, Roscoe, Allen, & Dai, 2015). iSTART’s self-explanation scoring algorithm produces 95% adjacent accuracy (i.e., compared scores differ by one level or less) compared to human-scored self-explanations (McNamara et al., 2007). Similarly, W-Pal scores of writing quality are highly accurate in relation to human-scored essays (Crossley, Dempsey, & McNamara, 2012). Further, these NLP algorithms have the power to assess different elements of student responses, not solely producing a numerical score, but allowing immediate elaborated feedback on different components of the composition. Consequently, students receive formative comments they can use to improve their responses and, by extension, their literacy skills. For example, if a student’s self-explanation is too similar in content to the sentence being explained (referred to as the target sentence), iSTART will provide feedback instructing the student to try to go beyond paraphrasing the target sentence. Thus, through the utilization of NLP, we have been able to evaluate students’ open-ended responses and provide feedback that helps them to improve their literacy skills.

Considering Inner vs. Outer Loop Feedback

While iSTART and W-Pal have shown significant learning gains (Allen, Crossley, Snow, & McNamara, 2014; Jackson, Boonthum, & McNamara, 2009; McNamara, 2004; McNamara, Levinstein, & Boonthum, 2004; Roscoe, Brandon, Snow, & McNamara, 2013; Snow, Jacovina, Jackson, & McNamara, 2016), the team continues to consider ways to improve instruction. One feature of other ITSs that has yielded increased efficacy is the implementation of outer-loop adaptivity. The distinction between inner-loop and outer-loop adaptivity is that inner-loop feedback refers to the immediate feedback students are given when they complete of an individual task. In contrast, outer-loop adaptivity refers to the selection of subsequent tasks based on students’ past performance (VanLehn, 2006). Using various techniques to update the student model (e.g., constraint-based models and Bayesian belief networks), systems that employ outer-loop adaptivity demonstrate notable learning effects (d = ~0.75; Van Lehn, 2011). 
Previous iterations of the systems provided inner-loop feedback in the form of summative scores and actionable feedback, but the systems have not implemented adaptivity to the students’ developing skills throughout learning sessions. Specifically, the selection of succeeding texts for practice, instructional lessons, or practice modules has not been based on an evolving model of student performance. In the following sections, we outline the systems and then describe our recent work in incorporating a greater degree of individualized instruction through the implementation of outer-loop adaptivity.

The outer loop for ill-defined domains

As described previously, the outer loop of adaptive instruction in ITSs traditionally requires an intelligent student model, which is typically deduced using student performance data in comparison to a pre-defined expert model. Selection of subsequent instruction and practice tasks uses the overlay of student model and expert model, in conjunction with the instructional model grounded on educational theory and empirical results. Within ITSs developed to improve literacy skills, construction of an ‘expert model’ and ‘student model’ of performance (as they are traditionally conceived in ITSs) is challenging due to a few factors. First, subject matter experts often cannot produce a full representation of the procedures underlying the domain because, unlike in mathematical domains, one approach within the same reading or writing scenario might be just as appropriate as another (Crossley, Roscoe, & McNamara, 2013). Second, but related to the first, unlike well-defined domains that use correct and incorrect answers, discourse analyses of students’ composition are not precise or entirely objective, and thus the diagnosis of student knowledge is less precise. Third, available NLP analytic techniques to assess rhetorical aspects of composition are just now emerging in the literature, and hence there has been little success in identifying specific argumentative structures within writing. For these reasons, the development of an intelligent student model in literacy ITSs poses unique challenges. 
Our ITSs estimate students’ evolving literacy skills using NLP assessment of writing quality and self-explanations. Within these systems, a wide variety of linguistic, semantic, and rhetorical indices assess writing compositions to produce scores on essay components (e.g., introduction, conclusion). These scores are highly accurate in relation to human-scored essays (Crossley & McNamara, 2012). The algorithm used to evaluate students’ self-explanations produces 95% adjacent accuracy compared to human-scored self-explanations. Within some modules, students’ self-explanation scores are combined with scores on objective multiple-choice comprehension measures to estimate reading comprehension skills. In conjunction with the systems’ instructional models, the student models based on NLP indices direct the adaptation of subsequent instruction. For example, when W-Pal’s assessment indicates an introduction paragraph is deficient, the system infers that the student requires further instruction in strategies for writing introductions. As a result of this inference regarding the students’ writing skill, the system delivers training videos and practice opportunities to develop introduction-writing skills. The following sections describe iSTART and W-Pal in more detail.

ISTART
Foundations of iSTART

The Interactive Strategy Training for Active Reading and Thinking (iSTART) is a web-based ITS created to improve adolescent students’ comprehension of the kinds of complex scientific texts they read in school. Because difficult textbooks often include information that is unfamiliar to the students, they often struggle to understand the material. Theoretically grounded in constructivist theories of comprehension, the instruction in iSTART emphasizes the importance of active use of knowledge (McNamara, Levinstein, & Boonthum, 2004). 
The primary comprehension theory underlying iSTART is the construction-integration model (CI model; Kintsch, 1988; 1998). The CI model suggests that comprehension relies on the construction of a durable, elaborated mental representation of the text. The model describes two primary levels of mental representations of text: the textbase and situation. The textbase is a representation that captures the explicit information in the text, while the situation model includes inferences within the text and between the text and prior knowledge (Kintsch, 1988; 1998). Critical to the construction of an elaborated situation model is inferencing. Inferencing is the process of connecting information in an environment (e.g., current sentence or text) to information that is not in the current environment (e.g., previous concepts in the text, general world knowledge, knowledge for specific domains). There are a variety of inferences that readers construct as they read (see O’Brien, Cook, & Lorch, 2015). Two types of inferences specifically relevant for iSTART are bridging and elaborative inferences. A connection between the information one is reading and previous information in the text is called a bridging inference. When readers connect the information currently being read to knowledge they possess that is not in the text, it is called a knowledge-based, or elaborative inference. Both bridging and knowledge-based inferencing allow readers to create a more comprehensive mental representation of the text.
Unfortunately, readers without sufficient prior knowledge often fail to create inferences, which fill in the conceptual gaps within the texts (McNamara & Kintsch, 1996; McNamara, Kintsch, Songer, & Kintsch, 1996). This is particularly problematic in expository texts, like science textbooks, because they tend to include information that is unfamiliar to the novice reader (Beck, McKeown, & Gromoll, 1989). One effective method for improving comprehension of science texts is through self-explanation. Self-explaining, or explaining text to oneself, improves comprehension by encouraging students to engage in both metacognition and inference generation (Chi, Bassok, Lewis, Reimann, & Glaser, 1989; Chi, de Leeuw, Chiu, & LaVancher, 1994). However, some readers do not naturally self-explain texts or self-explain poorly when prompted (Chi & Bassok, 1989). Therefore, self-explanation reading training (SERT) was developed to improve readers’ ability to self-explain texts through a combination of self-explanation and metacognitive reading strategies (McNamara & Scott, 1999; McNamara, 2004). iSTART, is the automated version of this in-person intervention.
The underlying assumption of both SERT and iSTART is that training readers to use more effective reading strategies will improve their ability to self-explain, which, in turn, improves their comprehension of texts (McNamara, 2004; McNamara, Levinstein, & Boonthum, 2004). SERT and iSTART provide instruction and practice for five empirically-validated reading strategies: comprehension monitoring, paraphrasing, prediction, bridging, and elaboration. Comprehension monitoring is a focus on how well one understands while reading. It enables the reader to identify when they are having trouble in understanding a text, which prompts them to use an active reading strategy to improve their comprehension. Paraphrasing is a restatement of the text in the reader’s own words. While not a self-explanation, it provides a check to ensure the reader understands the vocabulary and syntax of a text. When unfamiliar information is present, this can cue the reader to use the context or prior knowledge to deduce the meaning in the text. Prediction is when a reader anticipates forthcoming information in a text either by making educated guesses or taking note of information that, if present, will aid in comprehension of a previous concept. Bridging is the act of drawing a connection between the current sentence to previous information in the text. Elaboration is expanding on concepts in the text using prior knowledge of the domain from sources outside of the text, general world knowledge, or logic. In the following sections, we describe iSTART, its features, and how it provides instruction and practice opportunities. The subsequent section describes the integration of outer-loop adaptivity in iSTART. 
iSTART 

iSTART provides instruction and practice in self-explanation strategies in three primary phases: training, initial practice, and extended practice. During the training phase, iSTART uses animated lesson videos to provide instruction in reading comprehension strategies. The system presents a total of eight animated lesson videos: overview, comprehension monitoring, paraphrasing, prediction, bridging, elaboration, recap, and demonstration. Each video is approximately 5 minutes long, and the individual strategy videos are each followed by four learning check questions. iSTART classrooms can be set up to require a minimum score on the learning check questions to progress to later stages of instruction and practice. Completing the entire sequence of videos takes a little over half an hour. The videos define the strategies and provide examples. Videos are narrated using a human voice that provides the definitions and reads the examples from the presented slides. A screenshot of one of the lesson videos is provided in Figure 1. 

iSTART has been shown to improve self-explanation quality and reading comprehension for students from middle school through college and is particularly beneficial for low-knowledge readers (Jackson, Boonthum, & McNamara, 2009; McNamara, 2004; McNamara, Levinstein, & Boonthum, 2004;  Snow et al., 2016).

Self-Explanation Practice in iSTART

Upon completion of the self-explanation lesson videos, students are guided to the initial practice phase, Coached Practice (see Figure 2). Coached Practice is a non-game based module in which students read a text and type self-explanations for certain target sentences. Their self-explanations are analyzed using NLP algorithms that identify the strategies used in the self-explanation, and provide a holistic score for the quality of the self-explanation on a scale of 0-3 (0 = poor, 3 = great). If the student’s self-explanation score is less than 2, the system will also select and provide actionable feedback and allow students to revise their self-explanation. The type of feedback delivered is based on various linguistic and semantic features of the self-explanations (McNamara et al., 2007). For example, LSA overlap score determines the amount of content overlap between the text being explained and self-explanation. If the LSA overlap score indicates inadequate content overlap, feedback is provided suggesting that the student attempt to explain the information from the text. 
Within traditional ITSs built for well-defined domains, the inner loop provides feedback on students’ attempts for solution steps within a presented problem. In iSTART, the ‘problem’ can be considered understanding the text, and the ‘steps’ to understanding correspond to the individual self-explanations. Thus, the scores and formative feedback provided on students’ individual self-explanations represent the inner-loop adaptivity in iSTART (VanLehn, 2006). This immediate, individualized feedback is essential to developing students’ skills in the comprehension strategies (Ericsson et al., 1993; Healy et al., 1994). To further promote skill acquisition, we have complemented this inner-loop with outer-loop adaptations (described in detail later), which select tasks (i.e., practice texts and level of scaffolding for those texts) based on the estimations of students’ reading skills, given their performance. 
Once students have completed the initial round of Coached Practice, they continue to extended practice. Students can elect to revisit Coached Practice or they can play self-explanation games. There are two types of game-based practice. The first includes generative practice games (Map Conquest and Showdown). In generative games, students produce their own self-explanations, and based on self-explanation score, they earn points, system currency, and trophies. Points earned in the games can be used to play embedded games and will also increase students’ iSTART level. As an example of an embedded game element, in Map Conquest (see Figure 3), students earn flags from high quality self-explanations that can then be used in the game play to conquer territories on a map. When students reach higher iSTART levels, the system can be set to unlock new games and new avatar customization elements (see Figure 4). Earned system currency (iBucks), also earned by producing strong SE scores, can be used to purchase customization features for students’ iSTART avatars; for example, students can purchase new styles of hats or eye-glasses and sunglasses with iBucks (see Figure 5). Both the students’ customized avatar and their earned trophies can be displayed to classmates in the (anonymous) leaderboard and trophy case. All of these ‘metagame’ elements were designed to promote student motivation (Jackson & McNamara, 2013). 
The second type of game-based practice are identification games. In these games, students read example self-explanations of a text and then identify which reading strategies were used in the examples. This game-based practice enhances enjoyment and motivation, which promotes persistence in the system (Jackson & McNamara, 2013; 2017).
Outer-Loop Adaptivity in iSTART

In contrast to the inner-loop adaptivity described earlier, outer-loop adaptivity operates at the task level (VanLehn, 2006). Specifically, the outer loop uses estimations of students’ skills and the instructional model to select which task is appropriate for the individual student at any given time in the instruction. As described earlier, in iSTART, the task corresponds to reading and attempting to understand one text within the various available generative practice activities. Features of each presented task (i.e., the difficulty of the text and scaffolds to support comprehension) can be adapted using the evolving student model and the instructional model. In iSTART, the student model is created from students’ self-explanation scores and scores on multiple-choice measures. Generally speaking, the instructional model dictates that when an individual’s scores are low, features of the subsequent task should support student comprehension, and when scores are high, such supportive features should be removed (Collins, Brown, & Newman, 1988; Puntambekar & Hubscher, 2005).  
In previous versions of iSTART, texts were selected by the experimenter or instructor and the entire iSTART ‘classroom’ received the same texts in the same order. While this allows for uniformity of treatment, it ignores the reading skill of individual students. Take for example two students in the same class reading the same text: one is a highly skilled reader who is well above grade level and the other is a struggling reader reading below grade level. Drawing on theories of proximal development (Vygotsky, 1978) and desirable difficulty (Bjork, 1994), the difficulty of practice texts used by individuals in strategy practice should adapt to the estimation of individual’s reading skill. That is, more difficult texts would be more appropriate for students performing very well and less difficult texts for students that appear to be struggling to understand the texts.  Thus, the purpose of implementing outer-loop adaptivity in iSTART is to optimize student learning by approximating students’ current reading skill (e.g., the student model) based on past performance and adjusting text difficulty accordingly. 
 
Assigning Text Difficulty. The texts that were used in generative practice (coached practice, map conquest, and showdown) in earlier iterations of iSTART included only difficult expository science texts with low coherence (Flesch-Kincaid grade levels range: 6-14). These texts, similar to those found in textbooks, are best understood by readers who have either high prior domain knowledge or high reading skills to create coherent mental representations of the content (McNamara & Kintsch, 1996; Ozuru, Best, & McNamara, 2004). In order to afford greater variability in text difficulty levels used in adaptivity, we added 162 new expository texts from two websites offering publically-available texts. Consequently, iSTART includes a library of texts collected across various studies and situations over the last 15 years. Each text’s or sets of texts’ difficulty have been estimated using a variety of measures (see e.g., Crossley, Allen, & McNamara, 2011). And thus, one question that arose regarded which type of text difficulty measure to use, and in turn, which particular measure to use. In the end, the particular nature of the texts and target tasks led to the consensus that a first step was to generate human ratings. To this end, a group of four expert raters iteratively sorted the iSTART library texts until agreement was reached, placing each text into one of 14 difficulty levels with each difficulty level containing at least eight texts. The final ratings correlated strongly with Flesch-Kincaid grade level (r = .79) and Dale-Chall readability (r = .77), indicating the ratings were reliable with, but not redundant to these measures. Hence, an NLP algorithm was subsequently developed based on the human ratings to automate the text difficulty classification within iSTART (for more information on the development of this algorithm, see Balyan, McCarthy, & McNamara, accepted). The NLP algorithm was crucial so that researchers and instructors can add new texts which can be used in iSTART’s outer-loop adaptivity. 
Adaptive Flow. iSTART uses students’ average self-explanation score for a text to determine which text will be presented next. For example, after completing a round of generative practice where the text presented is at difficulty level 10, the student’s average self-explanation score is compared to a pre-determined threshold. The default self-explanation threshold value is currently set to 2.0 (out of 3.0), but the value of this threshold is a parameter that can be changed by the experimenter or instructor.  If the students’ average self-explanation score is below the threshold, the system will select the next text for practice from the immediately lower difficulty rank (level 9). If the student scores at or above the threshold, the next text selected for practice will be from the immediately higher difficulty rank (level 11). Critically, the purpose of this outer-loop adaptivity is to adjust to students’ abilities in order to provide just-in-time scaffolding, not to punish or reward students for their performance. As such, this adaptivity is not explicitly communicated to the student; they are not shown the difficulty ranking of the texts nor are they informed when text difficulty is changing. 
Outer-loop adaptivity is also applied in iSTART within a new practice module, StairStepper. StairStepper incorporates both generative self-explanation practice and multiple-choice recognition quizzes. The design of StairStepper was modeled on familiar computer adaptive standardized tests that deliver more difficult test items using previous item performance (e.g. the Graduate Record Examination). The StairStepper interface is depicted in Figure 6. In StairStepper, students read texts and answer multiple-choice questions designed to mimic the types of questions that students encounter in standardized reading assessments. These questions ask textbased questions (answers require information directly found in the text), bridging questions (answers require the students make inferences across several sentences), and elaboration questions (answers require students make inferences across the text and their prior knowledge) to assess comprehension.

Students’ performance on the multiple-choice items in StairStepper is used to estimate their reading skills and guides two types of adaptive instruction. In addition to the immediate feedback provided for each individual self-explanation or answer to multiple choice questions in this module, the adaptive instruction in StairStepper represents outer loop adaptivity in iSTART. First, multiple-choice performance on an entire text within the StairStepper module drives the level of self-explanation scaffolding provided to the student for each subsequent text. When multiple-choice performance is high for one text, self-explanation scaffolds are removed for the subsequent text; when the performance is low, self-explanation scaffolds are added for the next text. The first level of self-explanation scaffold is simply prompting the student to self-explain in a provided text entry box. If performance is still below the pre-set threshold, the next level of scaffolding for self-explanation is provided. At this level, feedback is provided on the student’s self-explanation, and the student is able to revise the explanation. Second, the MC performance is used to select text difficulty of subsequent texts. When multiple-choice performance is high, text difficulty increases and vice versa. The students’ goal when playing StairStepper is to reach the highest stair by successfully answering the multiple-choice comprehension questions. The flow of the game is depicted in Figure 7.
For each text displayed to the student, StairStepper scaffolds the use of self-explanation through three modes of increasing student independence: 1) self-explanation prompts with feedback, 2) self-explanation prompts without feedback, and 3) no self-explanation prompts. The underlying assumption of this scaffolding approach is that, if students are struggling to comprehend the texts without explicitly prompting self-explanations, the addition of SE prompts will potentially improve comprehension scores. Furthermore, if they are still struggling to understand without feedback on their self-explanation, the addition of formative feedback messages will potentially further enhance comprehension. A second underlying assumption of this design is that if the student can experience increased comprehension scores with the addition of self-explanation scaffolding, they may implicitly understand the value of the self-explanation activity and the application of the corresponding comprehension strategies. At the second level of adaptation, StairStepper adapts the difficulty of texts used in the game. Specifically, if the student is at the highest level of independence from scaffolding (i.e., no self-explanation prompts), and she receives a high multiple-choice score, the next text used in the practice game will be at the immediately higher difficulty level. Conversely, if the student is at the lowest level of independence (i.e., self-explanation prompts with feedback) and still receives a low multiple-choice score, the difficulty of the subsequent text will be at the immediately lower difficulty level. A third underlying assumption of StairStepper design is that students will exert more effort to score well on the multiple-choice questions when they experience declines in difficulty level.  
Students begin StairStepper reading a text at a pre-set difficulty level. The default difficulty level is 5 (out of 13); this starting point is another parameter that can be changed for each iSTART ‘classroom’. The student reads the text, then answers a series of multiple-choice questions with the text available to them for reference. This format of reading and answering multiple-choice questions, while the test is still available, mimics some standardized reading assessments. After completing the questions for this test, the students’ multiple-choice score is compared to the threshold for questions accuracy (Q score threshold). The default threshold is a comprehension question accuracy score of 75% correct; this threshold is another parameter that can be changed in each iSTART ‘classroom’. At this point, if the participant scores above the Q score threshold, the avatar in the interface progresses up the staircase and the next text used is selected from the immediately higher difficulty level.  On the other hand, if the student does not meet the threshold, they remain on the current stair step and read another text at the same difficulty level, with self-explanations prompted during reading. If the student still scores below the threshold on the text with self-explanation prompting, the subsequent text will prompt self-explanation and also provide formative feedback on the self-explanations produced. The adaptation of task selection in StairStepper is overt to the student through the advancement or decline of the avatar on the stairs.

W-PAL
Foundations of W-Pal

The Writing Pal (W-Pal) is a web-based ITS developed to improve adolescent students’ persuasive essay writing skills. Although writing is a crucial skill needed in almost all aspects of life, it is a particularly challenging activity for students (NCES, 2011). W-Pal is theoretically grounded in writing pedagogy and includes instruction that promotes knowledge and use of writing strategies (Roscoe & McNamara, 2013). 
According to the cognitive process model of writing provided by Flower and Hays (1981), the ill-defined nature of writing involves many complex tasks that constitute the writing process. Within this model, the writing process is divided into three primary cognitive processes: Planning, Translating, and Reviewing. These three processes are further sub-divided into sub-processes. For example, the planning process is comprised of generating ideas, organizing, and goal-setting. Furthermore, there are a wide variety of knowledge types that students need to efficiently manipulate in order to successfully write a text (Elliot & Klobucar, 2013). The process consists of different stages and each stage involves different tasks. Instruction and practice should target different strategies within each stage with the goal of providing students with (1) concrete knowledge of strategies, (2) background knowledge to use these strategies, and (3) opportunities for extended practice, all of which is emphasized by writing pedagogy (Graham & Perin, 2007). More specifically, according to the Framework for Success in Postsecondary Writing (FSPW), effective instruction on writing strategy should promote rhetorical knowledge (e.g., understanding audiences), critical thinking, knowledge of the writing process and conventions, and the ability to write for a variety of environments (Council of Writing Program Administrators, 2011). 
In order for students to become proficient writers, it is important to ensure that instruction adhere to the constructs of writing skills that are essential for students to grasp in order to become proficient. For example, when students become more proficient and gain knowledge in areas such as vocabulary and grammar or addressing different audiences, there is a higher likelihood to improve writing skill (Condon, 2013). Writing strategy instruction should also emphasize the components of the writing process. For example, the process of writing the first draft should follow the process of freewriting (i.e., a brainstorming strategy used to produce as much ideas as fast as possible; Elbow, 1973; Weston, Crossley, & McNamara, 2010). Instruction on strategies for the different components is beneficial to students because they provide concrete information to help manage the process in more efficient manners (Healy et al., 2012). Effective instruction adheres to these constructs and provides explicit instruction and ample opportunities for practice to achieve proficiency (Graham & Perin, 2007; Kellogg, 2008). 
In addition to instruction and practice, feedback is essential to improve writing skill. Feedback helps the student identify any weaknesses or deficiencies they may have in their knowledge about the writing process and allows for a more effective use of strategies (Shute, 2008). Feedback is especially important for the revision process. Typically, students make superficial edits, instead of deeper level changes to their writing when simply asked to revise (Fitzgerald, 1987; Crawford, Lloyd, & Knoth, 2008). Targeted, formative feedback allows students to correct errors and modify their written product at both surface and deeper levels. When students receive informative feedback about the quality of their writing and the issues they should consider, then students are more likely to make deeper level revisions. 
Developing writers may not have quite mastered the knowledge to become proficient. Feedback, deliberate practice, and targeted instruction are important for the development of writing skills. W-Pal was developed based on these ideas with the goal of teaching developing writers the strategies that can improve their writing ability. Specifically, W-Pal contains lessons and activities designed to enhance students’ knowledge of the stages of prewriting, drafting, and revision. The instruction targets specific strategies within each stage with the goal of effectively teaching how and why these strategies should be used. Strategies allow a writer to achieve a goal by employing effortful and purposeful procedures that facilitate the accomplishment of the task (Alexander, Graham, & Harris, 1998). What follows is a description of W-Pal, its features, and how it functions. This is followed by a discussion of the addition of the adaptive outer loop to the W-Pal system. 

W-Pal 

Writing Pal is composed of nine modules. The first module is an introductory module that familiarizes the student with the W-Pal system interface and acquaints students with the strategies they will learn. The remaining eight modules are divided among the three phases of writing: prewriting, drafting, and revising.  Each of these modules provide lesson videos guided by a pedagogical agent and corresponding practice games. The prewriting modules include freewriting and planning. The drafting phase includes lessons on introduction building, body building, and conclusion building. Finally, the revising phase includes modules on paraphrasing, cohesion building, and polishing. Within the modules, 5 to 10-minute lesson videos offer strategy instruction in the form of mnemonics. 
After viewing the lesson videos, students practice applying these strategies in practice games. Practice games were designed to target key strategies within the lesson videos. These games provide identification and generative practice tasks. In the identification games students read excerpts of texts and identify which strategies are being used or identify which strategies would help to improve the excerpt. In generative games, students are asked to write short texts using the strategies presented in the lesson videos. Each game provides a score and feedback is provided as in-game progress. Students can judge their performance by the amount of progress they make in the game (e.g., winning or losing, the amount of treasure obtained). In some instances, students are provided with formative feedback via suggestion of strategy use to help improve their scores. 
Students who interact with the entire W-Pal system are more likely to learn and apply new writing strategies than students who only write practice essays (Roscoe, Brandon, Snow, & McNamara, 2013). W-Pal also increases students’ strategy use (Allen, Crossley, Snow, & McNamara, 2014). This has been demonstrated in the lab (Roscoe, Brandon, Snow, & McNamara, 2013) as well as within classroom settings where student and teachers found W-Pal to be valuable, informative, enjoyable, and a beneficial addition to the classroom curriculum (Roscoe, Allen, Weston, Crossley, & McNamara, 2014; Roscoe & McNamara, 2013).

Essay Writing Practice in W-Pal

After having received instruction within the learning modules, students have the opportunity to compose persuasive essays. Essay practice allows students to employ the strategies they have practiced throughout the modules in an authentic writing context. In a typical scenario, students are given 25 minutes to write an essay on a compare and contrast topic. These prompts are intended to mimic the types of essays that students compose in standardized tests and other high school educational scenarios. Upon submission, W-Pal automatically provides a holistic score from 1-6 as well as informative feedback relating to one or more of the strategies introduced in the modules. This feedback allows students to consider the instructional information within the context of their essay and think about how they might make appropriate revisions to improve the essay. After reading the feedback, students are given 10 minutes to revise their essay. Providing feedback on specific aspects evaluated as deficient and reminding students of useful remedial strategies allows students who may be overwhelmed with the entire writing process to focus on specific strategies so they can better apply them in their revision and their future assignments (Roscoe, Snow, & McNamara, 2013).  
The holistic score and feedback messages are driven by NLP algorithms developed using indices drawn from Coh-Metrix (McNamara & Graesser, 2012) and other NLP tools (McNamara, Crossley, & Roscoe, 2013). The feedback that W-Pal provides on the individual essays represents the system’s inner loop adaptivity. Table 1 presents example feedback messages associated with categories that are identified using a series of NLP algorithms that perform hierarchically organized “checks.” The initial feedback checks relate to length and structure. If the submitted essay fails to reach a threshold for the number of words, feedback for length would be triggered. If there are too few paragraphs, feedback for structure might be triggered. The system then checks for the quality of the essay components such as the introduction, body, and conclusion. The feedback categories are designed to target flaws associated with strategies discussed within the lesson videos. When the algorithm indicates a flaw, the system presents a message from a feedback category associated with a strategy to improve the essay that is associated with that flaw. For example, Figure 8 illustrates feedback that a student might receive if there were insufficient evidence within the body of the essay. Analyses of student revisions indicate that students who use the W-Pal feedback to generate their revision are more likely to incorporate substantive changes to their essays (Roscoe, Snow, & McNamara, 2013). 
Outer-Loop Adaptivity in W-Pal

Previous versions of W-Pal have been adaptive in the sense that they provide tailored feedback on essays (i.e., inner loop adaptivity), but the student proceeds through the system one module at a time, and the selection of instructional content is made by the teacher or student; thus the order of modules is not adaptive to the student’s performance. Also, a student may receive the same feedback on multiple essays as they proceed through the system. Hence, in addition to more targeted practice, a goal of the adaptive outer loop is to decrease potential repetition of the type of feedback across multiple sessions. 
In the newly developed outer-loop adaptive flow, the system uses the evaluation of a student’s essay to direct the student to a module about a particular strategy that targets at a specific weakness within the essay (see the right column in Table 1). The outer loop thus serves as remediation-based training. As shown in Figure 9, the student begins a ’cycle‘ by writing the essay. The system then assesses the essay, provides feedback, directs the student to a remediation instructional module based on this feedback, and then provides the opportunity to revise the essay. As such, the outer loop selects remediation task based on student performance, and then students revise their essay. The intention of this design is for the student to learn strategies for improving deficient elements of the essay from the remediation module, and subsequently apply those strategies to improve the essay in the revision phase. For example, if a student’s initial essay lacks a conclusion, the student is directed to the conclusion module to watch the video lessons and play games that emphasize strategies associated with building the conclusion. After the remediation module has been completed, students then have the opportunity to revise their initial draft. The system assesses the revision and provides another round of feedback. At that point, the system allows the student to select an activity (i.e., either a game or a video) relevant to the feedback on the revision, thus concluding the cycle (see Figure 9). 
The module used for remediation in the outer loop adaptivity is chosen based on the lowest scoring aspect of an essay. For example, imagine a student whose essay triggers feedback for both introduction and conclusion. If the introduction assessment receives the lower score, the student is sent to the introduction module to view lesson videos on strategy and engage in practice games. After completing this remediation module, the student is given the opportunity to revise the essay. The revision is evaluated by the system and two categories of feedback is again provided. In the case where the system still detects flaws with the introduction and the conclusion (and the introduction is the lowest scoring flaw), the student is directed to an activity from the introduction module as a refresher. In this case, the student will have the option to either re-watch the overview video from the introduction module, or play a game from that module. However, if after the revision, the conclusion is now the lowest scoring flaw, the student would be directed to watch the overview of the conclusion module without the option of playing games.  The logic behind this is that the games are meant to strengthen the knowledge gained from watching the lesson videos and would likely not benefit the student at this point.  
This sequence of events represents one “cycle” of the outer loop. Once the cycle is finished, the student begins a new cycle by drafting a new essay on a different topic. The system is built to promote learning over time; thus, interaction with the system is an iterative process, and the remediation modules provided depend on what the student has already encountered. There are many different paths through the modules that a student can take based on their performance (See Figure 10). Consider the case in which, during the second cycle, the same student from our example above writes a second essay. If introduction and conclusion are again both triggered, and if the introduction indices are again the lowest, instead of repeatedly assigning a module that has already been completed (i.e., introduction module), the student is directed to the module that provides strategy instruction on the second lowest scoring flaw. In this case, it would be the conclusion building module. After the revision of this second essay, if the student is still having trouble with the introduction, then an activity from the introduction module is presented as a refresher.Continuing with our example instructional path, imagine that this student completes a third essay (i.e., the third cycle), and they still have low scores for the introduction and the conclusion. In this situation, because the student has already seen these modules, the system randomly selects a module from one of the three revision module (i.e., parphrasing, cohesion building, or revision). This sequencing is intended to maximize practice and instruction, while minimizing repetitiveness.

 CONCLUSION
Individualized feedback is integral to intelligent tutoring environments. However, there are unique challenges of providing this feedback in the context of ill-defined domains such as reading and writing. Intelligent tutoring systems for literacy instruction are essential pedagogical tools because they afford immediate, individualized instruction to students, which is often not possible in traditional classroom environments. 
This chapter has focused on the refinement of adaptive instruction in iSTART and W-Pal, and in particular, the integration of outer-loop adaptivity. In iSTART, this adaptivity takes the form of text selection and scaffolding support for self-explanation. Students who successfully comprehend a text (as evidenced in the self-explanation quality or question responses), are challenged with a more difficult text in the subsequent task. Conversely, students who struggle to comprehend a given text, subsequently practice with a less difficult text. This adaptivity helps to better scaffold students’ self-explanation and comprehension strategy skills. In W-Pal, NLP scores for various components of students’ essays guide the deployment of relevant feedback and remediation instruction to support high quality revisions. Our expectation is that adaptively selecting assigned practice tasks (i.e., practice texts, practice games) based on these performance measures will enhance comprehension and writing skills. 
We are currently collecting data to assess the effects of these modifications, both in terms of system perceptions (e.g., enjoyment and motivation) as well as post-training learning outcomes. As with the development of any ITS, we anticipate iterative advances in the designs of these systems. Although we anticipate the new adaptive elements will enhance learning, one potential concern is that system-selected tasks may reduce student agency, an important consideration in student motivation and learning (Flowerday & Schraw, 2000; Snow, 1980). In the previous versions of iSTART, students were free to select which texts they wanted to read at any given time. In our new design, the system selects the text based on prior performance. Similarly, in the previous versions of W-Pal, students were able to select which instructional module they wanted to view at any time. In the new version using outer loop adaptivity, the module is selected based on essay deficiencies, providing remediation intended to improve the students’ writing. Despite the reduction in student agency, learners retain agency to select among practice activities within iSTART and practice games within a given W-Pal module. Nonetheless, the appropriate balance of learner agency and individualized instruction is an important direction for research for iSTART and W-Pal. 
	In comparison to well-defined domains (e.g., mathematics), the implementation of outer-loop adaptivity in ITSs for literacy instruction poses unique challenges we have described in this chapter: 1) subject matter experts would struggle to fully represent the problem space for ill-defined domains, 2) unlike well-defined domains, literacy tasks do not have right and wrong answers or solution paths, and 3) NLP analytic techniques are required to estimate students’ literacy skills.  Nonetheless, the effort to implement this outer loop adaptivity in such technologies will afford distinct advantages to students and teachers. For students, this individualized instruction can support development of particular skills and calibrate practice tasks to students’ particular competencies. For teachers, personalized selection of instructional and practice tasks is rarely possible in the classroom. Thus, ITSs can supplement instruction and homework activities, making extended literacy practice more efficient and tailored to the needs of all students in the class. One of our objectives for this chapter is to encourage researchers and developers continue to seek new opportunities to examine the effects of adaptivity in ITSs, and for ill-defined domains more broadly. 

