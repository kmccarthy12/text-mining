You've got some explaining to do: Effects of explanation prompts on science text comprehension

Abstract
The use of active comprehension strategies that encourage students to explain what they have read can improve students’ comprehension of complex scientific texts. Most research has focused on either strategies that are engaged during reading (online) or those used after reading (offline) -- often ignoring potential interactions that might occur in authentic learning. This study uses a 2(online: think-aloud, self-explain) × 3(offline: reread, free recall, explanatory retrieval) design with a 7-day delayed comprehension test to examine how online and offline explanation strategies affect comprehension. Linear mixed effects modeling will be used to examine both main effects and interactions.
 
Keywords: Text Comprehension, Retrieval Practice, Self-Explanation
 

Students in STEM courses are often required to read and learn from dense informational texts such as textbooks and research papers. However, students often struggle to deeply comprehend these texts, which impedes scientific knowledge-building and learning (e.g., Otero & Graesser, 2014). In addition to immediate comprehension, successful science students must also be able to recall and apply their scientific knowledge in future situations. 
Applied cognitive research has demonstrated that learning (i.e., comprehension and retention of information) is best supported by activities that encourage active, as compared to passive, processes and strategies (Chi & Wylie, 2014; Karpicke & Roediger, 2008; McNamara, 2007). Researchers interested in text and discourse comprehension often focus on the strategies that students leverage during reading (i.e., online processing). By contrast, applied memory researchers often focus on the strategies that students use after reading (i.e., offline processing), including testing and feedback. Both bodies of research have shown that prompting students to engage in more active and explanatory strategies yield more meaningful, long-term learning. However, this separation across the two related fields has led to investigations that examine these online and offline strategies in isolation. While such studies afford empirical rigor, they likely do not fully reflect real-world learning. That is, students are likely to rely on a combination of strategies and successful learning requires both increased memory for text and deep comprehension of the material (McCrudden & McNamara, 2017). Thus, the purpose of the present work is to examine how different combinations of online and offline strategies influence science text comprehension and retention. More specifically, participants were randomly assigned to both an online strategy (thinking aloud, self-explanation) and an offline strategy (rereading, recall, explanatory retrieval) to explore the extent to which these strategies, independently and in concert, influence students’ memory and comprehension of science text content. Importantly, we explore the effects of these manipulations on delayed comprehension performance (seven days after initial reading), as delays can reveal important differences between strategies (Wheeler et al., 2003).
Below we describe the theoretical motivations and empirical findings that drive the need to examine both online and offline comprehension strategies. We then explore potential interactions between them and consider how these effects may be impacted by individual differences.
Theoretical Foundations: Mental Model Construction
This study is grounded in cognitive theories of comprehension that assume that learners engage in a number of cognitive processes as they read. These processes allow the reader to construct a mental model or mental representation of the text content (e.g., Graesser et al., 1994; Kintsch, 1988; van den Broek et al., 1999; see for review, McNamara & Magliano, 2009). The most influential theory of comprehension, the Construction-Integration Model (e.g., Kintsch, 1988, 1998), assumes that the mental model is comprised of a multiple levels or layers – including a textbase representation that reflects information that is explicit in the text as well as a situation model, which includes the inferences that are needed to connect information from different parts of the text together, and to integrate information from the text with knowledge in long-term memory. These inferences lead to a mental model that is more interconnected. A well-connected model is said to be coherent and elaborated. Elaborated mental models support deeper understanding of the material and they are more durable, meaning that they also support retention of that material over time (Kintsch, 1988; McNamara & Magliano, 2009; van den Broek et al., 1999). Activities that encourage readers to generate more inferences and to draw upon their prior knowledge support the construction of a more elaborated mental model (Chi et al., 1994).
Researchers often use different types of items in order to evaluate the different levels of the mental model. Surface and textbase representations are measured using memory questions -- these items probe for recognition or recall of content stated in the text. Performance on these items may be supported by deep comprehension of the material, but the item does not require going beyond what is explicit in the text. Inference questions, on the other hand, require learners to connect parts of the text, make predictions, or otherwise apply information beyond explicit text content. These items are certainly reliant on memory traces, but they are designed to serve as evidence of a coherent situation model which suggests deeper comprehension (McCarthy et al., 2018; Wiley et al., 2005). Based on this distinction, our study proposes to assess learning from text via both memory and inference questions. More generally, these subtle, but important distinctions highlight the need to consider not only what a learner remembers (memory), but also what they understand (comprehension). We assume that both of these are involved in retention of information over time and learning of the content. 
Strategies for Enhancing the Mental Model
Online Strategies
Online strategies are those that occur during reading, typically with text content available for reference. Though online strategies are commonly employed by students, they tend to rely on ineffective strategies such as marking (e.g., highlighting or underlining; Dunlosky et al., 2013; Miyatsu et al., 2018). While these activities are more active than passive listening alone, students tend to be relatively indiscriminate in identifying key ideas, and marking alone does not employ knowledge building or knowledge transforming processes that encourage inference generation (Beireiter & Scardamalia, 1986; Chi & Wylie, 2014). A more effective online comprehension strategy is self-explanation. Self-explanation, as its name implies, is when learners explain the text to themselves as they read (Chi, 2000; Fonseca & Chi, 2011). Skilled readers spontaneously self-explain. They monitor their comprehension as they go and they construct bridging inferences that connect information from across the text as well as elaborative inferences that integrate information from prior knowledge (Chi et al., 1989; Goldman et al., 2012; Wolfe & Goldman, 2005). This constant monitoring and inferencing supports mental model building. 
Given that skilled readers generate self-explanations on their own, researchers have explored the degree to which prompting students to self-explain can support better comprehension. In these experimental manipulations (e.g., Allen et al., 2014; Chi et al., 1994) researchers prompt some learners to self-explain and others to think-aloud. In think-alouds, which serve as an active control condition, readers are asked to simply verbalize their internal thoughts (e.g., Ericcson & Simon, 1984; Pressley & Afflerbach, 1995), without specific prompts to explain the content. These studies demonstrate that prompting students to think aloud has little effect on comprehension as compared to silent reading (Chi, 2000; Magliano & Millis, 2003; Zwaan & Brown, 1996). In contrast, prompting self-explanation has positive effects on comprehension (e.g., Chi et al., 1994; Ericsson & Simon, 1998). This suggests that the explanation, and not just the verbalization, is responsible for the benefits. Analysis of students’ self-explanations indicate that self-explaining increases the number of inferences that the students make which, in turn, improves comprehension (McNamara, 2004; McNamara & Magliano, 2009).
There are a variety of ways to engender self-explanation. In the present study, we examine the effect of a general instruction to explain the content of the reading at targeted parts of the text. That is, readers are given generic prompts to write an explanation of what they are reading, rather than to answer a specific question prompt or to explain a particular idea or relationship. A generic instruction like this is a flexible scaffold that can be applied toward “any task, any time.” A recent meta-analysis (Bisra et al., 2018) revealed that generic self-explanation prompts are somewhat less effective than directions to explain a particular aspect of a text or task, but that they still significantly improve learning relative to control. This meta-analysis also showed that self-explanation is an effective means of improving learning across a variety of domains and differing tasks and test types (Bisra et al., 2018). Relevant to the current study, the effects of self-explanation were strongest when the target task was to study a text (g = .787). Self-explanation was found to be even more effective than viewing an existing instructional explanation, emphasizing the importance of generative processing. Notably, self-explanation was found to be most effective in tests designed to measure deeper comprehension (e.g., inference items, application questions) as opposed to items that relied on memory for explicit text information. This is consistent with the idea of elaborative, explanatory processing - self-explanation is beneficial not only because readers add more to their mental models, but because they construct a more interconnected and coherent representation.
These findings offer strong evidence that self-explanation serves as a meaningful online comprehension strategy. The current study attempts to replicate these effects. However, the majority of previous studies have explored how online self-explanation affects immediate comprehension tasks. While discourse theory assumes that a more coherent mental model is more durable and, thus, less susceptible to decay (Kintsch et al., 1990), it is unclear the extent to which the effects of self-explanation are maintained when tests are administered at delay. Thus, one aspect of this study is to explore the effects of self-explanation on longer-term outcomes.
Offline Strategies
Online strategies alter the processes that occur during reading. By contrast, offline strategies are deployed after reading. Some of the more effective offline strategies encourage learners to retrieve information from memory. Retrieval strategies can include tasks such as attempting to summarize a text from memory, or answering questions made available in a practice quiz. While online strategies like self-explanation demonstrate benefits for immediate testing, offline retrieval strategies show benefits particularly for longer-term retention (Roediger & Butler, 2011). Offline activities can have indirect benefits, such as an opportunity to identify gaps in knowledge (Roediger et al., 2011) as well as substantial direct effects on retention of the retrieved information (Roediger & Butler, 2011; Roediger & Karpicke, 2006a). This direct benefit of retrieving information from memory as compared to restudying is referred to as the testing effect or retrieval practice effect. We use the term retrieval practice to emphasize that the primary mechanism is an attempt to retrieve information from memory, which can be engaged by a number of offline activities, not just by formal tests or quizzes.
  Retrieval Practice. The direct benefit of retrieval practice for long-term retention (i.e., on delayed tests) is a robust effect. Retrieval practice yields better retention when compared to control conditions even when carefully matched for exposure time (Carrier & Pashler, 1992), and even when participants receive no feedback after retrieval attempts (Hinze & Wiley, 2011; Roediger & Karpicke, 2006b). Meta-analyses indicate a stable, medium-sized effect of retrieval practice compared to a restudy control (0.51, Adesope et al., 2017; 0.58, Rowland, 2014). Specifically focusing on final test outcomes with new questions requiring some transfer from the retrieval practice test to the final test, Pan & Rickard’s (2018) meta-analysis found an effect size of 0.40. Based on this literature, readers’ performance on delayed tests tends to be stronger if provided a low-stakes opportunity to retrieve content from memory after reading, as compared to equivalent time dedicated to rereading. Notably, low-stakes quizzing which emphasizes retrieval practice has been applied effectively in a number of classroom contexts (see Agarwal et al., 2012; 2021; Yang et al., 2021). Additionally, when effective, retrieval practice seems to benefit responses both to questions testing verbatim memory of text content (“detail” questions), and questions requiring application beyond the explicit text content (“inference” questions; Hinze et al., 2013; Smith & Karpicke, 2014). 
Retrieval practice can be instantiated effectively through a number of interventions, including formal and informal quizzes and tests, using different response types (e.g., short-answer, multiple-choice, etc.). In the present study, retrieval practice involves open-ended free recall of text content. We chose free recall prompts because, like more directed quiz questions, these prompts have demonstrated retrieval practice effects in previous research (e.g., Bae et al., 2019; Roediger & Karpicke, 2006b). Additionally, very specific prompts may lead to specific benefits only for the content elicited by the prompts, with little transfer to related information in the text (Anderson & Biddle, 1975; Hinze & Wiley, 2011; Wooldridge et al., 2014). Given our focus on text comprehension, which requires a mental model constructed across disparate parts of a text, retrieval prompts should ideally allow for processing that extends beyond a specific piece of information. Finally, free recall is a practical and portable study strategy that can be implemented by readers in everyday learning scenarios with little scaffolding or additional materials (McDaniel et al., 2009). Our study will further test the hypothesis that free recall leads to superior retention, as compared to restudying.
Explanatory Retrieval. In addition to testing the benefits of free recall attempts (relative to restudying), we will also test the hypothesis that the benefits of retrieval are strongest when learners are given instruction to explain or elaborate (for a review, see Pan & Rickard, 2018). Elaboration occurs when learners supplement the information provided with information from prior knowledge, including examples or explanations, as can occur in self-explanation (see above) or elaborative interrogation tasks (Pressley et al., 1987). The elaborative retrieval hypothesis (Carpenter, 2009, 2011) posits that the benefits of retrieval practice accrue because retrieval attempts activate related information to a greater degree than is elicited by restudying. This related information elaborates the memory trace, and can be used as a retrieval cue, or mediator (Pyc & Rawson, 2010), to facilitate later retrieval of the target information on the final test.
Regardless of whether the elaborative retrieval hypothesis can fully account for the benefits of retrieval practice (cf., Karpicke & Smith, 2012; Lehman et al., 2014), research shows that more elaborative processing during retrieval can supplement the effectiveness of retrieval attempts (Pan & Rickard, 2018). Elaborated retrieval for educational content can be operationalized in a number of ways. As one example, Endres and colleagues (2017) had participants either free recall the contents of an instructional video or engage in prompted recall, where they were asked to elaborate their recall attempts with examples from their own lives. As predicted, the prompted recall condition engaged in more elaborative strategies which, in turn, led to superior performance on subsequent tests.     
In the present work, we adopt the term explanatory retrieval to refer to our specific manipulation and to emphasize the similar mechanism in relation to the online process of self-explanation. While the general idea is that any elaborative processes may benefit learning from retrieval, explanatory retrieval more specifically describes the current task instructions. Hinze and colleagues (2013) implemented this intervention by comparing explanatory retrieval prompts to free recall. They found that prompting learners to explain from memory led to more cohesive and sophisticated responses, as compared to free recall prompts, and these responses were associated with superior comprehension outcomes on the delayed final test (see also McCarthy & Hinze, 2019). If these findings replicate in the current study, we expect the explanatory retrieval prompts to lead to higher comprehension scores compared to both a rereading control condition and a free recall condition.  
Combining Strategies
As demonstrated above, there is empirical evidence suggesting benefits for both online and offline explanation strategies. However, these two areas of research have largely existed in parallel. While we, and others, have speculated on how these processes operate in combination, there has been little work directly investigating these interactions. In addition to open theoretical questions, exploring the combination of strategies has important implications for the classroom. Students are likely to “mix and match” strategies when they are reading a textbook or studying for an exam. Understanding how these strategies can be used in concert can shed light upon optimal, real world educational practices. Below, we describe how we combine online and offline strategies in the current study and present hypotheses regarding the potential outcomes.
The Current Study
 Our overarching research question asks How do different types of explanatory strategies impact the long-term comprehension of scientific text information? In the current study, participants will read science texts and complete delayed comprehension tests (7 days later) that assess both memory for specific content in the text and students’ deeper comprehension of information. Participants will be randomly assigned to a 2(online: think-aloud, self-explanation)× 3(offline: reread, recall, explain) between-subjects design.
In part, this study aims to replicate extant findings from both the online and offline strategy research. We also systematically examine potential interactions between online and offline strategies through a fully-crossed experimental design. This design allows us to simultaneously address three primary research questions, including (a) is there an effect of online self-explanation strategies on long-term comprehension as measured by a delayed comprehension test?, (b) is there an effect of offline retrieval practice and/or explanatory retrieval strategies on long-term comprehension?, and (c) how do online strategies interact with offline strategies to influence long-term comprehension? Generally, we predict that explanation prompts (either online or offline) will increase participants’ delayed comprehension test performance, but that these effects will be moderated by differences in the process engaged during the other stage of learning (online, offline).
Hypotheses
Hypothesis 1: If self-explanation promotes a more durable mental representation of information, there will be a main effect of online prompt condition on a delayed comprehension test. 
Self-explanations increase the number of inferences that readers make during reading, which facilitates the construction of a more coherent and elaborated mental model (Chi et al., 1994; McNamara, 2004). Thus, we predict that participants prompted to self-explain will yield higher comprehension scores on the delayed test than those who are prompted to think aloud.
Hypothesis 2: If offline strategies affect long-term retention, there will be a main effect of offline prompt condition on delayed comprehension test performance. 
Assuming a main effect of offline condition, we will conduct two planned comparisons. First, a replication of the benefits of retrieval practice on delayed tests (the “testing effect”), would demonstrate higher performance in the free recall condition than the rereading control (H2a). Second, as a replication of the benefits of explanatory retrieval prompts, we predict higher performance in the explanatory retrieval condition relative to free recall (H2b).
Hypothesis 3: If the effects of offline processing moderate the effects of online processing, these main effects will be qualified by a significant interaction.
We also predict a significant online by offline prompt interaction. However, the exact nature of this interaction is uncertain. Below, we present two competing hypotheses, which will be tested with exploratory follow-up analyses.
First, it may be the case that explanatory retrieval prompts will be most beneficial for participants who were prompted to self-explain during reading. Previous research has shown higher-quality responses during retrieval practice are related to higher test performance (Arnold et al., 2017; Endres et al., 2017), and that the benefits of explanatory prompts are mediated by response quality (Hinze et al., 2013). As such, explanatory retrieval may be most beneficial if the reader has a quality mental model to retrieve and explain, and less effective with a lower-quality mental model (i.e., “garbage-in-garbage-out”). This formulation suggests the effects of explanatory retrieval would be strongest for learners who were also prompted to self-explain during reading.
An alternative hypothesis would be that the benefits of explanatory retrieval would be weakest for those who were prompted to self-explain during reading. This would come from the notion that participants in the self-explanation condition would have already experienced the benefits of explanation (i.e., generating inferences) during reading. Any additional prompt to explain during the offline task could be redundant (Einstein et al., 1990). If this were the case, learners in the think-aloud condition would benefit the most from explanatory retrieval prompts.
As outlined in the planned analyses below, these hypotheses are aimed at overall comprehension performance collapsed across item type as both levels (memory, inference) are important for learning. However, we will also examine performance as a function of item type. Prior work suggests that prompting and training self-explanation has little effect on memory items, but significantly improves students’ inference item comprehension test performance (e.g., Bisra et al., 2019; McCarthy et al., 2018). By contrast, retrieval practice research suggests that retrieval improves both memory and inference item performance (see Hinze et al., 2013).
Exploratory Analyses
In addition to deeper exploration of possible interactions, exploratory linear mixed effects models will be used to examine the treatment effects as a function of individual differences (e.g., reading skill, metacognitive awareness, prior knowledge). Both self-explanation and retrieval practice have shown aptitude-by-treatment interactions. However, these interactions are quite complex. Previous research has demonstrated that self-explanation training is particularly beneficial for readers with less prior knowledge (McNamara et al., 1996; McNamara, 2017). In the context of a prompt instruction, rather than strategy training, more skilled readers and those with more relevant knowledge may be better able to take advantage of the benefits of self-explanation by generating more effective strategies and higher quality explanations (e.g., Best et al., 2004). Moreover, there are also complex interactive effects across reader knowledge, reading skill, and aspects of the text and task (McNamara & Kintsch, 1996; O’Reilly & McNamara, 2007; Ozuru et al., 2009). Thus, although it is likely that skills and knowledge will influence the effects of self-explaining, the precise nature of these effects is unclear.   
In terms of individual differences in retrieval practice effects, the available research is fairly limited and mixed. For example, Moreira and colleagues (2019), found no effects of reading skill. They showed that testing effects were not related to word recognition abilities for 4th or 6th grade children. By contrast, Callendar and McDaniel (2007) found that college-level readers with lower reading skill, as measured by structure-building ability, benefitted more from answering adjunct questions (which require retrieval similar to a practice test) as compared to more skilled readers.
The effects of retrieval practice as a function of prior knowledge are also unclear. One study demonstrated no influence of prior topic knowledge on retrieval practice effects (Xiaofeng et al., 2016). Another study (Cogliano et al., 2019), found that retrieval practice effects were strongest for low-knowledge learners. By contrast, Carpenter and colleagues (2016) found benefits of retrieval practice only for students who performed well in a previous class, but not for middle- and low-performing students, potentially suggesting that higher levels of knowledge (or other skills) facilitate retrieval practice. Given the mixed results of the findings above, we do not propose strong predictions for individual differences in retrieval practice effects.  
In terms of the interactive effects of the online and offline strategies, we anticipate that the combination of online and offline explanation strategies (self-explanation + explanatory retrieval) may be particularly beneficial for readers with low reading skill and prior knowledge as they may benefit from these opportunities to engage in effective elaboration. However, given the complexity of the possible relationships, and the number of variables involved, we have not preregistered specific hypotheses regarding these findings. These results should inform future considerations of individual differences in strategy use. 
Method
Preregistration and Study Modifications
Full details of the methods, materials, predictions, and analyses are pre-registered on the Open Science Framework (OSF). The preregistration information can be found at https://osf.io/4hbwk/?view_only=bed68e2726d549efb16a55230d71d6f8 
Participants
Existing research demonstrates medium-sized effects for both self-explanation (Bisra et al., 2018; McNamara, 2004, O’Reilly & McNamara, 2007) and elaborative retrieval practice (Endres et al., 2018; Hinze et al., 2013; Pan & Rickard, 2018). Based on this, an a priori power analysis (G*Power, Faul et al., 2009) indicated a sample of 158 is necessary to detect the 2 × 3 interactive effect, with power of .80. We will recruit 216 (~36 per cell) undergraduates, which will help to account for potential attrition, and to facilitate follow-up and exploratory analyses. As an example of sensitivity for replicating effects, a pairwise comparison of the retrieval practice versus rereading conditions (n = 72 in each condition), is sensitive to an effect size as small as d = .416 (one-tailed), with power of .80.
Materials
Texts and Comprehension Tests. The texts are two science passages adapted from textbooks and the accompanying comprehension tests were developed by Hinze and colleagues (2013, see Appendix A for the texts and Appendix B for the test questions). The Fight or Flight text describes how hormones and the endocrine system help the body to react to stressful situations and the Vision text describes the path of light through the eye. The comprehension tests are 10-item multiple-choice tests (a total of 20 items across the two texts). In each test, there are five memory items that ask about information found directly in the text and five inference items that require the participant to connect or apply ideas from the text. We will compute both total comprehension test score (out of 20) as well as scores by item type.
Online Prompts. Participants will be randomly assigned to receive instruction to 1) to type out their thoughts (think-aloud), or 2) to provide an explanation regarding what they are reading (self-explanation). The full prompts are available in Appendix C. In the think-aloud condition, participants are asked to report whatever thoughts come to mind as they read and are given an example of a think-aloud. In the self-explanation condition, participants are asked to use anything they know about the text to explain it and are given an example of a self-explanation. Participants will be prompted to generate verbal protocols at nine target sentences throughout each text (18 prompts total). Participants will see a section of text, with the target sentence presented in boldface (target sentence locations are indicated in bold in Appendix A). They will then type their think-aloud or self-explanation in a text box below the target sentence. The positioning of these prompts throughout the texts is identical for the think-aloud and self-explain conditions. Participants will be able to complete the reading at their own pace. 
Offline Prompts. Participants will be randomly assigned to receive instruction to 1) reread the text, 2) recall the text’s content (free recall), or 3) explain the text’s content from memory (explanatory retrieval). The instructions for these tasks have been modified based on Hinze and colleagues (2013, see Appendix D). There are two primary differences between the free recall and explanatory retrieval prompt: most obviously, one asks the participant to retrieve the content from memory, while the other asks for an explanation. Additionally, the free recall prompt emphasizes the importance of the quantity of information retrieved, while the explanation prompt emphasizes the importance of the quality and clarity of the response.
Individual Difference Measures
We will also collect individual difference measures to ensure that our conditions are relatively matched as well as to explore potential interactions. We will collect measures of reading skill (Gates-MacGinitie Reading Test; MacGinitie & MacGinitie, 1989), metacognitive awareness (MARSI-R; Moktari et al., 2018), and science prior knowledge (O’Reilly & McNamara, 2007). 
Procedure
The study will be completed in two sessions. Out of an abundance of caution in relation to COVID-19, all data will be collected online. Participants will attend group video conference sessions (groups of 5-10 at a time). The experimenter will introduce the study and share a link to the study on the online survey system, Qualtrics. Participants will complete the tasks independently, but will stay on the call so that an experimenter can monitor the session and answer any questions.
In Session 1, Participants will be randomly assigned to an online reading prompt condition (think-aloud, self-explanation) and will read the two texts (in counterbalanced order) under the instructions provided for that online task. Timing will be self-paced for these online tasks, with time-on-task recorded by Qualtrics. Next, participants will be randomly assigned to an offline task prompt (rereading, free recall, explanatory retrieval). Participants will have 5 minutes to complete the offline task for the first text, and then complete the same offline task for the second text. This procedure ensures that the offline task does not occur immediately after reading for either text. Note that both online and offline strategy manipulations are conducted between-subjects, in order to avoid the substantial risk of contamination from one strategy instruction to another.
In Session 2, participants will return to the study one week later to complete the comprehension tests for each text. Participants who do not return for the scheduled session will have an opportunity to participate within 24 hours. If they do not complete Session 2 within that time window, they will be replaced with another participant. The tests will be administered in the same counterbalanced order as the reading phase and will be untimed. Participants will then complete the individual differences measures (reading skill, science knowledge, and MARSI).  
Data Analytic Plan
Data Cleaning and Preliminary Analyses
Participants’ data will be excluded from the analysis if their online or offline protocols indicate non-compliance. This includes repeated off-topic online verbal protocols or offline retrievals of a single sentence or less. These exclusions will be made blind to condition and prior to analysis.
Preliminary analyses will compare conditions based on cognitive individual differences factors (i.e., reading skill, prior knowledge), to evaluate random assignment. If groups differ significantly on either of these factors, we will include that factor as a covariate in subsequent analyses.
We will use existing coding schemes to evaluate the quality of participants’ online (0-3; McCarthy et al., 2021; Appendix E) and offline protocols (0-5; Hinze et al., 2013; Appendix F). Both rubrics have been used in multiple studies and have been shown to yield good interrater reliability (online: k = .71, McCarthy et al., 2021; offline: gamma = .89, Hinze et al., 2013). Comparisons of these scores using t-tests will serve as manipulation checks to verify that the prompts encouraged differential processing, as intended. Note that open-ended coding sometimes results in categorical or ordinal codes, and/or non-normal distributions. In the case of response distributions that are not continuous, we will utilize chi-square (if the responses are categorical) or Mann-Whitney U (if ordinal) tests.
Analyses
Mixed effects modeling analyses will be conducted using the lme4 package in R (Bates et al., 2015). Table 1 presents a conceptual breakdown of the LME model building procedure for this study. The baseline model (m0) will include intercepts for participants and items as random factors and text (Vision, Fight or Flight) as a fixed effect. Model 1 (m1) will add the two manipulations (online condition, offline condition) as fixed effects as well as the online*offline interaction term. Statistical significance of this overall model will be assessed via a likelihood ratio test, and the significance of fixed effects and primary hypotheses will be indicated by coefficients within the model. Follow-up tests will be conducted via the emmeans package (Lenth et al., 2021). Model 2 will add item type (memory, inference) as a main effect and the three-way interaction, to explore any differences based on question type. Hypotheses for main effects, interactions, and planned comparisons are provided above.

Investigations of aptitude-treatment interactions (i.e., with reading skill, prior knowledge, metacognitive awareness) will be conducted as exploratory analyses and noted as such in any publication.
Implications
The strength of the present research is the 2 × 3 design which affords a systematic examination of the combined effects of online and offline comprehension strategies on memory and comprehension. Previous work has been limited in that it explored only a few of these conditions at a time, making it difficult to determine how processes engaged online and offline might work together. This experiment serves to replicate existing findings and to empirically validate additional assumptions integrating research on explanatory processes in learning. Most importantly, these experiments serve as necessary first steps toward deeper investigation into the effects of combining online and offline learning strategies. The exploratory analyses, both in terms of the interactions across manipulated conditions as well as the potential interactions with individual differences, will offer guidance toward future research that investigates different aspects of these complex relations as well as more applied work that can leverage the most effective combinations of these strategies for different learners.
