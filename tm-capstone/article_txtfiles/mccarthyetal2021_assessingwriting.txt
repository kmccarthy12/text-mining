Automated writing evaluation: Does spelling and grammar feedback support high-quality writing and revision? 

Abstract
The benefits of writing strategy feedback are well established. This study examined the extent to which adding spelling and grammar checkers support writing and revision in comparison to providing writing strategy feedback alone. High school students (n = 119) wrote and revised six persuasive essays in Writing Pal, an automated writing evaluation and tutoring system. All participants received automated strategy feedback after writing the first draft of their essays. Half of the participants were also given access to spelling and grammar checkers while writing. Spelling and grammar feedback on its own had no effect on the quality of students’ first draft. Linear mixed effects models revealed improvements from initial draft to revision on most subscales. The addition of spelling and grammar feedback contributed small but significant gains after revision on five subscales (i.e., mechanics, word choice, voice, conclusion, and organization) but no other aspects of the students’ essays. Qualitative exploration of exemplar students’ revision moves revealed how students incorporated both strategy and spelling and grammar feedback into their revisions. Findings from this study demonstrate that strategy feedback with an opportunity to revise contributed to improved essay quality, but that spelling and grammar feedback provided modest, complementary benefits.
Keywords: writing; revision; AWE feedback; writing strategies; feedback uptake; mechanics

Automated Writing Evaluation: Does Spelling and Grammar Feedback Support High-Quality Writing and Revision?
1.	Introduction
The widespread use of word processors has made instantaneous feedback on spelling and grammar a common part of the writing experience (e.g., Morphy & Graham, 2012). Modern automated writing evaluation (AWE) systems are often expected to include spelling and grammar checking tools, and services such as Grammarly promise that the use of their checkers will make “everyone a great writer” (Grammarly, 2019). However, despite the growing list of spelling and grammar checkers available in a variety of languages, as well as the assumption that spelling and grammar feedback is necessary for good writing, there is little empirical work that assesses whether these tools improve overall writing quality . High-quality writing emerges not only from mastery of spelling and mechanics, but from a clear structure and coherence of the content (see Graham & Harris, 2016). Across a number of age groups, the most effective means of improving writing is through instruction on writing strategies for planning, drafting, and revising essays for content coupled with opportunities for writing practice with formative feedback (Gillespie & Graham, 2014; Graham, 2006; Graham et al., 2013, 2014; 2015, Graham & Perin, 2007; Kiuhara et al., 2009). However, this does not exclude the need for supporting spelling and grammar improvements alongside formative strategy feedback.
Indeed, recent work aimed at supporting students’ writing growth has emphasized the need for comprehensive, integrated writing support (Graham et al., 2012, 2016). Effective writing clearly conveys the author’s intended meaning and is appropriate for the given audience and context. Experts on writing instruction emphasize that effective essays are marked by specific features that include organization and structure, a clear voice, correct use of genre conventions, as well as “grammar, punctuation, and spelling” (Graham et al., 2016, pg. 36). These panel recommendations argue that the most effective way of supporting writing growth is through iterative model-practice-reflect cycles that encourage instruction, practice, and formative feedback along all key features of writing (Graham, 2021). Thus, the goal of writing instruction and feedback research should not be to determine which type of feedback is best, but rather to develop a more nuanced understanding of how to best implement integrative writing support.
Although spelling and grammar feedback tools are essentially omnipresent during most writing experiences, prior research on writing instruction and assessment does not provide clear answers regarding the potential effects of providing spelling and grammar feedback in concert with formative strategy feedback. Spelling and grammar feedback on their own are unlikely to yield substantial increases in overall writing quality; yet, it is unknown how such targeted feedback may help or hinder writing when provided alongside strategy feedback known to support better writing. To address this gap in the literature, this study examines the effects of providing spelling and grammar checkers above and beyond writing strategy feedback on the quality of initial essays and revisions. Specifically, we contrast the effects of receiving (a) formative strategy feedback alone, versus (b) formative strategy feedback along with feedback on spelling and grammar. The effects of feedback are examined for six essays composed by 119 high school students in terms of holistic scores as well as nine subscales: (1) grammar, style, and mechanics, (2) word choice, (3) voice, (4) sentence structure, (5) introduction paragraph quality, (6) body paragraph quality, (7) conclusion paragraph quality, (8) organization, and (9) unity. As such, this study assesses which aspects of essay revisions are affected by online feedback regarding mechanics, and the extent of strategy feedback uptake on essay revisions. 
1.1 Improving Writing via Strategies and Feedback
Less skilled and developing writers tend to knowledge-tell (e.g., Scardamalia & Bereiter, 1986) in stream-of-consciousness, transcribing each idea linearly as it comes to them with little regard to the writing goal or the intended audience. By contrast, skilled writers tend to be more strategic, intentional, and recursive. Successful writers engage in (at least) three overarching strategic activities: (a) planning, (b) drafting, and (c) revising (e.g., Flower & Hayes, 1981; Hayes, 2012). Coordinating these processes, enacting underlying skills, and drawing upon relevant knowledge are often challenging to developing writers (Authors, xxxx) and often lead students to struggle on writing assessments (NAEP, 2011). The central aim of writing instruction is thus to prepare students to understand and navigate these challenges (Harris et al., 2011). 
The most successful writing strategy interventions involve providing developing writers with actionable and intentional procedures and “tools” that they can apply to a variety of writing tasks (e.g., Gillespie & Graham, 2014; Graham & Perin, 2007; Graham et al., 2013, 2014, 2020). Developing writers must also have the opportunity to engage in deliberate practice with the strategies and receive feedback. That is, they must be offered ample time to compose their essays and receive formative feedback that helps them to understand how to better plan, draft, and revise their essays. Strategy instruction that targets key writing processes (e.g., planning and revising) leads to strong and positive effects on writing performance (e.g., ES = 0.82; Graham & Perin, 2007). Further, engaging in iterative drafting and revision cycles with formative feedback leads to improved essay quality and writing skill (e.g., Authors, xxxx; Butler & Britt, 2011; Graham & Perin, 2007; Hillocks, 1984; Kellogg & Raulerson, 2007; Midgette et al., 2008, Parr & Timperley, 2010; Proske et al., 2012; Santangelo et al., 2016).
1.2 Spelling and Grammar Feedback
Despite consistent research findings on the importance of writing strategy instruction and feedback, many students and instructors tend to focus on mechanical errors such as spelling and grammar (e.g., Otnes & Solheim, 2019; Underwood & Tregidgo, 2006). However, the salience and impact of mechanical errors in writing depends on the audience and the measure. For example, Graham and colleagues’ (2020) found that children with reading difficulties scored lower on essays, but that norm referenced measures (e.g., standardized tests) tended to be more sensitive to mechanical errors as compared to writing assessments developed by researchers.  Along these lines, Crossley and colleagues (2014) found that there was little influence of mechanical errors on expert raters’ evaluations of essays. They asked expert raters with at least 4 years of experience in teaching composition to score 100 student essays using the standardized SAT rubric (1-6). They found that mechanics errors in students’ essays were not significantly predictive of the raters’ scores. 
By contrast, mechanical errors in writing can be highly salient to typical or “everyday” readers (e.g., peers, colleagues, and potential employers; Boland & Queen, 2016; Figueredo & Varnhagen, 2005; Johnson et al., 2017; Marshall, 1967). For example, Johnson and colleagues (2017) asked college students to read essays exhibiting spelling and grammar errors versus content and structure errors, or a mix of both. Student raters assigned significantly lower writing quality scores based on spelling and grammar errors. Moreover, spelling and grammar errors also led participants to perceive the essay writers more negatively (e.g., unintelligent, disloyal, and unkind). 
Notably, students who place greater value on conventions and mechanics tend to be less skilled writers compared to those who recognize the importance of content and structure (MacArthur et al., 2016). Additionally, there are mixed findings regarding the benefits or detriments of providing feedback and instruction regarding writing mechanics. On the one hand, spelling and grammar feedback and instruction can be beneficial for elementary school students who are developing lower-level literacy skills (Graham & Santangelo, 2014) as well as non-native language learners (e.g., Chodorow et al., 2010; Heift & Rimrott, 2008). Moreover, several studies have demonstrated that grammar checkers can increase writers’ motivation and confidence (Cavaleri & Dianati, 2016; Potter & Fuller, 2008). However, there is reason to suspect that the availability and use of spelling and grammar feedback for proficient speakers (e.g., native language adolescents and adults) may have little effect and even negative effects on writing quality. Drawing attention to these mechanical errors can misdirect students’ attention to less important features of writing, biasing their attention to these errors and preventing them from giving their full attention to the content or structure of their essays (Graham & Santangelo, 2014; Morphy & Graham, 2012). Consequently, providing immediate spelling and grammar feedback (e.g., via checking tools) may indirectly hinder writing quality by reinforcing such biased attention. In terms of instruction beyond feedback messages, there is evidence that grammar instruction is not effective in improving writing quality. In several writing interventions, grammar instruction served as a control condition for other instruction types. Indeed, the meta-analysis conducted by Graham and Perin (2007) indicated that explicit grammar instruction had a reliably negative impact (ES = -.32) on writing performance, further suggesting that increasing attention to mechanical issues is ineffective for enhancing writing quality. Importantly, this same meta-analysis highlights that little work has been conducted to examine the effects of spelling feedback or instruction on adolescent and adult writers.
	In sum, research suggests that spelling and grammar feedback in isolation may be relatively ineffective for helping students develop their writing skills. However, most AWE systems have the capability of providing formative strategy feedback in addition to (or instead of) spelling and grammar feedback alone. Few or no studies have directly tested the benefits of spelling and grammar feedback within an AWE context (cf., Grimes & Warschauer, 2010; Lin et al., 2017), and no studies have directly examined the effects of combining spelling and grammar feedback with strategy feedback. 
1.3 Automated Writing Evaluation and Feedback
Writing technologies have reshaped the way that people write and the way that writing is taught (Graham, 2021). Advances in technology have not only made spelling and grammar checkers readily available, but also made the assessment and feedback of writing more rapid and personalized. Automated essay scoring (AES) systems originally emerged as a means of assessing student writing in large-scale standardized testing contexts; however, many AES systems have been modified for classroom use such that they also provide formative feedback. In automated writing evaluation (AWE)   systems, students engage in cycles of writing, receiving feedback, and revising. The most familiar tools include Educational Testing Service’s e-Rater (Attali & Burstein, 2006; see also Hazelton et al., 2021), Vantage Learning’s IntelliMetric (Elliot, 2003), and PEG Writing (now MI Write; Page, 2003). However, there are a variety of different AWEs available commercially and in the research sector (Authors, xxxx). In a recent systematic review, Strobl and colleagues (2019) identified nearly 90 automated writing evaluators. AWEs use natural language processing-driven scoring algorithms (or scoring engines) to provide both summative numeric scores as well as formative feedback for essays and other open-ended responses (see Shermis & Burstein, 2013; see also Authors, xxxx; Strobl et al., 2019). The types and quality of feedback provided vary from system to system. Notably these systems are not designed to replace instruction. Rather, AWEs are viewed as instructional supplements. They can provide rapid scoring and deliver feedback at scale. An entire class of students can receive one-on-one support and engage in multiple cycles of writing and revisions in a single sitting (Stevenson & Phakiti, 2014). AWEs can also enable multiple iterations of feedback, such that students can address mechanical errors and basic organizational or structural issues before submitting to their instructor. Instructors can then dedicate their energy toward feedback on content (Link et al., 2020; Wilson & Czik, 2016).
Early work in AWEs was conducted primarily with native-speaking high school and college students (Grimes & Warschauer, 2010; Stevenson & Phakiti, 2014). More recently, a relatively large body of research has focused on second language (L2) students learning to read and write in English (e.g., El-Ebryary & Windeatt, 2010; Li et al., 2015; 2017; Zhang & Hyland, 2018). Research with L2 students has reported on AWEs’ accuracy and quality as compared to human feedback (e.g., Attali & Burstein, 2006; Dikli & Bleyle, 2014; Powers et al., 2002); how teachers and students perceive the system feedback (e.g., Bai & Hu, 2017; Dikli & Bleyle, 2014; O’Neill & Russell, 2019; Ranalli, 2018); and how students integrate AWE feedback their revisions (Huang & Renandya, 2020; Koltovskaia, 2020). Students tend to find the feedback helpful and tend to be aware of the limitations and fallibility of computer-based feedback (Bai & Hu, 2017;). AWE feedback can lead to improved essay quality and, to some extent, improved writing quality over repeated practice (e.g., Authors, xxxx; Li et al., 2017). 
Broadly speaking, AWEs have positive effects on student experience and writing quality (e.g., Shermis et al., 2016). Nonetheless, there are a number of studies that find no effects of AWEs. Differences in findings likely emerge due to the wide variety of manipulations and measures that have been used in these studies. In some cases, AWEs are compared to a no feedback control. In others, AWEs are directly compared to instructor feedback. These studies also vary in terms of the outcome of interest. In some studies, efficacy is measured using error rates that are specific to mechanic issues. In other studies, researchers examine the effect of AWE feedback on distal outcomes including over AWE-generated score, human ratings of overall quality and improved course grades (see Stevenson & Phakiti, 2014, for a review). The differences in findings may also be a function of the AWE in question. Some systems (e.g., Grammarly) primarily provide targeted feedback related to spelling, mechanics, and grammar . By contrast, the Writing Pal offers feedback specific to writing processes and strategies. Other systems (e.g., Criterion) provide feedback at both levels (see Stevenson & Phakiti, 2019). Different types and combinations of feedback are likely to lead to varying results in terms of students’ perceptions and more objective outcomes. 
Collectively, research to-date suggests that AWEs can serve as a powerful vehicle to support student writing. However, we lack a coherent understanding of which pedagogical approaches embedded within AWEs are most appropriate and yield meaningful learning outcomes (Palermo & Wilson, 2020; Warschauer & Ware, 2006). As such, the present study aims to better understand best practices for writing instruction within the context of automated writing evaluation.
2. The Current Study
The current study assesses the effects of providing (a) spelling and grammar feedback and/or (b) formative writing strategy feedback within an AWE system . Specifically, we examined how adding spelling and grammar checkers to The Writing Pal (W-Pal)  AWE and tutoring system affected multiple dimensions of essay quality. High school participants wrote and revised a series of persuasive essays over several sessions. All participants received writing strategy feedback to guide their revisions, but half of the participants were also given access to spelling and grammar checking tools while writing and revising. 
Given that strategy feedback was provided between initial draft and revision, our analyses investigated the effects of spelling and grammar feedback generally, but more specifically how they influenced the improvement of essay quality from first draft to final draft, above and beyond the increases afforded by the strategy feedback.
Our broadest research question pertained to overall essay quality. Prior research informed several competing hypotheses. First, one hypothesis is that spelling and grammar feedback is detrimental (H1). During writing, spelling and grammar feedback may be intrusive to the writing process (Morphy & Graham, 2012). In the context of revision, participants tend to rely on less-productive revisions of mechanical errors (Crawford et al., 2008; Fitzgerald, 1987). By further directing students’ attention to these issues, they may neglect more substantive revisions (e.g., ignore the strategy feedback in favor of minor corrections). Further, additional feedback may be “too much of a good thing”. For example, authors (xxxx) explored the effects of adding more metacognitive feedback to a literacy tutor and found that adding feedback after every task, in addition to feedback at the end of a full instructional activity showed no positive gains in pre- to post-intervention and that this additional feedback was harmful for less-skilled students. Thus, receiving spelling and grammar feedback may be detrimental in the sense that it could impede the use of strategy feedback, resulting in less improvement in the quality of students’ essays from initial draft to revision.
By contrast, spelling and grammar feedback could benefit writing quality above and beyond strategy feedback alone (H2). Giving feedback on spelling and grammar may free up students’ time and resources for developing content and structure (Graham & Santangelo, 2014: Morphy & Graham, 2012). In this case, we would expect that students who have spelling and grammar checkers available to them would write higher quality first drafts and have ample time to allocate to substantive revisions suggested by the strategy feedback. If this were the case, we might observe higher scores for initial drafts and/or larger gains from initial drafts to revised drafts. 
Finally, a third hypothesis is that there is no effect of spelling and grammar feedback (H3). Prior work (e.g., Crossley et al., 2014) demonstrates that expert ratings of essay quality are driven by deep features of composition (e.g., the cohesion, or unity of ideas across the essay) and spelling and grammar mistakes have minimal impact. By this logic, the addition of spelling and grammar checking tools would have no direct effect on essay quality or improvements.
These hypotheses are relatively coarse-grained in the sense that they predict how spelling and grammar relate to overall essay quality. We thus extend our analyses by assessing more fine-grained components of essay quality -- including specific scores for quality of the essays’ introductions, bodies, and conclusions as well as their mechanics, sentence structure, and organization. We anticipate that those who are provided spelling and grammar feedback are likely to outperform their peers on the mechanics subscore given that they have access to explicit corrections. However, it is less clear how spelling and grammar feedback might influence the other subscores. Thus, we explore whether spelling and grammar helps, harms, or has no effect on each of these subscores in addition to overall holistic score.
We also recognize that essay scores will be affected by other factors. For example, essay quality tends to vary systemically as a function of writing prompt (Huot, 1990). In addition, essay quality is influenced by individual differences in general writing proficiency as well as by other related literacy skills. Previous work has demonstrated that reading skill is strongly correlated with writing proficiency (Authors, xxxx). To account for this variance, we examine the impact of both prompt and reading skill on the uptake of feedback in the AWE system. In addition to these quantitative analyses of human ratings of essay quality, we examine four exemplar essays to illustrate the various ways that students leverage both strategy and spelling and grammar feedback to revise their essays and how these revisions relate to changes in essay subscores and overall quality scores.
3. Method
3.1 Participants
High school students (n = 121) were recruited from a large metropolitan area in the southwestern United States through flyers as well as radio and social media ads. Two participants did not complete all 6 essays. The 119 participants who completed all essay drafts and are included in the subsequent analyses. One student did not complete the demographic questionnaire. The remaining 118 participants reported an average age of 17 (M = 17.19, SD = 1.28, Range: 13-19). Demographically, 61.89% of participants self-identified as female and 38.13% as male. Participants self-identified as Caucasian (54.23%), Hispanic/Latin American (21.19%), Asian (10.17%), African American (7.62%), or as another race/ethnicity or multiracial (6.78%). The majority of participants (87.28%) identified English as their native language.
Though randomly assigned, the non-native English speakers were split evenly across the two feedback conditions. The majority of L2 students (8) identified Spanish as their native language. Other languages include Amharic, French, German, Hebrew, Japanese, and Urdu. The majority indicated speaking English for more than 7 years. 
3.2 Design 
Participants were randomly assigned to one of two feedback conditions. Half of the participants received only automated formative strategy feedback (Strategy Condition, n = 60), and half of the participants received both formative strategy feedback and spelling and grammar feedback (Strategy + SG Condition, n = 59).
3.3 Materials
3.3.1 The Writing Pal (W-Pal). W-Pal (Authors, xxxx) is an automated writing evaluation (AWE) tool and intelligent tutoring system (ITS) that teaches writing strategies and enables multiple forms of writing practice (e.g., game-based strategy practice and essay writing). W-Pal instruction targets eight different aspects of writing: freewriting, planning, introduction building, body building, conclusion building, unity, paraphrasing, and revision. Importantly, in the current study, we examined only the AWE components of W-Pal—participants did not review strategy lessons or play practice games. In the AWE essay writing module, learners are assigned an SAT-style persuasive essay prompt and have about 25 minutes to compose an initial draft. Once time has elapsed, several natural language processing-driven algorithms provide both a holistic score (on a 6-point scale) and personalized formative feedback. More details about W-Pal have been reported in (Authors, xxxx).
3.3.2 Essay Prompts. Participants wrote and revised up to six essays in response to prompts adapted from publicly released SAT exam materials. These prompts asked participants to adopt a stance with regard to a central topic, and then to defend that position via evidence, examples, and/or logical reasoning. All prompts were designed to minimize prior knowledge demands such that participants could write from experience rather than constrained educational content or source materials. In each prompt, there was a brief introduction to the topic and then a final prompt question. For example, the prompt about “images and impressions” appears below:

All around us appearances are mistaken for reality. Clever advertisements create favorable impressions but say little or nothing about the products they promote. In stores, colorful packages are often better than their contents. In the media, how certain entertainers, politicians, and other public figures appear is sometimes considered more important than their abilities. All too often, what we think we see becomes far more important than what really is. Do images and impressions have a positive or negative effect on people?

Table 1 presents the focal question for each prompt, and the complete prompts are reported in Appendix A. For logistical purposes in administering the study, all prompts were presented in the same order for all participants.
3.3.3 Strategy Feedback Messages. In W-Pal, formative feedback messages recommend actionable steps and strategies for improving an essay via prewriting, drafting, and revising. These messages can address a variety of concerns such as generating and elaboration ideas, organizing ideas, crafting strong introductions and conclusions, providing meaningful examples and evidence, building cohesion, choosing appropriate and precise words, and overall revising. Crucially, participants never receive feedback on all categories for any given essay. Algorithms identify the most salient problem areas first. Such algorithms are based on NLP tools that evaluate linguistic, syntactic, and semantic features relevant to common challenges and weaknesses in participant writing (e.g., poorly developed ideas and low cohesion). Table 2 briefly provides example feedback messages participants could receive regarding structure, conclusion building, or cohesion.
3.3.4 Spelling and Grammar Feedback. Participants in the Strategy + SG Condition were given access to “Check Spelling” and “Check Grammar” buttons at the bottom of the essay window (Figure 1). Participants could access either function at any time during writing or revising. 
When participants used the checkers, relevant errors were underlined within the text, similar to features common in word processing software. Specifically, errors were detected using the open-source API LanguageTool (Miłkowski, 2010; Naber, 2003). Clicking on the error opened a small pop-up window containing potential corrections. To ensure that participants did not overlook the checking tools, a reminder about the checkers appeared when there were five minutes remaining in the writing session. However, participants were not forced to use the tools. 
3.3.5 Gates-MacGinitie Reading Test. The Gates-MacGinitie Reading Test (GMRT; MacGinitie et al., 1989) was included to assess reading skill, which is correlated with writing ability (Authors, xxxx). The GMRT is a standardized test in which participants read passages and then answer multiple-choice questions about them. The test contains 48-item multiple-choice items and is a reliable and well-established measure of reading comprehension (α = .85-.92; Phillips et al., 2002). 
3.4 Procedure
This project was reviewed and approved by the university’s Institutional Review Board prior to all data collection. The study took place over four sessions. In Session 1, participants completed a demographic questionnaire and the GMRT. In Sessions 2 through 4, participants authored essays in response to six persuasive prompts (i.e., two prompts per session) using W-Pal. For each prompt, participants were allotted 25 minutes to compose an initial draft. After the 25 minutes elapsed, a pop-up window appeared with an algorithm-generated holistic rating from “Poor” to “Great” (with six levels) along with formative strategy feedback. The window displayed only one feedback message that targeted a critical writing strategy for the current essay. Participants could voluntarily request to receive and view additional feedback—up to a maximum of 10 feedback messages. After participants reviewed their feedback, they were allotted 10 minutes to revise.
3.5 Essay Scoring 
W-Pal automatically assigns holistic ratings of essay quality using algorithms validated based on human raters (e.g., Authors, xxxx). However, in the present study, we relied on expert human ratings of essay quality to evaluate additional subscores. The W-Pal system score was correlated (r = .65) with the holistic scores generated by the human raters. This correlation is consistent with extant work comparing human and automated scores (Authors, xxxx).
Human ratings enabled us to assess not only holistic quality, but performance based on a variety of fine-grained subscales. Specifically, trained raters assigned a holistic essay quality score (see Table 3 for rubric) to each essay as well as ratings on nine separate subscales: (1) grammar, style, and mechanics, (2) word choice, (3) voice, (4) sentence structure, (5) introduction paragraph quality, (6) body paragraph quality, (7) conclusion paragraph quality, (8) organization, and (9) unity (see Table 4). Note that the holistic score is a separate scale (1-6) and not a sum of the subscale scores. However, it was provided by the same rater who assigned the subscores. It is also of note that the raters have not only scored these essays, but hundreds, if not thousands of other essays using this same rubric for a number of related research projects.
The trained human raters included four graduate students of English. Rater pairs were trained to a high level of reliability (i.e., all kappas > .80) on all metrics on practice essays. Each essay (N = 1428) was then scored by two raters. Across raters, the reliability of ratings for unadjudicated scores (ICC) ranged from .79 to .90. If the two raters scored differed by more than 2, the scores were adjudicated by a third party. The final scores for each essay reflect the average score of the two raters. 
Descriptive analyses of essay scores, collapsed across feedback condition, are provided in Table 5. Unsurprisingly, holistic scores were strongly correlated with all subscales (i.e., concurrent validity). These correlations are similar to those reported in other studies using expert ratings of holistic and subscores (Authors, xxxx). In addition, and consistent with prior research (Authors, xxxx), reading skill was positively correlated with holistic score and subscales (r = .47 to .68). These moderate to strong correlations provide both convergent validity for our scoring rubric as well as evidence for the need to consider the effect of reading skill on essay scores.
4. Results
4.1 Preliminary Analyses
	Exploratory t-tests indicated no difference in performance between L1 and L2 participants on holistic score and no differences on most of the subscores. The exception was for Grammar, Spelling and Mechanics in which L1 participants scored higher (M = 3.59, SD = .73) than the L2 participants (M = 3.45, SD = .80), t(223) = 2.11, p = .04. Due to the small sample and even division of L2 participants across conditions, we do not further examine the effects of the experimental manipulation as a function of speaker status.
4.1.1 Use of the Spelling and Grammar Feedback Tools
An important first step was to confirm that participants used the spelling and grammar tools provided by W-Pal. Participants in the Strategy + SG Condition authored more than 700 essays in total (i.e., 59 students x 6 essays x 2 drafts). Log data revealed that spelling and/or grammar checking tools were accessed for 622 of these essays; 149 essays exhibited no evidence of using the checkers. Among these 622 cases, the spelling checker was accessed for 592 essays (95.2%) and the grammar checker was accessed for 378 essays (60.8%). Thus, most participants used the spelling and grammar checker on multiple essays, and every participant used the checkers on more than one essay. 
When participants accessed a checker, all errors of that type (i.e., spelling or grammar) were visually underlined. Table 6 displays the number of errors of each type logged by the system for (a) initial drafts and (b) revised drafts. As an example, Table 7 shows the frequency of each type of error identified in the Competition and Cooperation prompt essays. The majority of errors were spelling errors, including both typographical errors (“teh”) and true misspellings (“Instrincly”). Notably, there were far more spelling errors than grammar errors. 
4.1.2 Effects of Spelling and Grammar Feedback Alone
The primary focus of the study was the combination of automated strategy feedback with spelling and grammar feedback. We were neither theoretically nor practically interested in the effects of spelling and grammar checking in isolation. Nonetheless, we were able to investigate the potential impact of spelling and grammar alone by comparing essay quality across the two conditions for participants’ initial draft of their first essay (on the topic of Images and Impressions). For this essay and draft, participants had not yet received any strategy feedback on any essay—the only source of support (in the SG condition) was the spelling and grammar checkers. Thus, this specific instance allowed us to assess benefits of spelling and grammar feedback versus a “no feedback” control case. A multivariate analysis of variance (MANOVA) tested the effect of condition on all of the scores (holistic and the nine subscores). This analysis revealed no effect of condition (and thus spelling and grammar feedback) on any of the expert holistic or subscale ratings (all Fs < 1.00; Table 8). This outcome suggests that spelling and grammar feedback on its own has little effect on essay quality, even when examining subscales that focus on surface-level features of the essays. 
4.2 Effects of Strategy Feedback + Spelling and Grammar Feedback 
Based on prior research, there are plausible reasons to hypothesize that combining spelling and grammar support with formative strategy feedback could be detrimental (H1), beneficial (H2), or have no effect (H3) on writing quality. Thus, we conducted analyses to examine the effects of combining feedback approaches, along with potential influences of the essay prompts and individual differences. 
To assess the effects of combining feedback approaches, along with potential effects of different prompts or individual differences in reading skill, we implemented a series of linear mixed effects models using the lme4 package in R (Bates, Maechler, Bolker, & Walker, 2015). The models allow us to enter between-subjects and within-subjects fixed factors along with the individual student as a random factor to account for person-level variance. Specifically, we conducted separate analyses for the holistic score and each subscale essay score as the dependent variable. The models also included (a) condition (strategy feedback, strategy + SG) and (b) essay draft (i.e., initial and revised), while controlling for (c) prompt and (d) reading ability (GMRT; see model structures in Appendix B). The Reghelper package (Hughes et al., 2017) was used to estimate simple slopes.
	Unsurprisingly, reading skill was correlated with all essay scores (r’s = .33-.49). For each essay score, a baseline model (M0) was created, including the two covariates, GMRT and essay prompt, in order to control for the influence of reading skill (M = .58, SD = .20) on writing proficiency as well as the differences that emerge as a function of prompt (Authors, xxxx; Huot, 1990). Model 1 (M1) added ‘draft’ (initial draft and revision) as a fixed effect to examine the effects of revision. Model 2 (M2) added the fixed effect of feedback condition (Strategy and Strategy + SG), as well as ‘draft by condition’ and ‘GMRT by condition’ interaction terms. 
Visual inspection of residual plots did not reveal any obvious deviations from homoscedasticity or normality. For each of the models listed below, significance was determined using likelihood ratio tests between each model and a reduced model. Significant chi-square (χ2) tests indicate that adding the additional variable(s) improved fit as compared to the previous model. In the following analyses, we report the best fitting model for each score. Thus, if main effects or interactions are not reported, they can be assumed to be nonsignificant.
Table 9 reports average essays scores, as a function of draft and condition, along with the likelihood ratio tests indicating model fit. Overall, Table 9 reveals that students’ essay scores improved from initial draft to revision in terms of the holistic score and all but one of the subscales (unity). This result substantiates the value of strategy feedback and opportunities to revise. By contrast, the effects of spelling and grammar feedback were limited to improvements in terms of mechanics (as expected) and three of the other subscores (word choice, voice, and conclusion). These results are discussed in more detail in the following sections. The full models for each subscale are provided in Appendix B.
4.2.1 Holistic Essay Quality
To evaluate the influence of different types of feedback on overall essay score, we first inspected the holistic score. Holistic essay quality increased significantly when participants revised (i.e., from initial to revised draft), supporting assumptions that strategy feedback and the opportunity to revise improves essay quality. There was not a significant effect of condition (Table 9). Thus, participants in both the Strategy and Strategy + SG conditions were able to use the automated formative feedback to improve their essays. However, the availability of spelling and grammar checking tools conferred no additional benefits. These findings support the hypothesis (H3) that adding spelling and grammar feedback to an AWE system has no discernable effects on overall essay quality. 
4.2.2 Grammar, Style, and Mechanics
Spelling and grammar checkers directly target spelling and grammatic errors, and thus it was unsurprising to observe a significant positive benefit for checking tools on the mechanics subscale score. Specifically, a simple slopes analysis showed that Strategy + SG Condition participants improved significantly from initial draft to revision; Estimate = 0.26, SE = 0.04, t(1283) = 4.86, p < 0.001; whereas Strategy Condition participants did not; Estimate = 0.02, SE = 0.04, t(1283) = 0.50, p = 0.62. Notably, Strategy + SG condition participants had access to spelling and grammar feedback during both stages of writing. The significant interaction suggests that the effects of spelling and grammar feedback primarily affected the quality of the revision. This finding supports the hypothesis (H2) that spelling and grammar feedback contribute positively to evaluations of participants’ use of grammar, style, and mechanics.
4.2.3 Writing Dimensions that Benefitted from Spelling and Grammar Feedback
Although students’ essays did not improve holistically with the availability of checking tools, several writing dimensions did appear to benefit. The addition of condition and the interaction terms improved model fit for word choice, voice, and conclusion, subscales (organization subscores showed a similar trend, but did not reach conventional levels of statistical significance). For all three of these subscales, analysis of the best fit models (M3) revealed significant main effects of draft (participants score improved from initial draft to revision), but no main effect of feedback condition. This was qualified by significant draft by condition interaction indicating that the effect of draft depended on feedback condition. Simple slope analyses, with draft as the focal predictor and condition as the moderator, indicated that those who had access to spelling and grammar checks tended to increase their subscales at revision (word Choice: Estimate = 0.22, SE = 0.04, t(1283) = 5.54, p < 0.01; voice: Estimate = 0.29, SE = 0.05, t(1283) = 5.97, p < 0.001; conclusion: Estimate = 0.38 SE = 0.06, t(1283) = 6.07, p < 0.001). In contrast, there were no statistically significant improvements for participants in the Strategy Condition (all Estimates < .09; t < 2). These findings further support the hypothesis (H2) that spelling and grammar feedback contribute positively to some aspects of writing quality. Notably, word choice and voice are primarily at the word level. Thus, it makes intuitive sense that these subscales would be influenced by the availability of spelling and grammar feedback. 
4.2.4 Writing Dimensions that Did Not Benefit from Spelling and Grammar Feedback
The availability of spelling and grammar feedback did not appear to influence the Introduction, Body, and Sentence Structure subscales. Students improved on these traits from initial to revised draft, but there was no significant effect of feedback condition. Although these specific findings support H3 (no effect), the broader implication of the mixed results for spelling and grammar feedback supports the conclusion that spelling and grammar feedback does not have uniform impact on essay writing. That is, spelling and grammar feedback supports some aspects of writing, but not all. 
4.3 Examples of Participant Revising
The above analyses suggest that availability of spelling and grammar checking tools did not improve students’ essay quality holistically or across all dimensions, but several traits appeared to benefit from these tools: word choice, voice, the quality of conclusion paragraphs, and essay organization. To more concretely illustrate how spelling and grammar feedback was (and was not) used by student writers, we provide below four authentic example essays from participants who received Strategy + SG feedback. These example essays were extracted from participants who demonstrated consistent improvements (e.g., larger mean gains across essays) on target dimensions during revising. Specifically, we targeted writers whose revisions tended to successfully improve word choice quality, voice, conclusion, or organization. These successful revisers were the most likely to exhibit use of the spelling and grammar tools (if at all). Importantly, these examples are not intended to provide a qualitative or mixed-method analysis (e.g., Creswell, 2013; Creswell & Clark, 2017; Saldaña, 2015) that derives generalizable themes or patterns across the dataset. Rather, our purpose was to provide meaningful examples of revising that complement the depersonalized, aggregate quantitative data.
All the example essays were written on the Loyalty prompt (i.e., “Should people always maintain their loyalties, or is it sometimes necessary to switch sides?”) (see Appendix C). The Loyalty prompt was the fourth essay during the study. We selected this essay because participants were far enough along in the study to be sufficiently familiar with the system and tools. For each of the selected example essays (n = 4), we identified (a) changes from initial draft to revised draft and (b) use of spelling and grammar feedback for essays 
4.3.1 Participant A
Across all essays, Participant A tended to demonstrate meaningful average gains (i.e., increases from initial to revised draft) on the dimensions of conclusion (from Minitial = 2.75 to Mrevised = 4.37) and organization (from Minitial = 3.58 to Mrevised = 4.62). That is, when revising, Participant A tended to consistently improve upon the quality of concluding paragraphs and essay structure. On the example Loyalty essay, Participant A increased from a score of 1.5 to 4.5 on conclusion and from a score of 3.0 to 5.0 on organization. Appendix C provides the full text of the revised essay, with deletions indicated via strikethrough and additions indicated in bold.
	The most substantive revisions were the elaboration of “the final reason” (i.e., additional support) and an entirely new concluding paragraph that summarized main ideas. Inspection of log data for the target essay revealed that Participant A accessed feedback on five potential spelling issues (i.e., “bihind,” “happns,” “strat,” “hagout,” and “br”) along with suggested corrections (i.e., “behind,” “happens,” “start,” “hangout,” and “be”). The correction of “hagout” addressed a misspelled word that was retained from the initial draft. All other suggestions appeared in the new content added by the participant; and all corrections were accepted. The participant also accessed feedback on one possible grammatical issue (i.e., “Who cares if they laugh at you” was flagged as a possible interrogatory statement), which they disregarded. The most substantive contributions—the revisions that influenced human ratings of conclusion and organization—likely stemmed from the rhetorical changes rather than edits to spelling. However, spelling and grammar feedback enabled the participant to better communicate (i.e., fewer typos) these added ideas. Overall, this student seemed to leverage spelling and grammar feedback fairly minimally, and instead allocated revisions to larger structural revisions.
4.3.2 Participant B
Across essays, Participant B tended to consistently improve word choice (from Minitial = 3.67 to Mrevised = 4.87) and essay organization (from Minitial = 3.83 to Mrevised = 4.87) from initial draft to revision. On the target Loyalty essay, Participant B increased from a score of 4.0 to 5.0 on word choice and from a score of 4.0 to 4.5 on organization. Appendix C provides the full and annotated text of the revised essay.
	Participant B implemented several revisions that potentially improved the precision or sophistication of wording within the essay. For instance, the phrase “a little crazy” was replaced with “very haughty,” the vague term “they” was replaced with “the person on the receiving end,” “unjust way” was replaced with “oppressed manner,” and the colloquial word “ok” was replaced with “valid.” When revising, log data showed that Participant B accessed feedback for only one spelling issue (i.e., “recieving”) and one potential grammatical issue (“in an oppressed way”), both of which were addressed. These changes improved the wording of the essay but represent only two of the participants’ many revisions. Thus, for Participant B, it was unclear whether the availability of spelling and grammar feedback had much impact on word choice or organization. 
4.3.3 Participant C
Across essays, Participant C tended to improve essay conclusion (from Minitial = 2.17 to Mrevised = 3.75) and organization (from Minitial = 3.50 to Mrevised = 4.91) via revising. On the target Loyalty essay, Participant C increased from a score of 1.0 to 4.5 on conclusion and from a score of 3.0 to 5.5 on organization. Appendix C provides the full and annotated text of the revised essay.
	Participant C made substantive revisions via elaboration (e.g., “This betrayal caused her mother to realize…” and “For example, they could know the location of…”), restructuring (e.g., repositioning sentences such as “This sense of nationalism…”), and the addition of a new concluding paragraph. Such revisions likely contributed positively to the improved subscale scores for essay organization and conclusion quality. On this essay, Participant C accessed no feedback on grammatical issues and only one issue for spelling (i.e., “brokeness”), which was resolved (i.e., “brokenness”). Thus, spelling and grammar feedback during revising appeared to contribute minimally to improvement of this essay for Participant C.
4.3.4 Participant D
Across essays, Participant D tended to improve essay voice (from Minitial = 2.87 to Mrevised = 4.17) and conclusion (from Minitial = 2.25 to Mrevised = 3.50) via revising. On the target Loyalty essay, Participant C increased from a score of 2.5 to 4.5 on voice and from a score of 2.0 to 4.5 on conclusion. Appendix C provides the full and annotated text of the revised essay.
	This participant implemented a variety of relatively minor revisions that improve clarity, such as correcting “No matter to who is loyalty you or not you or not you should always be loyal” to “No matter who is loyal to you or not you should always be loyal.” The participant also refined wording, such as replacing “will see it in the future” to “will affect your future,” and replacing “right decisions” with “correct decisions.” Other revisions were mechanical, such as correcting capitalization (e.g., from “i” to “I”) or repairing sentence fragments (e.g., from “Doesn’t have to be…” to “It doesn’t have to be…”). In the conclusion, the participant added a restated thesis with an exclamation point. Notably, the participant never accessed grammar feedback during revision and accessed spelling feedback only once to correct “loyaly” to “loyalty.” That is, this participant made additional spelling and grammar corrections that were not directly offered by the system. Thus, although Participant D revised the essay for spelling, grammar, and other conventions, these changes did not appear to be directly elicited by the spelling and grammar checker. 
4.3.5 Summary
Overall, these essays exemplify how spelling and grammar tools were used infrequently and contributed to incremental improvements (e.g., clarity and mechanical correctness), yet were not associated with substantive gains in holistic writing quality. These illustrative examples help to visualize how the availability of spelling and grammar feedback was neither harmful nor strongly useful. 
5. Discussion
Building upon the utility of word processing programs (e.g., Bangert-Drowns, 1993; Morphy & Graham, 2012), modern automated writing evaluation (AWE) systems provide computer-based support for multiple aspects of writing. Using NLP-based algorithms, AWEs can assess numerous lexical, syntactic, organization, semantic, and rhetorical features of essays to evaluate overall quality and detect potential problems (Authors, xxxx; Strobel et al., 2019). In turn, these assessments can yield actionable recommendations and strategies that learners can use to improve their writing (Authors, xxxx). This is important, given that prior research has established that formative strategy feedback is one of the most powerful components of writing instruction and writer development (e.g., Graham & Perin, 2007; Parr & Timperley, 2010). 
However, one assumption inherited from word processors is that AWE tools can and should assess and provide feedback on spelling and grammar. Spelling and grammar checkers are popular, and many educators and participants expect this functionality from AWE. Often, instructors and designers assume that any opportunity to deliver feedback should be leveraged. However, it is also possible that additional feedback can result in suboptimal effects (e.g., Authors, xxxx). Thus, experimental evaluations of new features represent an important aspect of effective design and development of AWEs and other computer-based learning environments (Authors, xxxx). Although automated spelling and grammar error detection is relatively easy to implement, it was important for us to demonstrate that adding this feedback would not dampen or nullify the benefits of the existing AWE system. 
This study explored the effects of incorporating spelling and grammar checking tools within an existing AWE (i.e., W-Pal) that provides formative strategy feedback (see Authors, xxxx). Due to the importance of strategy feedback for good writing, all conditions in our study included strategy feedback. The lack of a “no feedback” control limits the ability to discuss the magnitude of the effect of strategy feedback on its own, but such effects have been well-documented in prior literature. Prior writing research shaped three competing hypotheses. First, spelling and grammar feedback might be detrimental (H1) by guiding participants’ attention away from conceptual aspects of writing and reinforcing their tendency to focus on superficial edits (Crawford et al., 2008; Fitzgerald, 1987). Second, spelling and grammar feedback could be beneficial (H2) by helping participants correct superficial errors quickly, thus enabling or motivating them to expend more effort on deeper concerns (see Graham & Santangelo, 2014; Morphy & Graham, 2012). Finally, spelling and grammar feedback might have no effect—contributing little to ratings of writing quality (see Crossley et al., 2014). Importantly, we explored these possibilities across not only the holistic scores, but also in terms of the subscores representing various aspects of essay quality. Overall, our results suggest some modest benefits of spelling and grammar feedback (H2), but largely no effect (H3) of spelling and grammar feedback. 
Notably, there was no evidence that spelling and grammar feedback decreased essay quality (H1). More specifically, the availability of spelling and grammar checking tools did not significantly improve holistic essay quality nor did it improve several more fine-grained dimensions of essay quality (i.e., introduction paragraph quality, body paragraph quality, and sentence structure). However, analyses observed that spelling and grammar feedback appeared to modestly improve writing with respect to word choice, voice, and conclusion paragraphs, but only on the second draft. 
Our results indicate that correcting a few spelling and grammar errors had little impact on overall essay quality. This null result is unsurprising but emphasizes the need to focus on strategy feedback rather than spelling and grammar correction. However, our study also revealed that the addition of spelling and grammar feedback did not detract from essay quality. Students’ overall essay quality improved in both conditions. Thus, potential concerns that students might be overwhelmed and reject the feedback, or that they might become hyper-focused on less impactful revisions, were unfounded. 
It is likely that the effects of spelling and grammar feedback on subscores (e.g., conclusion and organization) was indirect. Given that students tend to focus their revisions on mechanical errors, by quickly identifying and resolving these issues, students had more time to respond to other aspects of the feedback. For example, less skilled writers tend to skip past an extended planning phase and instead engage in knowledge telling in which they transcribe their ideas as they go in relatively linear fashion (Bereiter et al., 1988). This would suggest that conclusion scores were weaker than introduction and body paragraph scores because they had less time to craft this section in this timed task. In the 10-minute revision time, students with spelling and grammar feedback were able to easily resolve spelling and grammar issues, leaving them more time to increase the length and quality of their concluding paragraph. Indeed, this assumption is supported by the qualitative inspections.
As observed across a few example essays, limited use of spelling and grammar checking tools helped participants produce more technically correct writing (i.e., fewer typos), but the most substantive revisions (e.g., added elaboration and arguments) seemed independent of the spelling and grammar support. Notably, actionable recommendations for improving essay structure, communication of ideas, cohesion, word choice, and so on are commonly provided by W-Pal formative feedback messages (see Table 2). These example essays suggest that the participants were using the formative strategy feedback to make more substantive changes above and beyond the recommendations made by the spelling and grammar checker. 
Overall, such findings are most consistent with existing work demonstrating that spelling and grammar feedback are not detrimental, but that this mechanical feedback influences only a few aspects of writing (e.g., Kellogg et al., 2010; Rock, 2007).
5.1 Implications 
Our study revealed that spelling and grammar feedback on its own had no significant effects on essay quality (i.e., on the first draft; see Section 4.1.2). Although this finding is not surprising given the extant literature on writing, it runs counter to many instructors’ intuitions that quality writing hinges on mechanics. Our findings provide further evidence in support of formative strategy feedback. 
Our log data suggested that students used the spelling and grammar tools, but not to great effect. It may be the case that students know how to navigate to use these tools, but they may not have the knowledge or skills to be able to use the feedback effectively. Thus, one consideration may be for AWE systems to provide instruction on how to best leverage spelling and grammar feedback along with their instruction on more sophisticated writing strategies. It is also likely that writers differentially seek out and leverage different types of feedback. For example, Hazelton and colleagues (2021) found that less confident writers tended to rely on grammar tools more than their more confident peers. Thus, the extent to which feedback is sought out and used is likely related to individual differences in skills, motivations, and attitudes. A more comprehensive understanding of the impact of various types of feedback requires considering both prior skills as well as how the feedback is perceived and implemented. 
	Notably, the findings do not discount the potential benefits of spelling and grammar feedback. The availability of spelling and grammar feedback in addition to writing strategy feedback showed no negative effects and, indeed, some modest benefits. Indeed, our findings highlight that quality writing involves mastery of both content and the language through which that content is conveyed. Developing writers are likely to need assistance with both. While the need to develop more foundational spelling and grammar knowledge and skills as well as development of knowledge of structure and content in tandem is likely apparent to writing researchers, it is not always obvious to instructors and students.  The current works represents part of a growing body of research aimed at a deeper understanding of how developing writers use feedback. These studies can help instructors (and AWEs) to deliver better just-in-time support that can help students to interweave quality content with good mechanics.
5.2 Future Work
The current study was aligned with prior W-Pal research (e.g., Authors, xxxx; Proske et al., 2014). Thus, the findings are constrained by the functionalities of W-Pal, such as how it permits writing and revising and how it delivers feedback. For example, W-Pal’s default setting, used in this study, is to give 25 minutes to author an initial draft and 10 minutes to revise. Participants were unable to submit their essays before this time had elapsed. These durations and restrictions were implemented to control time-on-task (e.g., prevent rushing to finish). However, given the impact of self-regulation on writing (Kellogg, 2008; Kellogg & Raulerson, 2009; Santangelo et al., 2016), it is plausible that writing or revision behaviors might differ if participants governed their own writing time. Indeed, a long history of research indicates that students do not often take full advantage of revision opportunities (Attali, 2004; Faigley & Witte, 1981), but students can and do make meaningful additions when encouraged to revise (Authors, xxxx). W-Pal also delivers strategy feedback only once the participant has submitted their essay, rather than during composition—a feature that appears in several other AWE systems (see Strobl et al., 2019). Thus, spelling and grammar feedback was deployed immediately, whereas strategy feedback was delayed. It would be of value to explore how changing the temporality of these two types of feedback, such as waiting to deliver spelling and grammar until after writing is complete, deploying strategy feedback in-the-moment, or waiting to provide spelling and grammar feedback until after larger structural and content issues have already been addressed (e.g., Koltovskaia, 2020) might impact writing quality.
It is also worth noting that W-Pal feedback targets writing strategies that are communicated in the tutorial video lessons and practice games. Participants in this study did not receive this writing strategy tutorial instruction that is part of the larger W-Pal tutoring system. Future work will examine how spelling and grammar feedback is employed in the context of strategy training.
It is also important to note that the strategies highlighted in W-Pal reflect only a subset of the possible types of writing feedback. W-Pal is one of many AWE systems. As outlined previously, different AWEs provide feedback on a variety of aspects of writing, some of which are captured in W-Pal and some of which are not. Thus, one clear direction for future comparative research is to evaluate whether spelling and grammar checking tools perform differently in AWE systems that employ different styles of feedback. Examining these effects in other AWEs will help to assess the generalizability of these findings as well as potentially important boundary conditions. Indeed, the sample in the current study also limits the degree to which broad generalizations can be made. Our study includes adolescents who are predominantly native English speakers. Preliminary analyses suggested that the non-native English speakers produced essays of similar quality, save for the grammar, spelling, and mechanics subscore. However, our sample is too small to investigate this more deeply. The finding suggests that further work should be done to examine the combination of both types of feedback for those who have more difficulty with more foundational writing skills including L2 writers and younger, developing writers. 
The present study examined how spelling and grammar feedback influenced essay writing and revision in the context of a lab-based setting wherein students were provided automated feedback. Subsequently, evaluations were provided by expert raters using an established rubric. It will be of value in future work to explore these effects across other forms of essay evaluation. As mentioned previously, non-expert raters appear to value different features of essays than their expert counterparts (e.g., Crossley et al., 2014; Johnson et al., 2017). Students also attend to different features than teachers when evaluating essay quality (Authors, xxxx). Although AWEs were built to emulate teacher feedback, there is evidence that teacher evaluation and feedback can be both qualitatively and quantitatively different from feedback provided by AWEs (e.g., Dikli & Bleyle, 2014). Thus, the effects of automated spelling and grammar feedback may differentially influence essay scores depending on the nature of the evaluation. Future work should examine these effects in the context of the classroom and with ecologically-appropriate evaluations.
Finally, our sample was predominantly native English-speaking. Although there is an abundance of work on spelling and grammar feedback in the context of L2/ESL writers, there is much less work specifically examining the combination of spelling and grammar feedback with strategy feedback on L2/ESL writers’ essay quality. Additional studies with larger L2/ESL samples must be conducted to examine this more directly. Such comparisons will contribute to our theoretical understanding of writing, and also help researchers and instructors to better tailor feedback to individual writers.
5.3 Conclusion
Although spelling and grammar feedback tools are a ubiquitous part of our writing experiences, prior research on writing instruction and assessment offered conflicting hypotheses regarding the potential effects of providing spelling and grammar feedback in concert with formative strategy feedback. Our findings suggest modest or minimal benefits of spelling and grammar feedback—formative writing strategy feedback remains the important feedback support that we should provide to participant writers. 
The rapid growth of AI-driven feedback shows that computers can do much more than simply detect typographical errors and spelling mistakes. However, it may be unwise to take this mechanical feedback for granted. Given the potential benefits of combining spelling and grammar feedback in concert with strategy feedback, we suggest possible benefits of including instruction in how to be more strategic and mindful of feedback on these errors in conjunction with instruction about deeper aspects of writing.
