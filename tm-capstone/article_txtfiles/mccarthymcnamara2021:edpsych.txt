The multidimensional knowledge in text comprehension framework

Abstract
Prior knowledge is one of the strongest contributors to comprehension, but there is little specificity about different aspects of prior knowledge and how they impact comprehension. This paper introduces the Multidimensional Knowledge in Text Comprehension (MDK-C) framework which conceptualizes prior knowledge along four intersecting dimensions: amount, accuracy, specificity, and coherence. Amount refers to how many relevant concepts the reader knows. Accuracy refers to the extent to which the reader's knowledge is correct. Specificity refers the degree to which the knowledge is related to information in the target text. Coherence refers to the interconnectedness of prior knowledge. Conceptualizing prior content knowledge along these dimensions deepens understanding of the construct and lends to more specific predictions about how learners process information. Considering knowledge across multiple dimensions is crucially important to the development and selection of prior knowledge assessments and, in turn, educators’ ability to capitalize on learners’ strengths across various comprehension tasks.

 
The Multidimensional Knowledge in Text Comprehension Framework
Prior content knowledge has a profound effect on what is understood, remembered, or learned. For example, consider the following text: 
First, light passes through the cornea. The cornea bends light to help the eye focus. Some of this light enters the eye through the pupil. The iris controls how much light enters. The light passes through the lens. The lens works together with the cornea to focus light on the retina. When light hits the retina, photoreceptors turn the light into electrical signals. These electrical signals travel from the retina through the optic nerve to the brain  .

	For a proficient reader, this text is fairly simple to read and to understand. However, even this simple text poses a number of comprehension challenges that may not be immediately apparent to a reader who is familiar with the topic. If a reader is unfamiliar with the term “cornea”, it is not obvious what this text is about. A few sentences later, the text reads “The iris controls how much light enters”. There is no explanation of what the iris is or how the iris is related to the pupil mentioned in the previous sentence. A reader who has knowledge that the iris is the colored part of the eye that changes the size of the pupil is likely better able to make sense of how light is passing through this portion of the eye. The phrase “photoreceptors turn the light into electrical signals” is also challenging. The student might know the stem “photo” and try to connect the idea that the eye will “develop” an image. Instead, the text indicates that the light is transformed into electrical signals. Without an understanding of how electrical signals are used by the brain (i.e., to convert the light into an image), it is difficult to connect these sentences together into a coherent understanding of vision. Even in this short passage, prior knowledge plays a strong role in how information is processed and understood. 
Many studies have been conducted to examine individual differences in prior knowledge, and how variation in knowledge can differentially affect comprehension. An overarching conclusion is that prior knowledge has a relatively large effect on comprehension: more knowledge lends itself to better comprehension of text and discourse. Indeed, prior knowledge predicts 30-60% of the variance in comprehension performance (Dochy et al., 1999; Shapiro, 2004). Prior knowledge directly effects comprehension in that more knowledge makes it easier to make inferences that connect the text together (Goldman et al., 2012; W. Kintsch, 1988, 1998; McNamara & Magliano, 2009; Shapiro, 2004). Prior knowledge also has indirect effects such that more knowledgeable learners are better at selecting and deploying effective comprehension strategies (Byrnes & Guthrie, 1992; Cromley & Azevedo, 2007; McNamara, 2007). 
Although the general consensus is that prior knowledge matters, there is little clarity around what aspects of prior knowledge matter and under what conditions. Assumptions regarding the underlying nature of prior knowledge and the methods of data collection and measurement vary widely from study to study (Dochy et al., 1999). In much of the existing research, including many of our own research studies, students are given relatively brief multiple-choice prior knowledge tests that include a variety of topic-relevant questions. Based on the student’s score on this test, they are labelled as a “high-knowledge” or “low-knowledge” reader. This unidimensional label obscures the inherent complexities of knowledge, and in particular, how differences in a readers’ knowledge can influence the way that a text is read, understood, and remembered. And so, whether prior knowledge influences comprehension is no longer the question at hand (it does). Instead, the question of how prior knowledge impacts comprehension is what must be addressed in future research, at least for those who seek a better understanding of either knowledge or comprehension. However, disagreements and mismatches in defining and operationalizing prior knowledge across text comprehension studies have made it difficult, if not impossible, to provide precise predictions regarding how prior knowledge influences processing of text. Our objective is to address this gap in the literature by introducing the Multidimensional Knowledge in Text Comprehension (MDK-C) framework. 
The MDK-C framework (1) identifies four key dimensions of knowledge in relation to text comprehension: amount, accuracy, specificity, and coherence and (2) posits that these four dimensions have independent and combined influences on text comprehension processes and products. As described below, the assessments used in previous studies often conflate two or more of these dimensions. These dimensions must be more carefully defined and examined in order to elucidate the relations between prior content knowledge and text comprehension. Our aim is to provide a means for educators and researchers to revise and expand their conceptualization of prior knowledge and to use this conceptualization to more intentionally and explicitly consider these dimensions. Conceptualization of prior knowledge is particularly important when selecting or developing knowledge assessments, especially with respect to the given learning context or research objectives. The intent of the framework is not to prescribe that researchers and educators assess all dimensions of prior knowledge in every study or before every classroom reading. Instead, MDK-C offers a means through which those interested in text comprehension can make more explicit and theory-driven decisions about how to build meaningful and context-sensitive prior knowledge assessments that can be used to inform theory and improve educational practice.
The remainder of the article is divided into three sections. The background section includes our definition of prior knowledge and our justification for a focus on prior content knowledge. We then briefly review existing prior knowledge frameworks along with their strengths and limitations. We then describe how the MDK-C framework’s grounding in cognitive theories of discourse comprehension drive the current inquiry and the dimensions of interest. In the second section, the MDK-C framework is introduced, outlining the four dimensions (amount, accuracy, specificity, and coherence) and how considerations of multidimensionality can lend to a deeper understanding of the impact of prior knowledge on text comprehension. This section includes empirical evidence for each dimension and approaches to assessment. We describe knowns and unknowns regarding each dimension of prior knowledge and their relations to comprehension. This analysis reveals gaps and inconsistencies in these findings which suggest the utility of examining multiple dimensions in tandem. We illustrate how the MDK-C framework has the potential to inspire novel interpretations of past findings and how it can guide future work on prior knowledge and text comprehension. Finally, we discuss limitations of the MDK-C framework and future directions through which researchers can engage in more systematic evaluation of how prior knowledge impacts text comprehension. 
Background
Defining Prior Knowledge
An agreed upon definition of prior knowledge is elusive. More than two decades ago, Dochy and Alexander (1995) noted that, despite a large body of research in prior knowledge, few researchers have provided an explicit definition (see also Alexander, 1992; Alexander et al., 1991; Dochy et al., 1999). Unfortunately, there has been little improvement in this regard. This is, in part, because the concept of “knowledge” is both ubiquitous and expansive. There are long traditions in the study of knowledge from a variety of disciplines (see Murphy et al., 2012) and a number of attempts to typify or organize all knowledge (e.g., de Jong & Ferguson-Hessler, 1996) as well as to reconceptualize specific types of knowledge (e.g., Baroody et al., 2007; Star, 2005). The MDK-C framework is an attempt to develop a deeper, more nuanced understanding of a particular type of prior knowledge -- declarative (content) knowledge and its relation to text comprehension. 
Our own grounding in theories of cognition and discourse comprehension guides our broader definition of knowledge, which in turn informs a more specific definition of relevant prior knowledge. We define knowledge as all of the information in one’s memory. This definition is aligned with one presented by Kendeou and O’Brien (2015), who define knowledge as “the theoretical or practical understanding of information and the representation of that understanding in memory” (pg. 151; see also Kendeou et al., 2003). It is implicit in these definitions that “information” is used in a broad sense to reflect anything in the mind. That is, information reflects not only facts, but also procedures and skills, beliefs and attitudes, and personal experiences. These definitions are consistent with Alexander and colleagues’ (1991) definition of knowledge as “an individual's personal stock of information, skills, experiences, beliefs, and memories” (Alexander et al., 1991, pg. 317) or Greene and colleagues’ (2016) definition of “all that is stored and accessible in long-term memory” (pg. 4, emphasis added). Thus, our definition is not intended to be controversial, but rather to reflect the general consensus of what the construct knowledge represents within the context of educational research.
Our specific interest is in the prior knowledge that a reader brings to a particular reading or comprehension task. We distinguish reader’s preexisting prior knowledge in long-term memory from background information provided just prior to reading. Providing a background reading is not “giving” students prior knowledge. For example, Anderson (1981) assessed both pre-experimental knowledge (i.e., prior knowledge) as well as experimental knowledge (i.e., background information provided immediately before the learning task) and found strong effects for pre-experimental knowledge, but not experimental knowledge. As such, recently read information does not have the same effects as stable knowledge in long-term memory. Indeed, the background readings that are often used to try to lessen disparities across students can actually exacerbate differences in knowledge by activating relevant prior knowledge for more knowledgeable readers (McNamara & Kintsch, 1996). For this reason, we reserve our consideration to studies of individual differences in prior knowledge, rather than “manipulations” of knowledge. While it may necessary to control for prior knowledge in a research context, in educational contexts, it behooves researchers and educators to strive toward better understanding and measurement of prior knowledge so that supports can be added to meet the student where they are.
We further limit the scope of our framework to prior semantic content knowledge. When someone asks if a particular person knows something, they are often asking if that person is familiar with that particular fact or piece of knowledge (i.e., Randall knows the names of all of the presidents; Priya knows that rain comes from the clouds). Thus, semantic content knowledge is often the focus of prior knowledge research. For the sake of parsimony, reference to prior knowledge in this article can be assumed to mean prior content knowledge unless otherwise noted. The focus on prior content knowledge in text comprehension is not intended to imply that other types of knowledge (e.g., epistemic, procedural, metacognitive) are not relevant to comprehension, nor to suggest that prior content knowledge is not relevant for other learning tasks. It is possible that the MDK-C framework may apply to other types of knowledge or other learning tasks. However, we focus on prior content knowledge in the context of text comprehension to constrain our review. Limiting the scope of the framework to the current literature on prior content knowledge in text comprehension allows us to appropriately articulate the boundaries of our findings and the claims made in the framework. A more robust body of research will afford validation or refinement of these dimensions, and potentially prompt evaluations of how prior content knowledge is acted upon or influenced by other types of knowledge as well as other individual differences (e.g., skills, beliefs, motivations, affect). 
Extant Frameworks and Evaluations of Prior Knowledge
As mentioned previously, this framework is far from the first to attempt to describe prior knowledge (e.g., Dochy, 1992; Dochy & Alexander, 1995; Dochy et al., 1999). For example, Dochy (1992) described the inherent qualities of knowledge, including amount, misconceptions, availability, accessibility, completeness, and structure. Availability and accessibility describe the state of knowledge for the learner and are assumed to be related to structure in the sense that knowledge that is more structured may facilitate accessibility to other knowledge. Dochy and Alexander (1995) refined these distinctions within a general, conceptual mapping of knowledge. This framework identifies key dimensions of varying states (declarative, procedural, conditional), explicitness, and contains both conceptual and metacognitive components that may be domain-specific or domain-transcendent. While this framework assumes that knowledge is structured, there is little discussion of how it is structured and how this would influence comprehension processes. This conceptual mapping is also not intended for making predictions specific to comprehension. 
de Jong & Ferguson-Hessler (1996) proposed a general knowledge framework specifically grounded in aspects of knowledge relevant to science learning. They argue that there are multiple types (situational, conceptual, procedural, strategic) and that each type of knowledge could be described with several qualities (surface-to-deep, isolated-to-structured, declarative-to-compiled, verbal-to-pictoral, general-to-domain specific). More specific to content knowledge and text comprehension, Kendeou and O’Brien (2015) suggest that knowledge can be described as having characteristics. These characteristics include forms (i.e., general world, domain, and topic), quantity (i.e., the amount or extent of knowledge), and quality (i.e., the accuracy, structure, and degree of interconnectedness). The MDK-C framework echoes these characteristics, but further decomposes them into dimensions that afford deeper investigation into the nature of a learner’s mental model. Indeed, each of the descriptions of knowledge  mentioned above acknowledge that different facets (e.g., types, qualities, dimensions) of knowledge are likely to overlap or intersect. However, these interactions are not prominent features of the frameworks. There are no current frameworks that describe the multiple characteristics of the potential prior content knowledge that a reader might bring to bear during comprehension. By contrast, clear operationalization and multidimensionality are key aspects of the MDK-C framework. This framework draws upon many of the facets described above, but more clearly defines each dimension and proposes means through which researchers can articulate rational emphasis on different dimensions or particular methods of assessment. Further, multidimensionality suggests that a reader’s prior knowledge simultaneously varies along multiple dimensions. While there may be relations across the dimensions, the assumption is that these dimensions are, at least to some extent, independent. Of course, the amount of knowledge will be somewhat requisite for the other dimensions, but researchers must consider going beyond conceptualizations of prior knowledge solely in terms of amount of prior knowledge by accounting for other dimensions. The whole of a reader’s prior content knowledge cannot be summarized as “correct to incorrect” or “general to specific” and describing knowledge along only one dimension ignores variability across the others.
Theoretical Foundations
Our focus on semantic content knowledge is, in part, based on its prominence within cognitive theories of discourse comprehension (e.g., Gernsbacher, 1991; Graesser et al., 1994; W. Kintsch, 1988; van den Broek et al., 1999). Though there are important differences across theories of comprehension, they generally agree on several tenets (McNamara & Magliano, 2009). In these theories, it is assumed that comprehension emerges from spreading activation of information across a connectionist network comprised of concepts (nodes) and connections between those concepts (links). Readers use information from the text and information from their prior knowledge to construct multi-layered mental representations of what they read. Thus, prior knowledge is a cornerstone for constructing mental models. Albeit implicit in many of these models, the prior knowledge that is implicated is predominantly semantic in nature. More recently, discourse psychologists have emphasized that everyday comprehension tasks are goal-directed and occur in the context of content and task (Britt et al., 2017; Magliano et al., 2018; McCrudden & Schraw, 2007; Snow, 2002). Readers are more likely to draw upon prior knowledge when the activity is content-driven and grounded in a specific task such as reading to solve a particular problem as compared to reading a passage in order to answer a set of standardized questions (Cervetti et al., 2020; Pearson & Billman, 2016). These notions of purposeful reading further highlight the need to understand how a reader’s prior knowledge can impact the processes and products of comprehension in educational settings.
The primary outcome of interest in text and discourse research is the quality of reader’s mental model. Discourse researchers use assessments that tap into different levels of representation, allowing researchers to go beyond how much did this reader remember? to what aspects of the content did the reader understand? Or how likely is it that the reader could use this information in a novel situation? (e.g., Fletcher & Chrysler, 1990; van Dijk & Kintsch, 1983). Thus, text comprehension assessments are often comprised of textbase and inference items. Textbase items prompt students about information explicitly from the text. On the other hand, answers to inference items cannot be found directly in the text. These items require the reader to connect, apply, or integrate information from various parts of the text or prior knowledge. Educators might readily recognize this as the distinction between shallow and deep questions (E. Kintsch, 2005). Readers who perform well on textbase items have good memory for the text and readers who are able to make more inferences have constructed a coherent and elaborated mental model. Differential performance on these different item types reveals important differences in the quality of readers’ mental models. 
Discourse researchers are also interested in the types processes that students engage in as they read. Readers often restate information that comes directly from the text (e.g., paraphrases), but that they also generate a variety of inferences, such as local bridging inferences that connect information from proximal sentences, distal bridging inferences that connect information from larger expanses of text, and elaborative inferences that demonstrate deeper integration of textual information with prior knowledge (Singer, 1994). Which inferences are generated when varies depending on the nature of the text(s), features of the task, and aspects of the reader (Higgs et al., 2017; McNamara & Magliano, 2009), but, generally speaking, readers are better able to construct local bridges as compared to distal and elaborative inferences (Ozuru et al., 2007; Yukhymenko-Lescroart et al., 2020; cf McNamara et al., 1996) and more skilled or more knowledgeable readers generate more elaborative inferences (e.g., Carlson et al., 2014; Goldman et al., 2012; Ozuru et al., 2009). 
These types of comprehension assessments afford a rich evaluation of a reader’s mental model for a given text or set of texts. While assessing comprehension is not the focus of this paper, it is mentioned to highlight the disparity between the methods of conceptualizing and measuring knowledge during and after reading as compared to assessments used to measure knowledge prior to reading. That is, researchers and educators rarely use this same, multidimensional approach to understand what a reader knows before they begin a learning task. We view this as an immense limitation to the understanding of mental model construction. Thus, the MDK-C framework was developed to drive novel ways of evaluating a reader’s prior knowledge that can be more clearly mapped onto comprehension processes and outcomes.
The MDK-C Framework
The MDK-C framework posits (1) that prior content knowledge is comprised of four intersecting dimensions: amount, accuracy, specificity, and coherence and (2) that these dimensions must be examined both independently and interactively in order to fully identify how prior knowledge influences text comprehension. Drawing on theories of knowledge and discourse comprehension, the MDK-C framework assumes that prior content knowledge can be represented as a multidimensional mental model, or semantic network comprised of nodes and links. Based on spreading activation, information in prior knowledge that is more semantically-related and more interconnected with the new incoming information from the text is more likely to be activated and to be activated more quickly. Providing clearer definitions and boundaries around the four dimensions that comprise this mental model will allow for more nuanced predictions about how prior knowledge will be used during comprehension. 
We use the term dimension for two reasons. The first is to highlight that there are likely few clear boundaries between categories. Instead, aspects of knowledge can fall along a spectrum. The second is to emphasize that these dimensions exist in parallel. A contribution of the MDK-C framework is the potential consideration of interactions between these different dimensions of knowledge. Dimensions allow us to conceptualize multiple facets of knowledge simultaneously. Differences in these dimensions affect what information is attended to, encoded, and integrated, which in turn affect what the reader is able to understand and later remember. The purpose of the MDK-C framework is to better elucidate dimensions of prior knowledge so that researchers can conduct more targeted work in order to better understand how these different dimensions uniquely and interactively influence how a student learns from text.
It is critical to note that the MDK-C framework is specific to prior content knowledge, but it is assumed that prior content knowledge is not the only factor involved in understanding, remembering, and learning from text. The framework is designed to be incorporated amongst more general theoretical and conceptual frameworks. In this way, researchers can better explore how prior content knowledge relates to and interacts with other individual differences including other types of knowledge as well as beliefs, attitudes, motivations, strategies, and abilities.
Dimensions of Knowledge
As illustrated in Figure 1, we propose four intersecting dimensions: amount, accuracy, specificity, and coherence. We first define and review each dimension independently with emphasis on the construct, empirical findings, and methods of assessment. We also describe the limitations and knowledge gaps in the current literature to explore how the MDK-C framework, and consideration of multiple dimensions at once, can serve to progress understanding of the relations between prior knowledge and comprehension. 

Amount
Amount refers to how many concepts the reader knows that are relevant to the text content. In a network representation of knowledge, amount would refer to the number of concepts (nodes) in memory that are relevant to the current text. Rather than trying to count the specific number of concepts in any one learner’s mental model, researchers and educators develop prior knowledge assessments that include items that reflect the breadth of a topic. Performance on prior knowledge assessments are generally relative rather than absolute. That is, a participant who scores a 75% on a prior knowledge test is not assumed to have five times as much knowledge about a topic than someone who scores a 15% on the same test; but better performing students are assumed to have more prior knowledge than peers who perform less well.
The amount of knowledge that a reader possesses can impact comprehension in a number of ways. The first is that more familiarity with relevant concepts and vocabulary aids in lower-level word processing (Priebe et al., 2012). Prior knowledge also supports higher-order processes. When relevant information is available, it is easier to make inferences that support comprehension (Graesser et al., 1994; W. Kintsch, 1988). More prior knowledge also supports readers’ ability to access and activate relevant information and ignore less relevant information (Afflerbach, 1986; McNamara & McDaniel, 2004; Voss et al., 1980). More knowledge is also associated with readers’ ability to use more effective comprehension strategies, which is, in turn, associated with better comprehension (e.g., Cromley & Azevedo, 2007; Mokhtari, 2018). The amount of knowledge a reader has is also beneficial because new information builds upon extant knowledge. That is, there is a positive learning cycle in which knowledge begets better comprehension. This improved comprehension, in turn, increases and improves the prior knowledge base for future learning (Goldman et al., 2012; Kendeou et al., 2016; McNamara & Kintsch, 1996). Although increased learning is generally good news, these findings also highlight why differences in prior knowledge can exacerbate achievement gaps as the “rich get richer” (also known as the Matthew effect; Merton, 1968; Stanovich, 2009). As mentioned earlier, background readings do not effectively bridge this divide. Thus, it is critical to develop a better understanding of how prior knowledge supports comprehension so that educators can leverage the knowledge that is available and provide scaffolds for those who need additional support. 
A number of early studies on prior knowledge in text comprehension and problem solving relied on knowledge of non-academic topics as measured by performance on a prior knowledge test. For example, in a series of studies by Voss and colleagues (Chiesi et al., 1979; Spilich et al., 1979; Voss et al., 1980), undergraduates completed a 40-item prior knowledge test that evaluated knowledge of baseball terminology. Students with extreme scores were selected as participants and listened to a play-by-play of an inning of a baseball game. The high prior knowledge participants were better able to identify the key goals and moments of the inning and recalled more information overall than did the low-knowledge students, suggesting that more knowledge facilitated recall of text information. Further work demonstrated that these effects emerged at the lexical level – readers with more baseball knowledge were better able to suppress dominant, but irrelevant meanings when reading ambiguous words in a baseball context (McNamara & McDaniel, 2004), but more knowledgeable readers also struggled to inhibit baseball-related meanings when ambiguous words were presented in a non-baseball context (Wiley et al., 2018). In a study of soccer fans, less skilled, but higher knowledge readers demonstrated better recall and comprehension than more highly skilled readers with low soccer knowledge (Schneider et al., 1989). Researchers have found similar effects of prior knowledge in other non-academic domains (e.g., Wang et al., under review) as well as across a variety of academic domains (e.g., Johnston, 1984; McNamara et al., 1996; Voss & Silfies, 1996) and across many age groups (e.g, Adams et al., 1995; Best et al., 2005; McNamara et al., 2011). Thus, the relative amount of prior knowledge that a reader possesses is a strong predictor of content memory and comprehension success above and beyond the effects of reading skill (O’Reilly & McNamara, 2007; Schneider et al., 1989). 
A common limitation in the study of prior content knowledge is that amount often serves as the default metric, but is often confounded with other dimensions, both conceptually and methodologically. In many studies that have investigated the effects of the amount of knowledge, participants are described as experts or novices based on their prior knowledge test score. In reality, most of these participants (mostly college students) are not truly experts as is traditionally defined in expertise research (e.g., Alexander, 2003; Ericsson et al., 1993). True experts certainly have more knowledge than their novice counterparts, but it is also assumed that experts’ knowledge is qualitatively different (Alexander, 2003, 2004; Bédard & Chi, 1992). This draws in notions of the structure and coherence of knowledge above and beyond amount. Thus, using short-hand to refer to those with more knowledge as “experts” conflates multiple dimensions.
One limitation in exploring prior knowledge as an individual difference is that researchers have often examined prior knowledge dichotomously. In some studies, researchers selected only extreme cases (Chiesi et al., 1979; E. Kinstch & W. Kintsch, 1995; Spilich et al., 1979; Voss et al., 1980). In other studies, researchers have relied on mean or median splits to compare “high” and “low” knowledge learners (e.g., McNamara et al., 1996; McNamara & Kintsch, 1996; Wiley et al., 2018). These categorizations were often derived due to constraints in analytic methods, as well as ease of interpretation, but they also reduce statistical power and create an artificial separation that may not be theoretically appropriate. In more recent work, the increased availability of sophisticated statistical techniques (e.g., regression), has rendered it easier for researchers to examine prior knowledge as a continuous measure. These more robust statistical approaches have also allowed researchers to explore non-linear relations between prior knowledge and comprehension. For example, O’Reilly and colleagues (2019) asked students to complete a vocabulary-based prior knowledge test before completing a scenario-based comprehension assessment. Using a broken-line regression technique, they demonstrated that the number of questions correctly answered was not uniformly related to comprehension performance. For students who scored greater than 59% on the prior knowledge test, there was a strong relation between prior knowledge and comprehension performance. Below this prior knowledge score, there was no significant relationship between prior knowledge and comprehension performance. Thus, this analysis uncovered a knowledge threshold. Interestingly, the researchers further demonstrated that some items on the prior knowledge assessment were more predictive of whether or not the student fell above or below the threshold than others. This finding alludes to the need to go beyond measuring prior knowledge only in terms of total amount. It may be the case that some of the items were more proximal to the texts in the experiment, suggesting an importance of knowledge specificity, but it may also be the case that a specific term was more integral to an elaborated mental model, which would suggest the importance of knowledge coherence. However, more work is needed to better establish the parameters and interpretations of these findings. 
Much of the work related on prior knowledge implicitly suggests that more knowledge is better. The findings from O’Reilly and colleagues (2019), however, suggest the importance of both quantity and quality, but also a potential distinction between the two. The amount of prior knowledge a reader possesses will always be an important dimension as it is, at least to some extent, prerequisite for the other dimensions. However, it is also important to examine situations in which “less, but better” may affect processing or cases in which too much knowledge may impede comprehension (e.g., McNamara et al., 1996), or, as described in the following section, when that information is not accurate. Indeed, despite the fact that the amount of knowledge is often the primary dimension along which knowledge is described, there are few studies that examine amount independent of the other dimensions. Thus, many of the studies that have reported differences between high-knowledge and low-knowledge learners are also referenced in the remaining sections.
Accuracy
Accuracy refers to the extent to which the reader's knowledge is correct or incorrect. A reader’s mental model may contain inaccurate nodes of information and/or it may also include incorrect links between the nodes. Accuracy of knowledge is more complex and nuanced than a “correct” conception versus a misconception (e.g., Smith et al., 1994; Vosniadou & Skopeliti, 2017). Knowledge can be more or less accurate, both in in terms of the information’s veracity in the world and the accuracy of a given idea within a given reader’s knowledge base. In some cases, factual knowledge can be labelled as “accurate” or “inaccurate”. For example, the statement “Abraham Lincoln was the 13th president of the United States” is inaccurate. However, there are other types of statements that are harder to categorize, either because the information lacks a veridical or definitive answer or because of the complexity or context of the information. For example, the statement “going on a diet can help you to lose weight” is true in the sense that decreasing calorie intake will result in weight loss, but it is also the case that “going on a diet” (as opposed to prolonged change in one’s habits) is often not a viable solution for sustained weight reduction. Moreover, not all types of diets result in weight loss. Thus, this statement falls along a spectrum of being more or less true depending on the context. 
Developmental psychologists and learning scientists in particular emphasize that the accuracy of knowledge is continuous and more nuanced than mere “correct” or “incorrect”. Children often possess “flawed” or naïve conceptions that reflect a developing understanding of more complex set of explanations (Carey, 1985; Vosniadou & Brewer, 1992, 1994). One common example regards children who learn that the world is round. This scientific concept is inconsistent with the students’ lived experience which suggests that the world is flat. Over time, children tend to develop a naïve conception that merges these two incompatible notions. When asked to draw what they think the world looks like, students sometimes draw something similar to a snow globe, with flat ground surrounded by a spherical sky (Vosniadou & Brewer, 1992). These flawed, but relatively coherent representations reflect “hybrid conceptions” that possess both accurate and inaccurate information and, although this conception is still inaccurate, these changes reflect positive growth in the student’s knowledge (e.g., Vosniadou, 2009; Vosniadou & Skopeliti, 2017). 
Inaccuracies can emerge from inaccurate nodes of information, but they may also result from inaccurate connections between nodes in the mental model. As a simplistic example, a reader may know three discrete pieces of information that a) a lemon is a yellow citrus fruit, b) a lime is a green citrus fruit, and c) underripe fruit tend to be green. The reader may incorrectly connect these three concepts by inferring that a lime is green because it is an underripe lemon. Thus, the idea that limes are simply underripe lemons is incorrect, but built upon a foundation of accurate information. With this in mind, a learner’s knowledge is not wholly accurate or inaccurate and assessment of the mental model would need to be sensitive to both amount and coherence of knowledge in addition to accuracy. 
Inaccurate knowledge interferes with comprehension and the acquisition of new knowledge (Driver et al., 1994). Indeed, in the era of information overload on the internet as well as the burgeoning issues of misinformation and disinformation (“fake news”), understanding how readers consume and leverage inaccurate information has become a hot topic in social sciences research (Kendeou et al., 2019; Lazer et al., 2018). Readers tend to be quite susceptible to inaccurate information and often lack the critical reasoning skills to notice or correct potential inaccuracies during encoding (Marsh et al., 2003; Rapp, 2016; Rapp & Braasch, 2014; Singer, 2013). Inaccurate information that is consolidated in long-term memory cannot simply be erased, but instead must go through processes of knowledge updating and revision (e.g., Dole & Sinatra, 1998; Kendeou & O’Brien, 2014). As a result, inaccurate information is easy acquire and often resistant to change (Guzzetti et al., 1992; Vosniadou, 2009). For example, Alvermann and colleagues (1985) asked students with a particular misconception to read a text with the correct information. Immediately after reading, the students were able to recall the correct information. However, after a delay, these students reverted back to their inaccurate understanding, suggesting that students with inaccurate understandings may struggle to integrate conflicting information into their long-term memory. This resistance to uptake may result from information being activated and integrated based on relevancy rather than accuracy. Thus, inaccurate information becomes part of the reader’s developing mental model. For example, in a study on physics misconceptions, students who held misconceptions generated significantly more incorrect inferences during reading and significantly fewer correct inferences than their peers (Kendeou & van den Broek, 2007; see also Kendeou & van den Broek, 2005). Similarly, elementary school students with naïve or fragmented conceptions generated incorrect inferences during reading. The children’s fragmented understandings were associated with lower recall, but perhaps more importantly, postreading interviews further suggested that these incorrect inferences supported the generation of new misconceptions (Vosniadou and Skopeliti, 2017). 
Inaccurate knowledge can also impact metacomprehension. In a statistics course, undergraduates with more misconceptions performed less well on a test of conceptual understanding than those with fewer misconceptions. Further, these students demonstrated greater overconfidence in their performance (Prinz et al., 2018; see also Braasch et al., 2013). Inaccurate judgments of one’s own understanding is likely to further impede future learning.
Similar to the issues of amount, assessment of inaccuracy of knowledge tends to be overly simplistic. There has been little research to date that focuses on inaccurate knowledge using multiple measures of knowledge or multiple dimensions of knowledge. Researchers tend to rely on concept inventories or brief multiple-choice pretests and posttests. Many studies use pretest surveys to categorize participants as those who show evidence of the misconception and those who do not. Those categorized has having the misconception are included in the study and those who either have the correct conception or no specific conception are either not included or included as a comparison condition (e.g., Hynd & Alvermann, 1986; Kendeou & van den Broek, 2007; McCrudden & Kendeou, 2014). In other cases, students with more misconceptions are compared to those with fewer misconceptions (e.g., Prinz et al., 2018). In both types of studies, the number of misconceptions is used as the estimate of knowledge. These assessments, while easy to administer, can conflate qualities of amount, accuracy, specificity, and coherence of a reader’s mental model and it is difficult to use any given item to evaluate the quality of the mental model. By contrast, in order to have a richer understanding of a reader’s prior knowledge, those who study the developmental trajectory of inaccurate knowledge tend to rely on more open-ended tools such as interviews or student drawings (e.g., Vosniadou & Brewer, 1992, 1994; Vosniadou & Skopeliti, 2017). These approaches afford analyzing a learner’s knowledge across multiple dimensions, but are more resource intensive. By considering multiple, separate dimensions, researchers can draw more novel and specific predictions about how a highly knowledgeable reader with a slight misunderstanding might process a text differently from a reader with a robust and coherent “misconception”. It may be the case that such an elaborated, but ultimately inaccurate mental model may be too tightly integrated for new information to gain enough activation strength to “break into” the existing representation. Combining different types of measures to assess multiple dimensions of knowledge would lend to a more complete understanding of how readers develop, persist, and even enhance misconceptions and misinformation.
Specificity
Specificity refers to the degree that the knowledge is related to information in the target text. By definition, relevant prior knowledge is, in some way, related to the to-be-read text. However, relatedness can range from very broad to topic-specific. At its broadest, prior knowledge is simply general knowledge. In most cases, educators and educational researchers are interested in students’ general academic knowledge. This type of knowledge is often assessed using measures such as the Woodcock-Johnson Academic Knowledge Test (Wendling et al., 2009) and the knowledge subtests of the Wechsler Intelligence Tests (Wechsler Adult Intelligence Scale; Weschler Intelligence Test for Children) . General world knowledge is a strong predictor of comprehension for those learning to read, particularly for children (e.g., Best et al., 2008). General knowledge also plays an important role for adult literacy learners and foreign language learners. For example, in a recent study of adult literacy learners, Talwar, Tighe, and Greenberg (2018) found that WJ-III academic knowledge subtests (history, science, humanities) loaded onto a single general prior knowledge factor and that this factor significantly predicted reading comprehension performance. They demonstrated that prior knowledge was a unique contributor to reading comprehension above and beyond the basic skills (decoding, listening comprehension, vocabulary) emphasized in the Simple View of Reading (see also Cervetti et al., 2020). Such findings are consistent with theories of discourse comprehension and emphasize the critical influence of prior knowledge. Notably, Talwar and colleagues used the Woodcock-Johnson Passage Comprehension (WJPC) test as the measure of reading comprehension. In this task, readers are tasked with providing the missing word in a one or two sentence passage. While this is an appropriate task for an adult literacy learner, it likely does not reflect the more content-rich texts and comprehension tasks faced by those reading to learn. The basic narratives read by students who are learning to read are often grounded in simple, everyday experiences. By contrast, informational texts like science textbooks often involve specific terminology and phenomena that may not be part of a reader’s daily experiences (Graesser et al., 2002; Wolfe & Mienko, 2007). Thus, as readers engage in more content-based reading, their comprehension success is less determined by reading skill and general knowledge and more dependent on more specific content knowledge (McNamara et al., 2011; Pearson & Billman, 2016).
Content knowledge, or subject-matter knowledge (Dochy & Alexander, 1995) can be further decomposed. Domain knowledge refers to the realm of knowledge that individuals have about a particular field of study (Alexander & Judy, 1988). A single text is likely to include information about more specific topics within that domain. In our example of how light passes through the eye, the text would activate knowledge within the domain of science. The reader might possess knowledge about the anatomy of the eye within relevant domain knowledge. For the text in question, this information would be topic-specific (Alexander et al., 1994; Ozuru et al., 2009) or passage-specific knowledge (Langer, 1984). 
Generally speaking, broader general knowledge supports more specific comprehension. For example, Murphy and Alexander (2002) demonstrated that a semester of instruction in a domain (educational psychology) increased specific subject matter knowledge. Braasch and Goldman (2010) demonstrated in a lab setting that those with more knowledge of a broader subject (air movement) showed greater gains in comprehension test performance about a related topic (El Niño). These studies reflect the majority of research in the study of prior knowledge that examines the effect of one level of knowledge specificity on gains in another. 
	The idea of knowledge specificity is not unique to the MDK-C framework. A number of researchers make distinctions between general world knowledge, domain knowledge, and topic-specific knowledge (Alexander et al., 1994, 1994; Buehl et al., 2002; Chiesi et al., 1979; Langer & Nicolich, 1981; McCarthy et al., 2018b). The unique contribution of the MDK-C framework is to better operationalize these distinctions and to provide additional granularity between these more common terms. The MDK-C framework attempts to explicitly delineate where a particular idea or concept lies on this continuum from broad world knowledge to esoteric knowledge contingent upon the text or task at hand. These delineations also require the use of a variety of assessments to evaluate knowledge at these different levels. Figure 2 provides labels for each level of specificity along the left-hand side and examples of each level in the hierarchical structure. The category labels were inspired by their common use in research and in educational practice. The domains of science and history are indeed “fields of study”, but a broader conception than the examples offered by Alexander and colleagues. We use the term subject based on common usage in schools. Within a given domain (e.g., math), students may take a number of subjects (e.g., geometry, algebra, calculus, trigonometry). Within these subjects, students encounter particular topics, such as “area” and “circumference” or “logarithms” and “exponents”. 
Figure 2 is neither an exhaustive list of all knowledge in a learner’s mental model nor are the categories intended to reflect clear boundaries. Rather, this figure is aimed at developing a common set of terms to describe the specificity of given concept relative to the text at hand. Knowledge that comprises topic-specific information to one text may reflect only subject-level knowledge for another text. That is, prior knowledge is not statically general or specific. Rather, the specificity of prior knowledge is related to the relevancy of the information in the reading task. One might consider identifying the specificity and relevance of particular information as addressing the question “in which classes might a student read this text? What other knowledge would be relevant in this course?”. Also note that knowledge may also belong to multiple categories of a broader level (e.g., subject or domain knowledge). For example, Figure 2 includes the implementation and subsequent repeal of the “Don’t Ask, Don’t Tell” (DADT) policy . DADT is listed under the subject of civil rights history and sub-subject of LGBTQIA+ issues. However, given that the policy was related to military service, it is also relevant to the subject of military history. Thus, the relevant subject knowledge test would depend on the context of the targeted material rather than on a rigid taxonomy.
 

Note: This taxonomy is neither definitive nor exhaustive. The intent is to provide some examples for how researchers and educators can begin to conceptualize, discuss, and evaluate these levels 
As a means of illustrating the utility of the MDK-C taxonomy, we examined five studies that yielded seemingly inconsistent findings regarding the unique contributions of more general knowledge as compared to more specific knowledge (Alexander et al., 1994; McCarthy et al., 2018b, 2019; O’Reilly et al., 2019b; Ozuru et al., 2009; Table 1). These studies were selected because they include multiple prior knowledge tests explicitly designed to measure multiple levels of specificity. Alexander and colleagues (1994) reported that students’ domain knowledge was more strongly correlated with comprehension (r = .30-.32) as compared to topic-specific knowledge (r = .23-.26). Similarly, Ozuru et al. (2009) reported that a general measure of biology knowledge was a stronger predictor of comprehension test performance (r = 0.48) than topic-specific knowledge (r = 0.32). By contrast, O’Reilly and colleagues (2019b) reported findings indicating that topic knowledge was more important than more general knowledge. Specifically, they asked participants to complete domain knowledge tests (science, history) as well as more text-specific prior knowledge tests. Both general history and general science test scores were predictive of comprehension performance, supporting the notion that general world knowledge is related to reading comprehension. However, when topic-specific prior knowledge scores were included in the regression models, the topic-specific knowledge tests emerged as the only significant predictor of comprehension score. Similarly, McCarthy and colleagues (2019) found (using linear mixed effects models) that domain knowledge, on its own, significantly predicted comprehension performance (β = .23-.53), but adding topic-specific knowledge scores to the model increased model fit. Most importantly, the best-fitting model revealed that only topic-specific knowledge was a significant predictor (β =.37-63). Findings reported by McCarthy and colleagues (2018b) further complicate the story. In this study, participants completed both general and specific knowledge tests prior to completing a comprehension task about invasive species. Of particular interest is that this study reported learning gains from completing a scenario-based science comprehension task, in contrast to a post-reading comprehension test as the outcome. Neither general knowledge (ecology) nor topic (invasive species) prior knowledge test scores predicted learning gains. However, these effects were qualified by a significant general-by-specific knowledge interaction, such that readers with high general knowledge, but low specific knowledge demonstrated the most robust learning gains from the comprehension task compared to those with other combinations of knowledge. 
Based on the findings in each of the aforementioned studies, it seems that there is discrepancy regarding whether more or less specific prior knowledge is important. However, a closer inspection of these studies suggests that the distinction between the more general and more specific knowledge is not the same across studies. Alexander and colleagues (1994) identified physics as the domain and knowledge of the quark as topic-specific knowledge. In contrast, Ozuru et al. (2009) examined biology knowledge and topic-specific knowledge, under the assumption that both of these fell under the larger domain of science. The term subject-matter knowledge (Alexander et al., 1991) is often used to indicate knowledge that is more specific than general knowledge, but has also been used interchangeably with domain knowledge (see Alexander, 1992). The general measure in O’Reilly et al. (2019b) is reflective of a broad domain (science, history), while McCarthy et al. (2018b) leveraged an ecology test as the more general test. We hypothesized that the differences in findings across these studies may be attributable to differences across definitions in levels of specificity. Hence, in Table 1 we recategorized the prior knowledge assessments in these studies using the taxonomic labels  proposed in the MDK-C framework with asterisks indicating which test was more strongly related to comprehension test performance. This recategorization reveals striking consistencies among the five studies. By using a common taxonomy, this albeit limited body of work suggests that knowledge closest to the midrange of specificity is more predictive of comprehension test performance than assessments that measure very broad or very specific prior knowledge. 


One theoretical explanation is that this degree of specificity is sufficiently proximal to the text to quickly activate relevant knowledge that may aid in inference generation, but also sufficiently general to provide a means of quickly organizing the new information (see McCarthy et al., 2018b). An alternative, statistical, explanation is that there must be variance in a measure for it to have predictive value. If measures are too specific with respect to the topic, there may be few readers who have this knowledge. If there is not enough variability in the sample, then it is unlikely to correlate with comprehension performance. A more thorough systematic investigation of the specificity of prior knowledge and its relation to comprehension is needed to more precisely discriminate between these two explanations and to flesh out comparisons across various combinations of levels of knowledge.
An overarching objective of the current work is to enhance researchers’ ability to triangulate where different prior knowledge assessment items lie along a continuum of specificity. Otherwise, the terms that researchers choose to use are generally constrained by their own lens, which may only consider a small range along within the broader spectrum of potential differences. A taxonomy provides a means of conceptualizing these differences so that researchers and educators are able to more precisely and consistently consider the impact of knowledge specificity on comprehension. 
Coherence
The final dimension of the MDK-C framework is coherence. Coherence refers to the quality of prior knowledge in terms of its interconnectedness. This dimension builds upon notions of the “structure” of knowledge. It is also similar to notions of “depth” of knowledge. However, when someone is thought to have a strong depth of knowledge about a topic, this is often taken to mean that they have a lot of very specific knowledge, which conflates both amount and specificity without comment on the degree to which the topic knowledge is integrated within itself or with a larger subset of knowledge (Wineburg, 1997). We have adopted the term coherence to emphasize the focus on interconnectedness and to reflect the primary role that coherence plays in the construction of an elaborated mental model (e.g., W. Kintsch, 1988) and in the development of expertise (e.g., Alexander, 2004). Expertise is defined not only by the quantity of knowledge that one possesses, but by the ways in which the knowledge is connected (Alexander, 2004; Bédard & Chi, 1992; Bereiter & Scardamalia, 1986). For example, the model of domain learning (MDL; Alexander, 2003, 2004) proposes that learners move from novice to expert through a series of qualitative, structural changes to their domain-relevant knowledge. A novice, or someone in the acclimation stage, possesses limited, disparate knowledge. When this knowledge becomes more interconnected, the learner is said to have moved to the competency stage. Thus, competency is marked not only by an increase in the amount of knowledge the learner possesses, but the coherence of that knowledge in memory. 
Similarly discourse comprehension theories suggest that a quality mental model is not merely a function of how much knowledge is available, but the structure and interconnectedness of that information in memory (W. Kintsch, 1988; McNamara & Magliano, 2009). Consider again, the connectionist assumptions in text processing research. Imagine two readers with the same amount of knowledge. If one of the readers has more connections between pieces of knowledge, that reader is going to be able to activate more information and do so with less effort than the reader whose knowledge is more disparate. The reader with the more connected, or coherent, mental model will have not only more information available to generate inferences, but also more resources available to attend to other aspects of the comprehension process. Indeed, a simulation study (McNamara, 1997) validated these assumptions. This study showed that a coherent knowledge base, as represented by a mental model with more links between ideas, led to increased production of inferences, which, in turn, increased the speed at which information was processed, validated, and integrated into memory. 
Most studies of text comprehension consider the coherence of a reader’s mental model after reading, but few studies have explicitly examined coherence of a mental model prior to reading. There are, however, several studies that have evaluated the quality of knowledge in terms of its structure and organization. While these ideas are not synonymous with coherence, these studies offer preliminary evidence and serve as a foundation upon which researchers can better understand how the coherence of a mental model influences text comprehension. Langer (1980, 1984) categorized students’ free associations in response to text-relevant keywords into levels of organization: highly organized, partially organized, and diffusely organized. Students with highly organized knowledge demonstrated superior recall (Langer & Nicolich, 1981) and passage comprehension, even when controlling for IQ and general reading skill (Langer, 1984). Although these levels are relatively coarse (e.g., ordinal), these results provide initial empirical evidence for the role that the organization or structure of knowledge influences comprehension. Instead of an open-ended verbal task, McNamara and colleagues (1996) used a key term sorting task to evaluate the organization of a reader’s knowledge. Readers’ organization of the terms changed from pre-reading to post-reading, suggesting that reading the text influenced the way they connected the information. While this particular study did not directly examine how different knowledge organization profiles influence processing and comprehension, this method of knowledge assessment may be potentially fruitful for further examination into the dimension of coherence.
A recent study by McCarthy and colleagues (2019) was designed to probe the coherence of knowledge more directly. The development of the prior knowledge tests was founded in the distinctions between textbase and inference items on comprehension tests. The researchers developed items that reflected both basic (textbase) and inference questions as a means of evaluating the coherence of the reader’s prior mental model. Basic prior knowledge items reflected factual knowledge (e.g., “Which of these is the correct definition of photosynthesis?”). Rather than finding the right answer in the text, the reader needed to “find” the right answer in their memory. The inference prior knowledge items required students to make connections across different pieces of knowledge or apply knowledge to a novel situation (e.g., “Why do gram-positive bacteria stain purple?”). The underlying assumption was that readers with more coherent mental models would have knowledge organized in ways that afford such inferences. After these prior knowledge tests, students completed a scenario-based comprehension assessment designed to evaluate higher-order comprehension. Consistent with the notion that a more coherent knowledge base is supportive of comprehension, analyses revealed that scores on the inference prior knowledge items provided stronger predictors of comprehension test scores than basic prior knowledge. This study provides strong preliminary evidence of the value of exploring prior knowledge in terms of coherence, but more empirical work must be done to replicate and extend these findings. In particular, it will be important to explore how these different types of items are related to the processes or strategies that occur during reading and how these concurrent processes, in turn, influence comprehension performance. 
Another potentially promising way of evaluating the coherence of prior knowledge is through computational linguistic analysis. This approach borrows from methods used to evaluate post-reading mental models. Recent studies using natural language processing have shown that linguistic features of students' responses can be used to infer the coherence of their mental model. These studies demonstrate that cohesion (i.e., overlapping ideas from sentence to sentence or paragraph to paragraph) evidenced in students’ concurrent verbal protocols or in post-reading summaries or explanations is predictive of their inference-level comprehension (e.g., Graesser et al., 2007; McCarthy & Hinze, 2019; Varner et al., 2013). That is, students who produce more cohesive open-ended responses can be assumed to have a more coherent understanding of the content. Using this same approach, one potential means of examining a reader’s prior mental model is to ask students to write open-ended explanations of a topic before reading and then using computational linguistic tools (e.g., Coh-Metrix, McNamara et al., 2014; Tool for the Automated Assessment of Cohesion, Crossley et al., 2016, 2019) to automatically evaluate the cohesion of this response. On the one hand, open-ended assessments of this nature are easier to create than close-ended multiple-choice tests that need to be carefully crafted, piloted, and refined to establish validity and reliability. On the other hand, open-ended prompts may provide only limited cues – students many bring up some, but not most or all of their relevant knowledge. Asking a student to write “everything you know about plants” may not bring to bear the specific knowledge of interest. Open-ended responses are also reliant on composition skill in ways that close-ended tasks are not. Thus, this may be a viable approach, but more work is needed to evaluate the extent that open-ended responses can capture the quality or coherence of the reader’s prior mental model and how evidence of coherence in a prior mental model influences concurrent processes and downstream comprehension.
Interactivity across Dimensions
	Prior knowledge can and should be described in terms of multiple dimensions and, critically, these dimensions depend upon each other. Imagine, for example, two students who both obtain a 20% on a prior knowledge assessment about the physiology of the eye. Student A has little knowledge about physiology in particular, but a relatively coherent understanding of other topics related to science. Student B, on the other hand, has little knowledge of physiology and little experience with other fields of science, including multiple misconceptions. Despite performing equally poorly on this topic-specific prior knowledge test, it is likely that Student A will have greater success in learning from the text by drawing upon other knowledge. However, without evaluating multiple aspects of these students’ knowledge, it would be difficult to discern why these differences emerged. As a second example, a Jeopardy! champion may clear an entire category about medical terminology. Despite knowing “a lot” about medicine, most people would not trust this polymath to accurately diagnosis and treat a complicated medical condition because a medical expert’s knowledge is not only great in quantity, but also highly coherent. A student may have watched movies like Limitless or Lucy and have learned the (inaccurate) “fact” that people use only 10% of their brain. This piece of knowledge is likely to impede the student’s understanding of a neuroscience text. However, this fact is likely too topic-specific to have a negative impact on the student’s understanding of physiology more generally. While these examples do not provide all possible combinations of different dimensions of knowledge, they serve as a way to highlight the need for a richer, more nuanced way of describing the qualities of that knowledge and how different aspects of knowledge might support, hinder, or augment each other in service of developing a mental model.
In order to illustrate the four dimensions in tandem, Figure 3 shows a simple hypothetical reader’s mental model prior to reading a text about the human eye. The amount of knowledge in the knowledge base is reflected in the number of nodes and connections. The accuracy of the knowledge is reflected in the color of the nodes and connections. In this example, accurate knowledge is represented in green circles while inaccurate knowledge is represented in red diamonds. Inaccurate connections are represented in red dashed lines. The slight variation in color brightness represents the continuous nature of accuracy, in which darker red can be interpreted as knowledge that is more inaccurate than a lighter shade of red. The specificity of the reader’s knowledge is represented here as a cone – knowledge that is more specific is represented as literally closer to the text, while knowledge that is more general is both farther from the text and broader. Finally, the coherence of the reader’s knowledge is reflected in the connections. In this diagram, the reader’s “mid-distance” subject knowledge is relatively coherent with many connections between the ideas, whereas the more general domain knowledge is more sparse. Taken as a whole, this reader’s prior knowledge could be described as relatively sparse (amount), but largely accurate. This reader’s knowledge is somewhat coherent and more strongly so for information that is related to the broader subject, rather than the specific topic of the text.


	Figure 3 is not intended as a literal representation of how information is positioned in space relative to a physical text nor does it reflect precisely how knowledge is represented in memory. However, this figure allows the reader to view the four dimensions simultaneously on a two-dimensional page. The figure serves to highlights a critical contribution of the MDK-C framework. First, a “snapshot” on any one dimension overlooks important differences that a reader may have in other dimension. Second, these dimensions are interrelated. Our argument is that each of these dimensions can depend on and affect the others. That is, if a reader were to have no knowledge (amount) about a given topic (specificity), then it would not be possible to assess the coherence or accuracy of that knowledge. However, these dimensions also assume that there are almost no plausible situations in a which a reader has no amount of relevant knowledge. For example, Shapiro (2004) demonstrated that, even when reading about an entirely fictitious scenario, readers still draw upon broad, general world knowledge to make sense of what they read. Thus, the question that researchers and educators must ask is not whether or not students have knowledge, but rather, what is the nature of the knowledge that they possess? 
Implications
The MDK-C framework identifies four dimensions (i.e., amount, accuracy, specificity, coherence) that have emerged to describe the nature of prior content knowledge. Empirical evidence suggests that each dimension has impact on comprehension and that there are likely important interactions across the different dimensions. However, the ambiguity in the current body of work and in the assessment of prior knowledge make it difficult to discern the unique contribution of any one dimension or the potential interactivity across the dimensions. Thus, a central purpose of the MDK-C framework is to push researchers and educators to move beyond including a measure of prior knowledge toward a more nuanced consideration of what dimension(s) of a reader’s knowledge might be more critical for a given comprehension task. Such research investigations will serve to help educators deliver more effective instruction and individualized support.
The empirical evidence for the MDK-C framework and the limitations within this current evidence yield three broad implications. The first implication of the MDK-C framework is that an adequate understanding of the relations between prior knowledge and comprehension requires assessing more than one dimension of prior knowledge. The second implication is that researchers must be more intentional about the design of prior knowledge assessments and the operationalization of constructs. The third implication is that the multiple dimensions of knowledge occur in tandem and the effect of one dimension of knowledge may depend on another. 
Assessing Multiple Dimensions
If prior knowledge is multidimensional, and if the objective is to understand the relations between prior knowledge and comprehension, then investigators and practitioners should consider multiple aspects of readers’ prior knowledge. The implication emerging from MDK-C is not that researchers must evaluate all dimensions in every study, but rather that they should be thoughtful about which dimensions of knowledge will be most relevant to a given research objective and to justify why the assessment they have used is appropriate for the given dimension(s) in the given context. Our intent is not to argue that there is one right way to assess prior knowledge, but rather that researchers and educators must be thoughtful and explicit about how these different dimensions relate to outcomes of interest. 
From a construct standpoint, MDK-C implies that each dimension is largely orthogonal to the others. Although some dimensions may be more closely linked than others (e.g., amount is, in part, prerequisite for having accurate knowledge), MDK-C assumes that each dimension can be reliably distinguished from another. There exist only a small number of studies that have explicitly explored these dimensions. Thus, one objective of this paper is to generate a call to action for more research to systematically test the validity of these dimensions. One step in this direction is the taxonomy presented in Figure 2 through which researchers can label levels of specificity. We anticipate refinement of this taxonomy, but it serves as an initial step toward clearer articulation of what is meant by “general” or “specific” knowledge. Similar work should be done to better measure and understand the coherence dimension of prior knowledge. We have suggested using close-ended inference-based conceptual or application questions in prior knowledge assessments that evaluate the degree to which different ideas are connected (e.g., McCarthy et al., 2019) as well as the leveraging of state-of-the-art natural language processing to measure coherence in open-ended responses. However, both of these approaches will require additional research to examine important considerations, in particular, the reliability and validity of these types of measures. 
The Need to Operationalize
	The need to operationalize constructs is an important issue of validity in any study of psychology and learning. The examination of empirical evidence within the previous section suggests that clearer operationalizations of prior knowledge and dimensions of prior knowledge will support greater specificity of predictions and interpretations of results. A systematic review by Dochy and colleagues (1999) demonstrated that conflicting findings across prior knowledge studies were reconciled through closer examination of the study methodologies (e.g., self-report versus multiple-choice; short lab studies versus semester long effects). The MDK-C framework echoes these findings and extends the work by Dochy and colleagues by encouraging researchers to more carefully consider not only how to measure prior knowledge, but also how to measure different aspects of prior knowledge. Researchers are often constrained by the practical aspects of data collection. Thus, it may not be feasible to include multiple, thorough prior knowledge tests. By having a stronger sense of what dimensions are of most importance, researchers can more easily select or develop measures that target critical dimensions of prior knowledge. Only through clearer evaluations of single dimensions can there be more systematic investigations of possible combinations. 
Dimension Interdependencies
It is not only of concern that researchers sometimes underspecify dimensions, but also that researchers rarely consider that these dimensions coexist within the reader’s knowledge simultaneously. The third implication of the MDK-C framework is that the effect of one dimension of knowledge may depend on another. Too often, researchers and educators implicitly measure one dimension of prior knowledge while ignoring others. Or, researchers use measures that conflate multiple dimensions, making it difficult to delineate the unique or combined contributions of the different dimensions. While it may be difficult, if not impossible, to fully disentangle each dimension, researchers should be more intentional with respect to which dimensions they are evaluating. Through these assessments, the boundaries and interdependencies of these dimensions can be more fully elucidated.
Given four dimensions, there are an extensive number of combinations that any one reader might possess. It may be of value to test all possible combinations, although some combinations are likely more theoretically-motivated than others. More practically, however, MDK-C is intended to encourage researchers to consider and evaluate which combinations of dimensions are likely to be most common and which are most critical to address. For example, a number of misconceptions in a reader’ prior knowledge may be unfortunate, but they may not impede the reader’s comprehension if they are not well-connected to the specific topic of the text. By contrast, a single, well-entrenched misconception may be enough to derail learning of a new concept. In this case, not all dimensions are of equal importance for the given learning task. From a research standpoint, having a priori predictions about which dimensions and which combinations of dimensions are most aligned with relevant comprehension processes and outcomes can ensure that the limited time available to measure prior knowledge can be used efficiently. From an applied standpoint, identifying common “prior knowledge profiles” of readers for given learning tasks could support the development of targeted activities and interventions. 
Limitations and Future Directions	
Little research has been done to directly isolate and measure the four dimensions of the MDK-C framework. Our examination of these dimensions independently and in tandem was predominantly based on our own posthoc analyses. In order to more effectively test the assumptions of the framework, more research is needed with explicit consideration of these dimensions during study development. If researchers are more explicit about which dimension(s) they intend to evaluate and why, they can develop measures that can better assess the construct validity of each dimension. More intentional consideration of which dimension is relevant to the given research question or learning objective, will afford systematic evaluation, refinement, and extension of the framework. Such intentionality will help to move the field beyond prior knowledge-as-covariate or prior knowledge-as-moderator to a more nuanced understanding and sensitivity to how individual differences across readers can influence comprehension. 
One practical constraint is the timing of the prior knowledge assessment. Any assessment of what one knows prior to the activity can serve to prime the learner to engage with the material in ways that they may not have otherwise. Alternatively, a test administered afterwards may reflect information learned during the task rather than true prior knowledge. While this is a limitation of prior knowledge assessments writ large, any suggestion to delve into more specific questions prior to learning runs the risk of further biasing performance. Thus, in the development of assessments relevant to the MDK-C dimensions, researchers must also consider when such assessments are most appropriate in the context of their own research questions or intervention purposes.
The four dimensions of prior content knowledge outlined in the present iteration of the framework reflect the most empirically-supported dimensions. There are, however, other facets or types of knowledge that were not included but may play a role in text comprehension. One possible additional dimension is the explicitness of knowledge. The focus of prior knowledge in comprehension studies is often on explicit content knowledge. This is in part because declarative knowledge is predominantly explicit (Dochy & Alexander, 1995). However, there is evidence to suggest that some content knowledge is more implicit or tacit and that this type of knowledge can influence processing. For example, readers are generally able to identify a text’s genre with little effort, but struggle to articulate what parts of the text or their knowledge they are using to make this determination (K. McCarthy, 2020; P. M. McCarthy & McNamara, 2007; Zeitz, 1994). Thus, while they lack explicit genre or disciplinary knowledge, this knowledge can influence readers’ goals and expectations and the comprehension processes in service of those goals (e.g., Zwaan, 1994). However, because tacit knowledge is more difficult to assess, there is far less research on the role of tacit knowledge in text comprehension research as compared to explicit knowledge. These findings also emphasize the need to further investigate how disciplinary knowledge fits amongst these dimensions, not just in terms of discipline-specific content knowledge, but also knowledge about the disciplines themselves (i.e., knowledge about appropriate epistemic frames and learning objectives; Goldman et al., 2016). Such knowledge bridges explicit declarative and procedural knowledge with epistemic knowledge and beliefs. More rigorous examining of multiple dimensions and types of knowledge at play in a learning task can further elucidate relations between different types of knowledge and other individual differences. 
Another potential limitation in the MDK-C framework is the absence of beliefs as a dimension of prior content knowledge. Readers can have beliefs about particular topics, but they may also have broader beliefs about knowledge in general or how knowledge is derived in particular domains (e.g., epistemic beliefs). There is lack of consensus around the relation between the constructs of knowledge and beliefs. Some argue that beliefs reflect a subset of knowledge, while others suggest that beliefs and knowledge reflect separate, but overlapping constructs (Alexander & Dochy, 1995; Murphy et al., 2012; Murphy & Mason, 2006; Southerland et al., 2001). Based on much reflection, we concluded that there is undoubtable overlap between beliefs and knowledge, but that there also remains utility in considering beliefs as a separate construct from prior concept knowledge. A number of studies explore the role of epistemic beliefs as well as prior knowledge (e.g., Baytelman et al., 2020; Ferguson & Bråten, 2013). For example, Mason and colleagues have demonstrated that readers with more sophisticated epistemic beliefs are more likely to engage in conceptual change, suggesting that these two constructs are related, but at least to some degree, independent (Mason et al., 2008; Mason & Gava, 2007). Beliefs also reflect aspects of “warm” processes that are often ignored by text comprehension research that has traditionally focused on cold cognitive processes. However, recent work, such as the Process, Emotion, Task (PET) framework (Bohn-Gettler, 2019) and Cognitive Affective Engagement Model (CAEM; List & Alexander, 2017) reflect a growing need for researchers to consider both affective and cognitive influences on comprehension. The affective quality of beliefs may or may not be present in the types of prior content knowledge described previously. Operationalizing beliefs as a separate construct that should be considered in conjunction with prior content knowledge affords the opportunity for a more systematic investigation of the relations between affective and cognitive processes. It will be of further interest to examine how these prior knowledge profiles might interact with other individual differences in both cognitive (e.g., reading skill) and noncognitive (e.g., motivation) aspects of the reader.
A final future direction of value will be to share our understanding of multidimensional knowledge with instructors and learners. One important implication for instructors is that there needs to be greater support provided to students beyond mere exposure. That is, providing a list of vocabulary words or a background reading prior to learning may be helpful, but ultimately superficial. It would be of greater value to work to develop relations between ideas that are very closely related (i.e., topic) as well as drawing on broader (domain) or general knowledge. The MDK-C Framework emphasizes the need for thoughtful classroom-based assessments. A teacher may only have a few minutes to assess students’ prior knowledge before beginning a new unit. Completing one longer subject or subdomain prior knowledge test may afford greater validity and reliability than a few scattered topic questions, but if these questions are largely basic, superficial items, they may overlook the coherence of the mental model. One benefit of classroom work is that instructors can use multiple assessments at various points throughout the semester or year. That is, one might imagine a beginning-of-semester general and domain knowledge test to get a broad sense of students’ prior knowledge. This can then be supplemented by shorter, more pointed assessments that can evaluate both specificity and coherence of knowledge as well as potentially detect evidence of inaccurate information or connections. If instructors are able to more accurately “diagnose” a student’s prior knowledge profile, they can provide more targeted support. For example, students with relatively low knowledge of a particular topic might be prompted to draw upon extant broader knowledge, while students with specific topic knowledge might be encouraged to focus on connections between ideas to increase coherence of these ideas. It may also be of value to help students to “make visible” their own knowledge along these dimensions. One might imagine an intervention where educators help students more explicitly recognize the nature of their knowledge. This could help them identify what knowledge they can leverage in a particular situation and what knowledge they may need to seek in order to fully make sense of a text.
Conclusion
In conclusion, prior knowledge impacts comprehension in a variety of ways. Prior knowledge measures are derived by different researchers for different purposes and constrained by different theoretical and methodological factors. As a result, there is consensus that prior knowledge matters in text comprehension: if a reader already knows almost everything in the text, then comprehending the text is a very different task than it is for the reader who knows almost none of the ideas presented in the text. While this is, in itself, crucial to recognize, this inherent relationship between the reader and the text is more complex than simply more or less. Moreover, there remain a number of inconsistencies and ambiguities around notions of how prior knowledge influences text comprehension. The Multidimensional Knowledge in Text Comprehension (MDK-C) framework affords the opportunity to better characterize the nature of readers’ prior knowledge and what aspects of that knowledge are most critical for a given learning context. It our hope that the framework encourages researchers and educators to be more intentional and explicit about how and why knowledge impacts comprehension, and in turn, rationales for selecting or developing particular prior knowledge assessments. Through these more systematic investigations, researchers can identify how prior knowledge influences different processes involved in comprehension and, in turn, educators can more accurately predict and provide support for comprehension issues for individual learners. 
 
