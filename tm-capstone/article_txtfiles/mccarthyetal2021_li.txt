On the basis of source: Impacts of individual differences on integrated reading and writing tasks

Abstract  
Few studies have explored how general skills in both reading and writing influence performance on integrated, source-based writing. The goal of the present study was to consider the relative contributions of reading and writing ability on multiple-document integrative reading and writing tasks. Students in the U.S. (n = 94) completed two tasks in which they read text sets about a socioscientific issue, generated constructed responses while reading, and then composed integrated essays. They also completed individual difference measures (general knowledge, reading skill, reading strategy use) and wrote independent essays to assess their writing ability. Mixed effect models revealed that general knowledge and reading skills contributed to integrated essay performance, but that once general writing ability was entered into the model, it became the strongest predictor of integrated writing scores. These results suggest the need for deeper consideration of the role of writing skills in integrated reading and writing tasks. 
  
1. Introduction 
A variety of academic and professional activities rely on source-based reading and writing. Source-based tasks involve writing about information gathered from an existing source or set of sources. For example, an elementary school student may write a book report about a novel or a Supreme Court justice might draw upon old court documents and laws to compose a dissenting opinion. In the current study, we are specifically interested in source-based writing tasks. In these tasks, students are asked to read multiple documents from varying sources and to use the information in the documents to compose an essay intended to summarize, describe, inform, or persuade. These multiple-document integrated reading and writing tasks (henceforth, integrated writing tasks) have become increasingly common. This is, in part, because of the increased availability of (often conflicting) information on the internet which has influenced the ways in which we encounter and use information both in and out of the classroom (Magliano et 
al., 2018).    
In the classroom, integrated writing tasks are sometimes used to provide writing instruction or assess writing skills. They are also used in classrooms and research studies as a means to assess learning outcomes, often to approximate the extent to which a student has understood the topic or phenomena. In the latter case, integrated writing tasks are intended as a measure of comprehension and learning, and the extent to which writing skills play a role is often ignored. This is problematic, given that source-based integrated writing tasks rely on both lower-level and higher-order literacy skills related to comprehension as well as a number of skills related to production including composition and argumentation (e.g., Spivey & King, 1989; List & Alexander, 2019).  
Understanding how various literacy skills and knowledge come together in an integrated writing task is necessary to provide more targeted support for students. While there is a growing body of research examining multiple-document literacies (see Braasch et al., 2018; Van Meter et al., 2020), multiple gaps in the literature remain. These gaps point to a need to devote greater consideration toward understanding the potential roles of individual differences in these integrated writing tasks. We examine these issues below. 
1.1 Theories of Comprehension 
In the majority of research on multiple-document comprehension, the researchers’ objective is to assess the degree to which the student understands and integrates information from the documents (Goldman & Scardamalia, 2013; Perfetti et al., 1999; Wiley et al., 2017). 
Usually, the students’ understanding is assessed via an integrated writing task (Primor & Katzir, 2018). As such, the majority of the research and methodologies are guided by assumptions inherent to models and theories of how individuals comprehend discourse. Discourse comprehension theories posit that readers construct a mental model of text information as they read. A coherent mental model is one that accurately represents the information that was explicitly conveyed, inferences generated, and the relations between these sources of information 
(McNamara & Magliano, 2009). For example, the most prevalent theory of discourse comprehension, the Construction-Integration model, specifies that readers construct a multi-level mental model during reading (Kintsch, 1988). The surface code contains the exact words and sentence structures from the text, whereas the textbase reflects a more abstracted gist of the text information. Finally, the situation model is composed of the inferences that connect different ideas from across the text as well as relevant information from the reader’s prior knowledge. Thus, a mental model that is more elaborated and well-integrated is likely to afford deeper understanding and retention. 
Much of this early work in discourse comprehension was aimed at explaining how a reader understood a single text. That is, many of these studies involve participants reading a text and answering comprehension questions that probe for the structure of the reader’s mental model 
(see McCarthy et al., 2018). More recently, however, researchers have acknowledged that these “text-then-test” reading tasks do not reflect the types of comprehension tasks that learners engage in throughout their daily lives (Magliano et al., 2018; Sabatini et al., 2019). Thanks in large part to the internet and growing demands for more complex reasoning skills, successful learners are often required to engage in multiple-document reading and writing tasks (Britt et al. 2012, 2018; Bråten & Braasch, 2017; OECD, 2019). In these tasks, students still need to form connections within a text (i.e., intra-textual inferences), but they must also generate inter-textual inferences that connect information across texts (Salmerón et al., 2010). Often these texts might be written for different purposes and may also include information that is not only complementary, but also discrepant, contradictory, or otherwise conflicting (Braasch & Bråten, 2017; Scharrer & Salmerón, 2016). As a result, readers often need to maintain an idea of “who said what” along with their general representations of the texts. Thus, theories of multipledocument comprehension do not aim to refute single text comprehension models, but rather to extend them to consider additional factors relevant to how we represent and integrate multiple documents while reading. 
One of the most influential models of multiple-document comprehension is the 
Documents Model Framework (DMF; Britt et al., 1999, 2012; Perfetti et al., 1999). The DMF extends the description of mental model building through the addition of both an integrated mental model of the global situation described in multiple texts and an intertext model which relies on the reader’s attention to source features (e.g., author, date of publication, publication type). More recently, the RESOLV model (Britt et al., 2018; Rouet et al., 2017) has been developed to expand on the DMF to provide a more general explanation of goal-driven reading. 
In addition to the integrated mental model and the intertext model described in the DMF, the RESOLV model includes two additional representations. The context model includes a representation of the current reading situation and task requirements while the task model includes a representation of the reader’s goals and plans. As a reader progresses through a multiple-document inquiry task, these models help the learner to monitor the completeness of their mental representation and to evaluate the quality of their comprehension in light of task goals. 
Both single and multiple-document comprehension theories highlight the importance of individual differences. Individual differences such as prior knowledge, reading skill (e.g., Ozuru et al., 2009), and strategy use (e.g., Anmarkrud et al., 2013) are strong predictors of comprehension performance. Research on prior knowledge shows that more knowledgeable readers can more quickly access and organize information and are better able to generate relevant inferences that support the development of a coherent mental model (see Dochy et al. 1999). Research has also shown that general reading skills, as measured by standardized measures such as the Gates-MacGinitie Reading Test and the Nelson-Denny are predictive of performance on content-based, task-oriented reading (e.g., learning from a text; Ozuru et al., 2009). More successful readers tend to engage in more effective metacognitive and comprehension strategies (e.g., Anmarkrud et al., 2013; Cote et al., 1998; Cromley & Azevedo, 2007; McNamara, 2007). 
Interestingly, explicit knowledge and use of metacognitive strategies have been shown to be particularly important for successful multiple-documents comprehension (Karimi, 2015).  
Related to strategy use, a number of single and multiple-document comprehension studies have explored the extent to which prompting students to use different comprehension strategies can impact how they learn from text. Research from think-aloud studies (e.g., Coté et al., 1998; Wolfe & Goldman, 2005; Wineburg, 1991) indicate that more skilled readers tend to engage in more active and integrative strategies such as self-explanation and source evaluation. Further, intervention work has shown that supporting students’ use of these strategies can lead to better comprehension, especially for less skilled and less knowledgeable readers (e.g., Britt & Aglinkas, 2002; McNamara et al., 2004). In the current work, we provide a brief constructed response instructional prompt to bias the readers toward using these strategies. While such prompts are less effective than longer-term interventions, we can use this manipulation to further explore the role of strategy use on the quality of integrated writing. 
Research in multiple-document comprehension often relies on a written product to assess the extent to which readers have successfully processed, integrated, and comprehended the documents. After reading, participants are asked to write a summary, description, or argument and the written product is evaluated for evidence of source use (referencing the source information of the document), content coverage, and the degree of integration. Integration is measured in a number of ways. A review of multiple-document integration tasks (Primor & Katzir, 2018) revealed that the majority of studies evaluate students’ written products for the degree of content coverage, source use, and the extent to which participants address conflicting viewpoints. Few, if any, of these studies examine aspects of writing such as mechanics or style. 
This is, in large part because this area of work tends to use the essay as a window into comprehension, rather than as a task unto itself (see also Wiley et al., 2018).  As a result, these studies have yielded a number of insights into the complex processes involved in multipledocument comprehension, but have far less to contribute to an understanding of integrated writing. For example, the Integrated Framework of Multiple Texts (IF-MT; List & Alexander, 2019) outlines three stages: preparation, execution, and production. The execution stage includes identification of the behavioral (e.g., search, inter- and intra-textual navigation), metacognitive (e.g., monitoring), and cognitive skills and strategies (e.g., knowledge activation, integration, perspective taking) involved in multiple-document processing. By contrast, the production stage is far less developed. The framework identifies the types of products (e.g., arguments, opinions, report), but does not explore or identify the skills and strategies involved in the writing process nor the extent to which these processes are similar or different from the comprehension process.  
Generally speaking, the essays in these studies are not explicitly scored for the quality of writing, but rather on the degree of key content coverage (e.g., Wiley et al., 2009, 2014), the number of sources referenced (e.g., Bråten et al., 2014; Britt & Aglinskas, 2002), the degree of integration (e.g., List et al., 2019) and organization (e.g., Wiley & Voss, 1996, 1999), or the quality of argumentation (e.g., Anmarkrud et al., 2013; Bråten et al., 2014). Even though writing quality is not the explicit focus of these measures, it is likely that students’ ability to represent this information is, at least in part, dependent on their writing skills. Thus, a limitation in the current body of work is that performance on integrated essays is often assumed to be a measure of comprehension processes; however, there is little recognition that differences in comprehension may be obscured by differences in production.  
1.2 Theories of Writing 
Theories of discourse production exist largely separately from theories of discourse comprehension, although both processes require multiple strategies and sources of knowledge to complete. At its simplest, discourse production in the form of writing requires an individual to transcribe their thoughts into a written form with minimal spelling and mechanical errors. Highquality writing, however, is more than simply a transcription of thought -- indeed, successful writing transforms thought into discourse that is understandable by the intended reader (Graham, 2018). Theoretical models have delineated the writing process into three primary components: planning, translating, and reviewing (Flower & Hayes, 1981; see also Hayes, 1996, 2012). In the planning phase, a writer generates and organizes their ideas. These ideas are then translated into words on a page and reviewed to determine whether they have achieved the writer’s goals and may subsequently be revised. Importantly, theories dictate that this is a nonlinear process; thus, a writer may frequently switch amongst these processes based on a variety of factors due to constraints of the task or the individual. 
 	Skilled writing is not only marked by differences in content, but also in style. Skilled writers produce longer essays (Ferrari et al., 1998; Haswell, 2000; McNamara et al., 2010; McNamara et al., 2013) with fewer mechanical errors (Ferrari et al., 1998). They also tend to use more abstract language (Crossley et al., 2011b; McNamara et al., 2010; McNamara et al., 2013) and more complex syntax (McCutchen et al., 1994). The individual differences that relate to skilled writing encompass a wide range of features. A number of models of writing proficiency have attempted to incorporate individual differences in their models ranging from world knowledge (McCutchen, 1986; Olinghouse et al., 2015) to reading comprehension (Allen et al., 2014; Fitzgerald & Shanahan, 2000; Tierney & Shanahan, 1991). Importantly, strong writers have a greater understanding of the writing process itself. Saddler and Graham (2007), for example, found that less skilled writers showed a weaker understanding of writing goals (d = 1.13), were less knowledgeable of the differences between high- and low-quality writing (d = .98), and had less knowledge of successful writing strategies (d = -1.10). These results are important, as they indicate that variability in knowledge of how to write can influence the quality of a written text. 
Generally speaking, the majority of work that has informed theories of writing has focused on independent essays in which writers are responding to a prompt by drawing upon their own knowledge and experience. However, consistent with the uptick in research on multiple-documents comprehension, there has been an increase in research on source-based writing and, in particular, multiple-document writing. Research in these integrated writing tasks (also referred to as hybrid or synthesis writing; e.g., Mateos et al., 2014; Spivey, 1997) have focused on the idea that writing tasks are not linear, but rather a recursive process in which the writer needs to move between processes. This is particularly important for integrated writing because the student needs to not only move between production processes, but also between reading and writing (Spivey, 1997; Spivey & King, 1989). Indeed, more recursive writers (e.g., those who go back-and-forth between sources and the essay) tend to write higher quality essays (e.g., Martinez et al., 2015; Vandermeulen et al., 2020).  In addition, integrated writing not only requires the planning, drafting, and revising identified in the Flowers and Hayes (1981) model, but also selecting information, organizing that information, and connecting the information (see van Ockenburg et al., 2019). Classroom intervention work has demonstrated that providing instruction and practice for these integrated writing strategies can improve integrated essay quality (e.g., Mateos et al., 2018; van Ockenburg et al., 2021; Wissinger et al., 2021). 
1.3 Individual Differences that Impact Integrated Writing 
As described earlier, high-quality integrated writing requires the reader to develop a coherent mental model of the text content (reading) and to convey that information in a coherent and organized manner (writing). Although there has been an increase in the research on integrated writing, there remains unknowns about the unique contributions of reading and writing skills on an individual’s ability to produce high quality integrated essays. A criticism of the work in multiple-documents comprehension research is the use of integrated writing as a measure of comprehension without considering the effects of the students’ general writing skill (e.g., Primor & Katzir, 2018; Wiley et al., 2018). Conversely, many studies collect performance on a standardized reading skill test (e.g., Gates-MacGinitie, Nelson Denny) as a control variable, but few have explored the effects of general reading skill(s) on integrated writing. One such study found that skilled readers showed improvement in all three integrated writing strategies, whereas less skilled readers were only able to improve in their ability to select (but not organize or integrate) relevant information in their post-intervention essays (e.g., De La Paz & Felton, 2010). These findings suggest that reading skill impacts writing quality, but it is unclear if these effects are related to issues in comprehension, production, or both.  
It is also important to consider that fundamental writing skills are crucial for producing any written product. For example, Du and List (2020) analyzed screen recordings of students completing an integrated writing task and found that the students spent very little time planning to write or revising their essay. The authors speculate that the lack of planning and revision behaviors in their data might reflect “more general difficulties with written response composition” (Du & List, 2020, pg. 18). One might imagine a student who was able to select and integrate important ideas into a coherent mental representation but struggled to convey their ideas clearly and coherently. Such a student would need different supports than a student who struggles to understand the texts. Thus, exploring the contributions of general reading and writing skill in an integrated writing task is an important endeavor for learning and instruction.   
1.4 The Current Study 
Integrated writing requires that the learner read and understand the source(s) and then also convey their ideas in an organized and coherent fashion. Thus, it is generally assumed that integrated essays require the coordination of literacy skills related to both comprehension and production. However, much of the research on multiple-document comprehension uses integrated writing as an outcome measure without considering how students’ writing skills might impact the essay. Thus, the current study examines the following question: To what extent does students’ more general writing ability predict performance on a multiple-document source-based integrated (reading and writing) essay above and beyond factors more directly related to comprehension (i.e., general knowledge, reading skill, and metacognitive reading strategies)?  	In this study, participants were asked to read multiple documents about a topic and then compose an integrated essay. To reduce the impact of a particular prompt or text set, participants completed two separate integrated writing tasks. Consistent with extant multiple-document research (e.g., Anmarkrud et al., 2014; de la Paz & Felton, 2010), the essays were evaluated on aspects of argumentation, source use and integration, and organization. These subscales have been used as evidence of comprehension in several studies (e.g., Bråten et al., 2014; Britt & Aglinskas, 2002, Wiley et al., 2009, 2014). In addition, the essays were evaluated for their general writing quality in terms of word choice, syntax, and spelling and mechanics. We then examined the extent to which performance on these integrated essays was related to individual differences in reading and writing. 
As part of a larger ongoing project, we prompted participants to generate constructed responses as they read and manipulated the constructed response prompt instruction so that some students were biased to engage in more effective comprehension strategies (self-explanation, source evaluation) as compared to a control (thinking aloud). Although strategy prompting can benefit comprehension and learning (e.g., Allen et al., 2016; Chi et al., 1994), most students need more prolonged instruction and training to use these strategies effectively (e.g., Jackson & McNamara, 2011). Thus, we did not anticipate robust effects of this manipulation, but were interested in exploring how encouraging more effective strategy use might impact the quality of the final written product. 
In contrast to the manipulation, we anticipated that participants’ individual differences in knowledge would be strongly related to the quality of the integrated essays. Students completed a general academic knowledge test because the impact of general knowledge has been implicated in both reading and writing. We anticipated that students with greater general knowledge would compose higher quality integrated essays due the benefits of that knowledge in both comprehension and production. Participants also completed the Gates-MacGinitie Reading Test and the Metacognitive Awareness of Reading Strategies Inventory for Multiple Documents (MARSI-MD; Karimi, 2015). These measures were conceptually-aligned with aspects of comprehension. More skilled readers tend to generate more inferences that support the construction of a coherent mental model and tend to draw upon more sophisticated and appropriate comprehension strategies. Both reading skill and awareness of reading strategies have been implicated in multiple-document comprehension (e.g., Barzilai & Strømsø, 2018; 
Karimi, 2015). 
In addition, participants wrote two independent SAT-style essays as a means of evaluating their general writing ability. Thus, performance on these essays was conceptually aligned with aspects of text production. While it stands to reason that general writing would be related to performance on the integrated writing task, few studies examining integrated writing have also included a separate writing measure to assess general writing skills. 
We predicted that higher scores on general knowledge, reading skills, and reading strategies assessments would be positively related to integrated essay quality. However, it was also predicted that general knowledge and reading skills would only partially explain variance in integrated essay performance. Prior work demonstrates that reading and writing are related, but that they rely on different processes and knowledge (Allen et al., 2014). Therefore, we also predicted that performance on the independent writing tasks would predict integrated essay quality above and beyond differences in general academic knowledge, reading skill, and comprehension strategies. 
 	In addition to these main effects, we also explored possible interactions between these individual difference measures. Previous research has shown that prior knowledge has both direct and indirect effects on learners’ comprehension processes (e.g., Cromley & Azevedo, 2007, McNamara & Magliano, 2009). Skilled writers also tend to make better use of their extant knowledge. Writing research also suggests possible interactions such that more knowledgeable students can write stronger essays and conversely, more skilled writers can better leverage the knowledge they possess (e.g., McCutchen, 1986, 2000). While there is limited research directly examining the contribution of reading skill to writing tasks, research suggests complex and potentially bi-directional development between reading and writing (e.g., Graham & Hebert, 
2011; Graham et al., 2018). For simplicity, we included only the two-way interactions across each of the three individual differences variables (e.g., general knowledge by reading, general knowledge by writing, reading by writing).  
2. Method 
2.1 Participants 
In the summer of 2019, we recruited 51 high school students. In the fall of 2019, we recruited 45 college freshmen students who had graduated high school the previous spring. Two students (both high school) were excluded from the analysis: one did not complete all four days of the study and one student’s data was lost due to technical error. 
Of the remaining 94 participants, all but one student (undergraduate) completed the demographic questionnaire. The sample was predominantly female, with 76 identifying as female and 17 participants identifying as male. The sample was also predominantly Black or African American (72%; n = 68); 10 participants identified as Asian/Pacific Islander, 10 identified as Mixed or Multiracial, 4 identified as White, and 1 identified as Native 
American/Alaska Native. Most of the participants (n = 82) were native English speakers. The high school students had a mean age of 16.22 (SD = .96) and the college students had a mean age of 18.34 (SD = .53). 
2.2 Design & Procedure 
The study was completed over 4 sessions. In Session 1, participants completed a multiple-document integrated reading and writing task in the Interactive Strategy Training for Active Reading and Thinking (iSTART; McNamara et al., 2004) interface (Watanabe et al., 2019). At the beginning of the activity, participants were told that they would be “completing a reading and writing task” using a set of documents. They were first given 4 minutes to skim the texts to familiarize themselves with the texts before they began the main activity. After skimming, participants were directed to the constructed response instructions in which they were told that they would be reading texts and then answering comprehension questions about the text. 
They were then given instructions on how to generate one of three types of constructed responses (think-aloud, self-explain, source evaluation) as they read. At target sentences throughout the texts, participants were prompted to generate constructed responses. The reading and constructed response portion of the task was untimed. After reading, participants were then provided the integrative essay instructions and given 25 minutes to write their essay. Notably, participants were able to access the texts and their constructed responses as they composed their essay.  
In Session 2, they completed a second multiple-document integrated reading and writing task with the alternate text set. In Sessions 3 and 4, participants completed the individual difference tasks. Specifically, in Session 3, they completed a multiple-document version of the Metacognitive Awareness of Reading Strategies Inventory (MARSI-MD; Karimi, 2015), a general knowledge test (O’Reilly & McNamara, 2007), and the Gates-MacGinitie Reading test as a measure of general reading skill. In Session 4, they wrote two SAT-style persuasive independent essays as a measure of general writing skill.  
2.3 Materials 
2.3.1 Multiple-Document Text Sets  
Two text sets were adapted from previous multiple-document studies (Anmarkrud et al., 2013; Ferguson et al., 2012; Strømsø et al., 2010). The texts were translations of the original materials published in Norwegian. In addition to the text content, each text was presented with the following source content: title, author’s name and credentials, publication source, and date of publication. Although these texts have been translated and slightly modified for clarity and context, the original texts were found in documents that offer authentic conflicting opinions on a socioscientific controversy. The original text sets contained eight texts that described two different positions on the topics of global warming (i.e., the positive and negative impacts of climate change and the extent to which climate change is a manmade phenomenon) and the linkage between cell phones and cancer (i.e., research that does and does not support that linkage). In the present study, we reduced the number of texts in each set to four texts. We also made minor revisions to the source information and content. Dates were updated to make the texts more contemporary, and the names and publication outlets were modified to be more culturally relevant for our US-based sample. For example, “Chief Engineer Thor Zackarisson, Mobiloperatørenes landsforening (MLF)” was changed to “Zachary Thornton, UN Telecom Association (UNTelecom)”. There were also minor modifications to the text content to increase clarity and to embed key ideas that had been removed in the reduction of the number of texts. 
Both text sets followed a similar pattern. One text introduced the controversy (Is climate change manmade?; Are cell phones linked to cancer?); One text provided scientific background information (i.e., how greenhouse gases affect climate; how cell phone towers work); and then two texts presented different sides of the controversy. The texts ranged from 263-504 words and had a Flesch-Kincaid grade level range from 9.6 to 13.5. 
2.3.2 Constructed Response Instructions  
Participants were randomly assigned to one of three constructed response instruction conditions: think-aloud, self-explanation, or source evaluation. In the think-aloud condition, participants were asked to “report your thoughts that immediately come to mind regarding how you understand the meaning of the text”. In the self-explanation condition, they were asked to explain the text to themselves as they read and to “Please explain the meaning of the text, elaborating beyond your initial understanding of the text.” Finally, participants in the source evaluation condition were asked to “reflect on the source (i.e., author, publication date/location, audience) of the text while you read. Please report your thoughts regarding how the source impacts the meaning of the text.” Participants were prompted to type their constructed responses at pre-determined target sentences throughout the texts. Each text contained between 5 and 9 target sentences and the target sentences were consistent across the three constructed response conditions. Although typed responses are somewhat different than spoken verbal protocols, typed protocols have been shown to reflect the same types of processes as spoken protocols 
(Muñoz et al., 2006). 
2.3.3 Integrated Essay Instructions  
Participants were given 25 minutes to compose each essay. For the global warming text set, the prompt read “Write an essay that explains the effects of climate change for life on earth and the extent to which humans are responsible.” For the Cell Phone text set, the prompt read “Write an essay that explains the effects of cell phones on humans and the extent to which cell phone use poses health risks.” As they read, participants could access both the texts that they read and the constructed responses that they generated during reading in a tabbed window to the right of the essay box (see Figure 1 for interface). This window also included additional instructions that encouraged the participants to use the documents as a resource and to source and elaborate upon the ideas in the documents (Appendix A for full instructions). 
2.3.4 Individual Difference Measures 
2.3.4.1 Reading Skill Measures. Participants completed an adapted version of the GatesMacGinitie Reading Test (GMRT; MacGinitie et al., 1989). The comprehension test includes a 48-item multiple-choice test in which participants read passages and answer questions about the passages. The vocabulary test is composed of 45 vocabulary items.  
2.3.4.2 General Writing Measure (Independent Essays). As a measure of generalized writing ability, participants wrote two SAT-style independent, or persuasive, essays. The essay prompts were adapted from publicly released SAT exam materials. These prompts asked participants to adopt a stance with regard to a central topic, and then to defend that position via evidence, examples, and/or logical reasoning. One prompt asked participants “Do images and impressions have a positive or negative effect on people?” (Images prompt) and the other asked “Do people achieve more success by cooperation or by competition?” (Competition prompt; for full prompts see Appendix B). All prompts were designed to minimize prior knowledge demands such that participants could write from experience rather than constrained educational content or source materials. Previous work has demonstrated robust prompt effects in SAT-style independent essay tasks (e.g., Kobrin et al., 2011). To attenuate this issue, participants were asked to write two essays. The order of the essays was counterbalanced across participants. Participants had 25 minutes to write each essay with a brief 5-10 minute break between each essay. 
2.3.4.3 General Knowledge Test. Participants complete a 30-item multiple-choice test that was developed in prior work examining reading comprehension, writing skill, and strategy acquisition skill (Allen et al., 2016; Roscoe et al., 2014). The test is designed to evaluate general academic knowledge and includes a mix of science items, history, and literature items. Prior work has shown good reliability (α’s = .72 - .81; Allen et al., 2016). 
2.3.4.4 Metacognitive Strategies. The Metacognitive Awareness of Reading Strategies Inventory 
(MARSI; Mohktari & Reichard, 2002) is a common measure of knowledge of reading strategies. 
The MARSI is a 30-item Likert survey. In this study, we used the modified MARSI presented in Karimi (2015) that was adapted for use within a multiple-document context. In this 27-item version of the assessments, participants were asked the extent to which they used the strategy in the task. For example, an original item from the MARSI is “I summarize what I read to reflect on important information in the text.”. The MARSI-MD (Karimi, 2015) version was modified to 
“While reading these multiple texts, I summarized what I read to reflect on important 
information in them.” MARSI-MD score was calculated by taking the average rating (1-5) on all 27 items. 
2.4 Scoring 
The two types of essays (independent essays and source-based integrated essays) were scored on different rubrics that have been developed and refined by writing experts. Both rubrics include a holistic score and multiple subscores. The rubrics and scales were developed and validated in the context of other research examining source-based writing and independent writing as separate tasks. Rather than trying to make the scores parallel, we elected to maintain the intended construct validity. However, prior work has shown some convergent validity as well as divergent validity across the two types of rubrics (e.g., Kim et al., 2021; Kyle & Crossley, 
2018). 
 	Although the integrated and independent essays were scored using different taskappropriate rubrics, the method of scoring and achieving reliability was the same. The expert raters were four graduate students with multiple years of experience as writing instructors. Rater pairs were trained to a high level of reliability on all metrics (i.e., all kappas > .80). Essays were randomly assigned to the pairs and each essay was then scored by two raters. If the two raters scored differed by more than 2, the scores were adjudicated by a third party. The final scores for each essay reflect the average score of the two raters.  
1.4.1 Independent Essays 
Independent essays were assigned both a holistic score (1-6) as well as subscores (see Appendix D). Independent essay subscores include: (1) introduction paragraph quality, (2) body paragraph quality, (3) conclusion paragraph quality, (4) organization, (5) topic and global cohesion, (6) grammar, style, and mechanics (7) voice, (8) word choice, and (9) sentence structure. The holistic rubric is based on previous iterations of the SAT rubric (CollegeBoard) and has been used in several other studies of writing (e.g., Authors, 2015). Across raters, the reliability of ratings for scores (ICC) ranged from .71 to .86. 
 	The average subscores (Tables 1 and 2) for the independent essays were moderately to strongly correlated (rs = .42-.84) and there were no significant differences in the subscores across the two prompts (all ps > .05). There was a strong correlation between holistic score on the two independent essays (r = .63). Thus, we used the combined holistic score (Images + Competition) to reflect a measure of general writing ability. This holistic score was strongly correlated with the summed subscores (r = .96).  
2.4.2   Integrated Essays 
Essays were assigned a holistic score (1-6) as well as subscores from 1-4 (see Appendix C). Integrated essay subscores include (1) argumentation, (2) source use, (3) language sophistication, and (4) organization. This rubric was developed based on existing writing rubrics (e.g., TOEFL) and consistent with the extant body of work in integrated writing (e.g., Martinez et al., 2015; Vandermeulen et al, 2020; van Ockenburg et al., 2019). Across raters, the reliability of ratings for scores (ICC) ranged from .65-.82. Sample essays with explanations for their scores appear in Appendix E. 
 	The average subscores scores and correlations for the two integrated essays are shown in 
Tables 3 and 4. T-tests indicated no differences as a function of text set (all ps > .05). Consistent with the independent essays, the correlation between a combined holistic score and the summed subscores was strongly correlated (r = .96). 
2.5 Analytic Approach 
The unique and combined effects of individual differences were examined on integrated essay scores. Due to the varying ranges of scores (Table 5), z-scores were computed for each of the individual difference measures. We conducted a series of linear mixed effects models using the lme4 package in R (Bates et al., 2014). The baseline model (m0) included participant nested within grade as a random factor and text set as a fixed factor. The additional models added the following fixed factors: constructed response prompt condition, general knowledge, GatesMacGinitie Reading, Gates-MacGinitie Vocabulary, MARSI-MD score, and independent essay score. Although preliminary analyses indicated no significant differences in integrated essay subscores across the two multiple-document text sets (global warming, cell phones), text set was included as a fixed effect in a baseline model (m0). To control for differences across the two grade levels (high school, college), we included participant nested within grade in the baseline model as well. Model 1 examined the effect of prompt condition (think-aloud, self-explanation, source evaluation). Each individual difference was then added in order from distal to proximal as the skill was assumed to related to the integrated writing task. Thus, general knowledge score was entered into the second model (m2) as it is the most general measure relative to the multipledocument inquiry task. In Model 3, we added Gates comprehension score, Gates vocabulary score, and metacognitive strategies (MARSI-MD). These measures were assumed to reflect skills and strategies more immediately related to reading comprehension. It was predicted that those with better reading skills would be able to construct a more coherent and integrated mental model during the reading task, which would afford a higher quality essay. However, it was also assumed that a coherent mental model of the text topic would be necessary, but not sufficient for writing a high-quality integrated essay because integrated writing also involves separate writing skills. Thus, writing skill as measured by the independent essay scores was entered in Model 4. By adding the individual difference in this order, we were able to examine the extent to which writing skill positively contributes to integrated writing performance above and beyond the contribution of reading skill. Finally, we tested three two-way interactions across each of the three types of individual differences: general knowledge by writing skill, general knowledge by reading skill, and reading skill by writing skill. For simplicity, Gates comprehension score was used to reflect variability in reading skill. In the first set of analyses, holistic score for each integrated essay was used as the dependent variable  . We then conducted parallel analyses with each of the subscores. 
3. Results 
3.1 Preliminary Analyses 
Preliminary analyses showed good reliability for the individual difference measures (general knowledge: α = .71; GMRT: α = .90; GMVT: α = .84; MARSI-MD: α = .86). These values were consistent with values from previous literature. Bivariate correlations (Table 5) reveal moderate correlations between the reading skills tests (Gates-MacGinitie Reading Comprehension and Gates-MacGinitie Vocabulary) and the independent essay scores, suggesting that these literacy skills are related, but at least somewhat independent. The correlations between both knowledge and the reading skill measures were stronger for the integrated essays than the independent essays. There were also moderate to strong correlations between general knowledge and the two Gates tasks. Integrated essay performance was moderately to strongly correlated with general writing skill, reading skill, and general knowledge. Interestingly, the participants’ self-reported strategy use (MARSI-MD) was not correlated with any of the individual difference measures nor the integrated essay scores.  
Although our focus was on high school students, we had expanded our recruitment efforts to college freshmen. Despite recruiting college freshmen who had just graduated high school in the previous spring, there were significant differences in performance across the college and high school students. T-tests revealed that college students performed significantly better on the general knowledge test, Gates-MacGinitie Reading, Gates-MacGinitie Vocabulary, and integrated essay score (Table 6). Although differences in MARSI-MD score and independent essay score were in this same direction, these tests failed to reach significance. These differences are unsurprising if one assumes that the selectivity of college admissions is likely to ensure that the college sample reflects the higher performing high school students. As we did not have any a priori predictions related to grade and that our constructed response prompt was randomly assigned across grade level, we do not explore this further. However, participant nested within grade is included in subsequent analyses to account for this variance. 
Preliminary analyses (ANOVAs) revealed that random assignment to constructed response prompt condition (think-aloud, self-explanation, source evaluation) yielded even groups in terms of participants’ general knowledge, reading skill, reading strategies, and independent essay score (all Fs < 1.00).  
3.2 Predicting Integrated Writing Scores 
3.2.1. Holistic Score 
Model summaries appear in the top rows of Table 7. Likelihood ratio tests indicate differences between each model and a reduced model. Significant chi-square (χ2) tests indicate that adding the additional variable(s) improved fit as compared to the previous model. 
Coefficients for each model are presented in Table 7. 
The addition of constructed response condition (think-aloud, self-explanation, or source evaluation) to Model 1 did not improve fit beyond the grade, participant, and text set factors included in the baseline model. Analysis of Model 2 showed that adding general knowledge to the model improved model fit and indicated that general knowledge was a strong, positive predictor of essay scores. Model 3 included the measures related to reading: Gates Reading, Gates Vocabulary, and MARSI-MD. Although beta weights indicated a positive relation between all three reading scores, the reading measures did not significantly improve model fit compared to Model 2. To test the contribution of general writing ability to integrated essay score, independent essay scores were added in Model 4. This model improved model fit as compared to the model that included text set, general knowledge, Gates Reading, Gates Vocabulary, and the 
MARSI-MD. Coefficients revealed that general knowledge remained a significant predictor (β = 
0.23), but that independent essay score was a stronger predictor of integrated essay score (β = 
0.43), even when controlling for the contributions of reading skills and self-reported strategy use. 
Finally, three interactions were tested in comparison to Model 4: general knowledge by writing skill (m5a), general knowledge by reading skill (m5b), and reading skill by writing skill (m5c). None of these interactions improved model fit relative to the main effect model, m4 (m5a: χ2 < 0.03, p = .87; m5b: χ2 = 1.67, p = .20; m5c: χ2 = 0.30, p = .58).
3.2.2. Subscores 
Holistic score was strongly correlated with each of the four subscores (Tables 3 and 4). However, holistic score may tap into overall writing quality rather than specific features of the essays that reflect content comprehension and integration. Thus, we conducted analyses to evaluate the extent to which the individual differences predicted the four subscores 
(argumentation, source use and integration, language sophistication, and organization). These analyses more closely reflect what has been done in previous work related to multiple document comprehension and source-based writing. 
For all four subscores, adding general knowledge to the model (m2) improved model fit (Tables 8-11). Adding the reading skill measures (m3) improved model fit for the argumentation score (Table 8) and source use score (Table 9) but did not improve model fit for the language sophistication score (Table 10) nor the organization score (Table 11). This finding supports the notion that argumentation and source are related to comprehension processes, whereas language use and organization of ideas may be more related to production and composition. Critically, consistent with what was found for the holistic score, these effects were subsumed by independent essay score representing writing skills. That is, once independent essay score was added to the models, it became the strongest predictor of subscores – even for those measures assumed to reflect comprehension. In these best fit models, independent essay score was the only significant predictor for the source use, language sophistication, and organization scores. 
4. Discussion 
Multiple-document integrated reading and writing tasks are a common practice in everyday life and in the classroom. While there has been an abundance of studies examining multiple-document comprehension through integrated writing, this work has been criticized because rather than treating the task as a hybrid task (e.g., Spivey, 1989), as it is in the study of synthesis writing, multiple-document comprehension researchers tend to view the essay as a means-to-an-end (making inferences about the processes that occurred during reading) without considering how students’ writing ability might impact the essays (e.g., Du & List, 2020; Primor & Katzir, 2018; Wiley et al., 2018). This study highlights this issue by showing that students’ general writing ability strongly impacts the quality of a multiple-document source-based integrated essay above and beyond the contributions of strategies and skills related to reading comprehension. In addition to evaluating the contribution of individual differences in reading and writing on overall writing quality (i.e., holistic score), we also found that general writing ability was a strong predictor of subscores that have been used as proxies for comprehension in prior studies of multiple documents comprehension (e.g., argumentation, source use, and organization). This further highlights the need to measure and account for writing skill when making assumptions about comprehension processes in multiple document studies. 
We used a constructed response prompt manipulation to encourage deeper comprehension through two strategies (i.e., self-explanation and source evaluation) that have been shown to support integration and comprehension in both single and multiple-document scenarios. Neither of these strategy prompts yielded differences in the integrated essay quality relative to a think-aloud control. On the one hand, this finding in inconsistent with the existing work showing benefits of self-explanation and source evaluation. On the other hand, it is consistent with the idea that single-dose prompt manipulations may be insufficient for improving strategy use (e.g., McNamara et al., 2004, 2017). Strategy interventions, such as iSTART (McNamara et al., 2004) for self-explanation and Sourcer’s Apprentice (Britt & Aglinskas, 2002) and SEEK (Sanchez et al., 2006) for source evaluation include multi-session interventions that include explicit instruction, training, and feedback. Thus, these findings encourage two further lines of the work; the first is to further investigate how the prompt manipulation may have impacted the nature of the constructed responses and the extent to which this might mediate differences in essay content or quality. The second is to provide more in-depth strategy instruction for both self-explanation and source evaluation to enhance the ability to examine the impact of these strategies on both comprehension and production. 
In terms of the individual differences, correlational analyses revealed that general knowledge, reading skill, vocabulary, and general writing ability were positively related to the quality of integrated essays. Interestingly, MARSI-MD was not related to the other individual difference measures. There are a few possible explanations. First, there is a potential issue of using self-report. Some research has shown that self-reported measures of strategy use are only weakly related to objective literacy measures (e.g., Cromley & Azevedo, 2006). However, others have argued that self-report measures may be valid if they are grounded to a specific task and judgments are made retrospectively (Bråten et al, 2020; McNamara, 2011). Whereas the original 
MARSI is designed to evaluate students’ general awareness of metacognitive strategies, MARSIMD is a grounded assessment, in which students are asked to reflect on what they did when reading these texts during the targeted reading task. As there were two MD tasks, each conducted on separate days (i.e., Sessions 1 and 2), participants completed the MARSI-MD in Session 3 of the study, rather than immediately after completing the MD tasks. This design was decidedly optimal because administering the MARSI-MD immediately after Session 1 may have biased performance in Session 2 and administering immediately after Session 2 may have led to responses that were specific to only Session 2. Nonetheless, administering MARSI-MD on day three potentially reduced its validity as a grounded measure if participants did not remember which strategies that they had used during sessions one and two. Additionally, participants may not have perceived the strategies specified in the MARSI-MD as relevant to the particular reading and writing tasks engaged in this study. Indeed, the low level of variability in responses suggests this may be the case. Finally, the original MARSI yields three subscores that have not been validated in the MARSI-MD. A single MARSI-MD score may obscure more subtle differences and in turn reduce its predictive validity. Notably, however, participants’ responses on the MARSI-MD were weakly predictive of argumentation subscores (r = .19), somewhat suggestive of relations between comprehension strategies and the ability to develop coherent arguments in integrated essays. However, given the multiple concerns regarding the MARSIMD, as well as the predominance of null correlations, this singular, modest finding should be interpreted with caution. Future research with larger samples will be necessary to further explore the role of metacognitive strategies through additional or alternative methods of capturing strategic processing.  
We also found that reading skill (as measured by the Gates Reading and Gates Vocabulary) did not contribute significantly to the best fitting models explaining the quality of the integrated writing essays for both overall holistic score as well as for the four subscores. This would be unexpected given that reading skill is an established predictor in multiple-document research (Barzilai & Strømsø, 2018). One possible explanation is that the measure of vocabulary knowledge (i.e., the Gates vocabulary score) was strongly correlated with both comprehension and general knowledge (rs = .69). Vocabulary knowledge is both a facilitator and outcome -- a larger vocabulary facilitates reading which, in turn, supports knowledge gain, so it is unsurprising that those with a greater breadth of vocabulary also demonstrate greater general academic knowledge (McCarthy & McNamara, 2021; Wang et al., 2021). Thus, the contributions of variability in both Gates scores may have failed to significantly account for variance because of an inherent overlap in general knowledge and reading skill.  
The results suggest that general proficiencies in text production and conveying one’s ideas in writing are critical contributors to the quality of integrated writing, above and beyond the contribution of abilities related to comprehension. From an applied standpoint, this is important because most of the interventions for multiple-document inquiry focus on the comprehension aspects of the task, and particularly on evaluating the credibility of the source. Although these processes are undoubtedly important, such interventions overlook the need to support students’ ability to translate their ideas onto the page and organize them coherently. From a more theoretical perspective, this study suggests the need to better understand the relations between comprehension and production and highlights a need for the development of an integrated model of reading and writing (e.g., McNamara & Allen, 2017). 
Researchers have identified a number of complex processes involved in successful multiple-document reading and writing (e.g., Barzilai & Strømsø, 2018; Britt et al., 2017; List & Alexander, 2019; Rouet et al., 2017).  However, much of the research has focused on the comprehension processes that occur during reading. The results of the present study clearly suggest that more emphasis must be placed on understanding the writing skills and processes involved as learners’ attempt to use their mental model of the document content and translate it to the page. 
4.1 Limitations & Future Directions 
The present study provides new insights into the relations between reading and writing in an integrated writing task. However, there are some limitations in this work that should be considered and drive future research. The first is related to the outcome measure.  While some of the subscores are related to comprehension of the content (e.g., source use), others draw more heavily upon other aspects of writing and composition. For example, argumentation score reflects not only comprehension of the materials, but also discipline-specific argumentation skills (e.g., Goldman et al., 2016). Both language and organization subscores are likely to reflect more general composition skill rather than understanding of the text content. It is not our intent to argue that existing multiple-document comprehension rubrics do not include these dimensions, but they are not often made explicit. This was an intentional decision to highlight the fact that integrated essays are a writing task and not a pure window into comprehension as is often assumed in research on multiple-document comprehension. A writing focused rubric is also more consistent with the type of evaluation that a student might receive as part of a multiple-document activity assigned in a classroom.  
An additional limitation in this study is our wording in the verbal protocol instructions, which indicated that the participant would be reading with the goal of “answering comprehension questions.” Our intent was to provide a relatively ambiguous reading goal, rather than providing the essay instruction prior to reading. Given the robust body of work on task relevance in single and multiple document comprehension research, it is likely that the effects in the present study would be moderated by different task instructions. Future work should consider this. Notably, we did not include comprehension questions in the study. Many multipledocument comprehension studies include comprehension questions such as inference verification tests (e.g., Bråten & Strømsø, 2011). These tests allow researchers to probe for evidence of specific intra- and inter-textual inferences that may not emerge in written products. We did not include these types of items in this study because including both essay and comprehension questions poses challenges for interpreting results. Providing comprehension questions prior to writing can cue participants to concepts and connections they may not have made on their own. Conversely, a writing task is likely to drive additional reflection and integration that may bias performance on subsequent comprehension questions. Thus, more systematic work should be conducted to identify the specific skills and knowledge that are being measured by these various tasks so that researchers can better identify when to use these various measures in their own research. 
Second, this study focused on domain-general individual difference measures. That is, the tests were designed to evaluate general academic knowledge, reading skill, and writing ability. However, multiple-document integrated writing tasks are likely to be dependent not only on these general factors, but also on knowledge, skills, and strategies that are specific to the topic and type of task at hand (McCarthy & McNamara, 2021). For example, general academic knowledge is superseded by relevant topic knowledge when learners are engaging in complex, multiple-document comprehension tasks (e.g., Wang et al., 2021). Topic knowledge is a known factor in both reading comprehension (e.g., Alexander et al. 1994; Ozuru et al., 2009) and writing quality (e.g., Hammann & Stevens, 2003; Langer, 1983). In the current study, we did not collect measures of topic-specific knowledge for each of the four essay topics. It is likely that the variation seen in essay quality across the essay type and essay topics can be at least partially explained by topic general knowledge. Similarly, general reading skill as measured by reading and answering isolated passages may only be one part of deeper meaning-making and integration in real-world learning tasks (e.g., Magliano et al., 2018; Sabatini et al., 2019) and the specific skills and strategies at play may vary depending on the type of text or the context in which the text is being read (e.g., McCarthy, 2020; Rouet et al., 2017; van den Broek et al., 2001). In a similar fashion, the independent essay score served as a proxy for general writing ability. Writing is comprised of a variety of lower and higher order skills and strategies. This study suggests a need to more carefully examine how these component skills might predict performance on integrated writing. 
In the current study, students were prompted to write an explanation of the socioscientific issues raised in the text sets. Prior work in multiple-documents reading and writing has shown that different essay prompts (e.g., write to inform vs. write to argue) may affect the way that the learner interprets that task (e.g., Britt et al., 2017; List et al., 2019) and may change the nature of the processes and strategies that emerge during reading and writing and the quality of the final written products (Gil et al., 2010; Vandermeulen et al, 2020; Wiley & Voss, 1999). In the same vein, the ability to translate and organize one’s ideas into writing is an important foundation, but students also need to engage higher-order disciplinary argumentation skills that may be specific to the goals and expectations of the discipline or genre in question (e.g., Goldman et al., 2016; Goldman, 2018). Our results suggest that future work in multiple-document integrated reading and writing should examine the contributions and potential interactions between both general proficiencies and more text, task, and discipline-specific knowledge and skills. 
Finally, there are a number of other individual differences beyond reading and writing skills that influence the way that text is processed and how integrated tasks might unfold. For example, several studies have demonstrated that attitudes, beliefs, and epistemologies can predict the extent to which readers seek out, use, and integrate information, especially in the context of controversial topics (e.g., Bråten & Strømsø, 2010, 2020; Kunda, 1990; Maier & Richter, 2016; McCrudden & Sparks, 2014). Future work should consider how the cold cognitive processes described in this paper interplay with warm, affective individual differences related to motivation and engagement (List & Alexander, 2017). 
4.3 Conclusions 
Multiple-document inquiry tasks in the classroom are an attempt to emulate the types of problem solving and learning that students must engage in to deeply understand and interact with the world around them. These tasks require learners to evaluate and integrate information from a variety of sources in order to develop and convey complex ideas and demand a variety of knowledge, skills, and strategies. The past few decades have elucidated the processes and factors related to multiple-document comprehension. The next few decades should be dedicated to extending our understanding of how these processes come together as students read and write about complex topics. A thorough investigation of this area would include the processes and factors involved in integrated writing quality as well as the interaction between comprehension measures and writing outcome measures. 
