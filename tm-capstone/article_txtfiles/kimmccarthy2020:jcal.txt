Improving summary writing through formative feedback in a technology-enhanced learning environment. 


Abstract
Summary writing is a useful instructional tool for learning. However, summary writing is a challenge to many students. This mixed-method study examined the potential of the Student Mental Model Analyzer for Research and Teaching (SMART) system to help students produce summaries that reflect key concepts and relations in a text. SMART uses the students' summary to generate a multi-dimensional 3S (surface, structure, semantic) evaluation of the students' mental model. This model is then used to drive feedback to help students revise their summary. The current study is an initial investigation examining whether writing and revising in SMART improves students' summary quality. Students (n = 38) in a graduate-level online course used SMART for seven reading assignments. The 38 students submitted a total of 357 summaries in response to the seven readings. In 47 cases, students produced both an initial draft and a modified revision. These 47 cases were selected for analysis. In the quantitative phase, MANOVA results indicated that students' summaries improved along the 3S dimensions from initial draft to revision. In the qualitative phase, inspection of exemplar cases revealed how students' mental models changed towards more robust and cohesive knowledge structure for texts.

1 INTRODUCTION
Learning emerges from the coordination of a variety of complex processes. One of the most critical classroom skills, and life skills, is the ability to learn from text. Texts continue to prevail as the main means of acquiring new information (Singer & Alexander, 2017). Unfortunately, many students struggle with reading comprehension (NCES, 2015), which, in turn, inhibits their ability to learn new content (Goldman et al., 2016; Snow, 2002).

A common practice in classrooms is to ask students to write summaries of what they have read. Summaries can be used as a tool to assess what a student has understood, but summarization is also frequently used as a tool for learning (see Dunlosky, Rawson, Marsh, Nathan, & Willingham, 2013). That is, writing a summary can help students to actively engage with the text material, identify main ideas and put the text into their own words. However, using summary writing as a learning strategy is often limited by the quality of the summary itself, and students, from early readers through college students, often need support in constructing high-quality summaries (Duke & Pearson, 2009; Friend, 2001). In this paper, we describe how the Student Mental Model Analyzer for Research and Teaching (SMART)—a web-based formative assessment and feedback technology—has been used as an instructional tool to help students write high-quality summaries that help them comprehend complex texts so that they are prepared for meaningful class discussion.

Reading instruction is often limited to early elementary school, and once students learn to read, little emphasis is put on helping students “read to learn” (Pearson & Billman, 2016). Content teachers spend minimal time (~3%) providing instruction on reading comprehension strategies (Heller & Greenleaf, 2007; Ness, 2016). One result of this limited discipline-specific comprehension instruction is that many proficient readers struggle to learn from text (Goldman, Snow, & Vaughn, 2016; McNamara, Roscoe, Allen, Balyan, & McCarthy, 2019). One can imagine how this is problematic for content courses. In many courses, readings (e.g., textbook chapters, research articles) are assigned as homework with the intent of using the reading as a springboard into deeper discussion and content. If students do not possess a solid understanding of the information, they are likely to fall further behind.

The distinction between memory for text and comprehension of the content is an important one in theories of discourse comprehension. These theories posit that deep comprehension emerges from the construction of a coherent and elaborated mental model, or mental representation, of the text (Graesser, Singer, & Trabasso, 1994; K. Kim & Clariana, 2017; M. Kim, 2015; M. Kim, Gaul, Kim, & Madathany, 2019; Kintsch, 1988). Mental models include information that is explicit in the text as well as necessary inferences that connect ideas from different parts of the text and connect information from the text to information in prior knowledge. Thus, students need to identify not only the key ideas but also the relations between those ideas. A written summary can serve as an externalized representation of this mental model. That is, educators and researchers can analyse the nature of a student's summary to infer the quality of the learner's mental model (Ifenthaler, 2014; K. Kim, 2017, 2018; K. Kim, Clarianay, & Kim, 2019; Pirnay-Dummer & Ifenthaler, 2011).

Further, the act of summarizing can also serve as an additional learning scaffold. Summarizing “requires readers to sift through large units of text, differentiate important from unimportant ideas, and then synthesize those ideas and create a new coherent text” (Dole, Duffy, Roehler, & Pearson, 1991, p. 244). Summary writing increases memory for and comprehension of text material (Rinehart, Stahl, & Erickson, 1986). Further, recent research indicates that summary-writing interventions are beneficial for younger students (Stevens, Park, & Vaughn, 2019) as well as college learners (Friend, 2001; Perin, Bork, Peverly, Mason, & Vaselewski, 2011).

One limitation of using summarization in the classroom is insufficient feedback. In order for students to improve their reading and writing skills, they need to receive personalized, formative feedback (Graham, MacArthur, & Fitzgerald, 2013; Schunk & Rice, 1991). This type of feedback is time-intensive and difficult to deliver at scale. In response to this challenge, researchers have turned towards technology-enhanced learning environments that can automate aspects of the learning process.

Automated summary evaluators (ASEs) leverage advanced natural language processing tools and techniques to assess linguistic features of students' written responses (Allen, Jacovina, & McNamara, 2016; Passonneau et al., 2018; Strobl et al., 2019). ASEs such as Summary Street (Wade-Stein & Kintsch, 2004), Online Summary Assessment and Feedback System (Sung, Liao, Chang, Chen, & Chang, 2016), crowd-source summary evaluation (Li, Cai, & Graesser, 2016, 2018), ROUGE (Lin, 2004), SEMILAR (Rus, D'Mello, Hu, & Graesser, 2013) and PryEval (Gao, Warner, & Passonneau, 2019) use hundreds of descriptive linguistic indices related to word-level (e.g., lexical diversity), sentence-level (e.g., syntactic complexity) and document-level (e.g., cohesion from sentence to sentence) qualities of the summary to examine the quality of writing and to drive feedback to help students improve their summary writing skills. However, feedback in ASEs tend to focus on the act of summarizing, rather than comprehension (Sung et al., 2016).

A model-based approach is an alternative means to provide a more comprehensive evaluation of the student's summary (Ifenthaler & Pirnay-Dummer, 2014). A model-based approach uses a concept map—a network of interrelated concepts (Axelrod, 1976; Schvaneveldt & Cohen, 2010) elicited from a summary—to generate indices in multiple dimensions of knowledge structure (Ifenthaler, 2014; M. Kim, 2012; Pirnay-Dummer & Ifenthaler, 2011; Spector & Koszalka, 2004). Model-based ASEs such as AKOVIA (Ifenthaler, 2014), GISK (Kim, 2018), HIMATT (Pirnay-Dummer & Ifenthaler, 2011) and SMART (M. Kim et al., 2019) evaluate the quality of students' knowledge representation and use concept maps for feedback to help them consider appropriate content and connect important ideas together. Scholars have shared that students made meaningful changes in their summaries through concept map feedback (Clariana, 2010; Ifenthaler, 2010; K. Kim, 2018; M. Kim, 2015; Pirnay-Dummer & Ifenthaler, 2011).

Although these ASEs show promise, our review of the literature identified remaining challenges. First, the algorithms used across different model-based ASEs are so diverse that they generate different concept maps and quality judgements on the same summary (M. Kim, 2012), which requires further methodology advancement and validation. Second, some ASEs limit the number of words in a summary (e.g., a maximum of 350 words) or analyse only pre-determined keywords, which may constrain the summary and concept map in ways that may not best reflect the content of the text. Third, different ASEs assume different structures. Some ASEs assume two dimensions (i.e., surface and deep structure; Bransford & Johnson, 1972; Gentner & Medina, 1998; Katz & Postal, 1964; Kintsch & van Dijk, 1978), while others assume three dimensions (i.e., surface, structure and semantic; Ifenthaler, 2014; M. Kim, 2012; Pirnay-Dummer & Ifenthaler, 2011; Spector & Koszalka, 2004). Moreover, most ASEs use indices that do not cover all aspects of mental models. We need to examine all three dimensions better to understand changes in students' multi-faceted mental representations.

Driven by the affordances and limitations of existing ASEs, we developed a beta version of Student Mental Model Analyzer for Research and Teaching (SMART)—a model-based ASE designed to help college and post-secondary students to write and revise high-quality summaries of the complex texts they encounter in their courses. The current study investigates how writing and revising summaries using SMART feedback affects multiple dimensions of the students' mental representations. In this regard, we first outline the foundations of SMART in the context of existing automated summary evaluators. We then describe the potential power of analysing students' summaries for their latent knowledge structures in terms of three dimensions: surface, structural, semantic (3S). Finally, we provide initial evidence for SMART's 3S approach in helping post-secondary students to write more expert-like summaries of their course materials.

2 STUDENT MENTAL MODEL ANALYZER FOR RESEARCH AND TEACHING (SMART)
2.1 SMART technology
The SMART project was initiated to develop an advanced formative assessment and feedback technology based on the premise that a student's deep knowledge of a particular complex reading or a problem situation can be developed and measured through writing a high-quality description of a text or a complex problem (Garnham, 1987, 2001; Johnson-Laird, 2005; Kintsch, 1998; Westby, Culatta, Lawrence, & Hall-Kenyon, 2010; Zimmerman et al., 2018). In short, students read a text and then write a summary of that text. Automated summary evaluation is used to assess the student's summary for its underlying structure as a means of understanding the nature of the student's mental model. This student model is compared to an expert model to deliver feedback to help the student identify overlooked key concepts and relations.

SMART technology is grounded in theories of mental models. Mental models are theorized as cognitive artefacts people develop to understand certain aspects of a problem situation (e.g., a complex reading material; Anzai & Yokoyama, 1984; Collins & Gentner, 1987; Johnson-Laird, 2005; Seel, 2004; Smith, diSessa, & Roschelle, 1993). In the context of reading and summary writing, students develop their mental models that consist of connected ideas and concepts drawn from the text. SMART elicits students' concept maps from their summaries as re-represented their internal mental models. Thus, one can evaluate learner comprehension by examining multi-dimensional aspects of concept maps (Clariana & Taricani, 2010; Gijbels, Dochy, den Bossche, & Segers, 2005; M. Kim et al., 2019; Zimmerman et al., 2018). A central assumption of the SMART design is that students' summary revisions facilitated by SMART feedback can indicate their evolving mental models towards the structure of the author's mind (Anzai & Yokoyama, 1984; Collins & Gentner, 1987; Johnson-Laird, 2005; Pirnay-Dummer & Ifenthaler, 2011; Seel, 2004; Smith et al., 1993). In turn, the use of an externally represented expert's mental model in the SMART feedback helps students not only to attend to the number of ideas and concepts that they should learn from the text but also to (re)organize their mental model (Bransford, Brown, & Cocking, 2000; Jonassen, Beissner, & Yacci, 1993; Kintsch, 1998; Segers, 1997).

SMART contributes to the growing cadre of ASE tools in a number of ways. First, SMART addresses a different population and goal than most ASEs. Many automated summary evaluators are designed for elementary and middle school students to practice reading and summarize short texts around 1,500 words (Merchie & Van Keer, 2016; Sung et al., 2016; Wade-Stein & Kintsch, 2004). In addition, extant ASEs focus on developing students' summary writing skill. That is, the focus of instruction and feedback is on generalizable summary writing strategies rather than on the specific content in the readings. By contrast, SMART is designed to be used as a supplement in post-secondary courses in which the assigned readings include longer and more complex texts (e.g., textbook chapters, journal articles). The goal of summary writing in this context is to help students develop a sufficient understanding of complex texts they read for their class activities so that they can better perform in student-centred learning activities (e.g., peer-led online discussions).

Secondly, SMART differs from other model-based ASEs in its assumptions and approach to analysis. SMART analyses linguistic components of a summary (i.e., an open-ended approach with no constraint on the number of words and used terms) and can create a more authentic and descriptive concept map than other ASEs. SMART generates the highest number of model-based indices in multiple dimensions, which helps drive more diverse and rich feedback information. These multi-dimensional indices are able to more capably trace mental model changes and distinguish better models from models of lesser quality. SMART's features and its underpinning theories are further described in the following section.

2.2 Linguistic analysis of student summaries
In many cases, ASE analytics rely on the linguistic features of students' summaries. Linguistic indices generated from a text-based analysis provide surface-level feedback information such as the number of words, different word choices and grammatical errors. Many ASE systems rely on Latent Semantic Analysis (LSA; Landauer & Dumais, 1997)—a statistical approach to build a semantic space only preserving essential concepts closer to one another. One can calculate the relations between student summaries and reference texts by comparing these semantic spaces (Sung et al., 2016). Beyond the use of the linguistic properties in the surface and semantic levels, scholars have shifted attention to the structural aspect of summaries in terms of cohesion and connections among concepts (Allen, Likens, & McNamara, 2019; K. Kim et al., 2019). SMART leverages students' summaries to infer their latent mental models from which their understanding of the texts is evaluated in the structural as well as surface and semantic dimensions.

Word-based and LSA-based analyses provide descriptive information about the usage of words, length, similarity score, content coverage, redundancy and topic relevance (Allen et al., 2019; Gao et al., 2019; Sung et al., 2016). However, such feedback lacks the targeted, elaborated guidance on what specific concepts and propositional relations from the texts should be better structured in students' summaries. To overcome, Sung et al. (2016) added sentence relevance feature in the feedback, indicating which sentences in a student summary are overlapped with a reference summary and also provided expert concept maps as a supplement to enhance summarization performance. However, the sentence-level feedback still seems broad, and the manual concept map creation in addition to entering expert summaries appear labour-intensive and likely inaccurate. SMART system automatically elicits both student and expert concept maps from their summaries and compares them to provide a micro-level feedback, targeting on specific concepts and propositional relations.

Our previous work has focused on the development of the summary evaluation algorithm. SMART analytics is based on the premise that a summary can be represented as a concept map and that this concept map reflects the reader's underlying mental model of the text (Graesser et al., 1994; Johnson-Laird, 2005; Kintsch, 1998). This is grounded in theories of mental models that assume that students build internal knowledge representations about a text which likely take a structural form consisting of semantically related concepts (Anzai & Yokoyama, 1984; Garnham, 2001; Johnson-Laird, 2005). A concept map, which is an array of concept-to-concept relations elicited from a written summary, can be used to infer the learner's latent knowledge structure (Axelrod, 1976; Jonassen et al., 1993). SMART generates both an expert concept map and a student concept map to drive feedback. SMART analytics use an expert summary to automatically generate an expert concept map alongside key concepts and their propositional relations embedded in that expert map. In a previous study (M. Kim et al., 2019), the SMART algorithm was able to identify key ideas in the expert summary with 79% accuracy. The same algorithm is applied to elicit a student model from a student summary. The student model (concept map) is then compared to the expert model to identify which key concepts and relations are missing from the student summary.

2.3 Knowledge structures
Technology-enhanced learning environments help students to learn by evaluating a student model and comparing it to an expert model. Learning progressions can be described as a set of intentional changes by an individual to form a more integrated mental model with substantial information of the text. In this vein, students can develop deep comprehension by gradually modifying their mental models through instructional supports (Pirnay-Dummer & Ifenthaler, 2011; Seel, 2004; Shute & Zapata-Rivera, 2007; Smith et al., 1993). Diagnosis of individuals' learning progression and resulting personalized feedback can be achieved by a comparison between a student mental model and an expert mental model.

Early technology-enhanced learning environments focused on well-defined domains (e.g., physics and math) in which right and wrong answers can be quickly assessed via correct/incorrect multiple-choice or numeric responses. However, recent advances in natural language processing afforded the use of automated tutoring in ill-defined tasks, such as reading and writing, and allows for the assessment of conversation and open-ended responses (McNamara, Allen, McCarthy, & Balyan, 2018). Thus, researchers are able to use natural language to infer the structure of a learner's knowledge. These assessment technologies, such as AutoTutor (Rus et al., 2013), e-rater (Burstein, Tetreault, & Madnani, 2013), ALA-Reader (Clariana, Wallace, & Godshalk, 2009), and GIKS (K. Kim, 2017, 2018) take student's spoken or written dialogues, short responses or essays and extract key concepts. These concepts are then compared to pre-identified target concepts to deliver relevant scaffolding (Shermis, 2010).

In SMART, students write summaries of the text they have read. Summaries serve as re-represented mental models of facts, concepts and symbols (K. Kim et al., 2019; M. Kim, Zouaq, & Kim, 2016; Spector & Koszalka, 2004). Importantly, theories of discourse comprehension and more general theories of learning emphasize that a learner's mental model or knowledge structure is multi-layered or multi-dimensional (Clariana, 2010; Ifenthaler, 2014; Ifenthaler & Pirnay-Dummer, 2014; M. Kim, 2012, 2015). While there are a variety of theories that aim to identify these different levels of representation, the current work examines the structure of student's knowledge using the 3S model (Ifenthaler, 2010; M. Kim, 2012; Spector & Koszalka, 2004). This model identifies three key dimensions: surface, structural and semantic.

2.3.1 Surface dimension
The surface dimension reflects the overall number of all concepts and relations in the mental model (Holyoak & Koh, 1987; Simon & Hayes, 1976). These surface features (e.g., relations among nouns) characterize the basic components of the mental model (Fodor, Bever, & Garrett, 1974; Katz & Postal, 1964).

2.3.2 Structural dimension
The structural dimension reflects aspects of the mental model such as size or complexity. Structure speaks to the degree to which information in the text is organized and connected (Bransford & Franks, 1972; Bransford & Johnson, 1972; Gentner & Medina, 1998; Kintsch & van Dijk, 1978).

2.3.3 Semantic dimension
The semantic dimension includes understanding of key concepts and relations in a knowledge structure and the overlap, or integration, or key propositions. That is, the semantic dimension focuses on the underlying ideas in the text (Bransford & Franks, 1972; Bransford & Johnson, 1972; Katz & Postal, 1964; M. Kim, 2013; Kintsch & van Dijk, 1978).

The 3S knowledge structure was used in the development of SMART's assessment algorithm. SMART utilizes “deep NLP” techniques to extract text variables (i.e., concepts and their relations from student summaries) from an in-depth syntactic analysis. It is important to note that we use the term “deep” here as in deep semantics (as opposed to shallow) to reflect the underlying deep structure of the corpus (Janssen, 2020; Montague, 1974). For example, NLP dependency analysis identifies the semantic relations (i.e., concept-to-concept) based on the way parts of a sentence are syntactically combined (e.g., “computational thinking [concept 1]”—“involves [predicate]”—“abstract thinking [concept 2]”). SMART also uses the dependency grammar representations to identify complex noun compounds as a single candidate concept (e.g., “computational thinking”). Using text variables, SMART deploys Social Network Analysis (SNA) to construct numeric parameters to characterize a concept map. NLP provides calculations of the number of words, concepts and relations. The SNA generates a visualized concept map—a two-dimensional image of concept-to-concept relations from which graph-based metrics are derived. This yield a series of 3S parameters that identify features of the three dimensions of the knowledge structure (these parameters are discussed at length in Section 3).

SMART uses 3S parameters to describe a student's knowledge structure and use them to compare a student's mental model to an expert's mental model. The comparison produces feedback information in multiple formats: an expert's summary that demonstrates an ideal size of lexical representation including the number of concepts and relations (i.e., surface dimension); an expert's concept map that explicitly exhibits the target knowledge structure where students can see the structural components missing in their mental model (i.e., structural dimension); and feedback message about what key concepts and propositions should be considered (i.e., semantic dimension). On SMART, students refer to multi-modal feedback information in the 3S dimensions to improve their summaries. The following section describes specific features of diverse feedback on SMART.

2.4 Formative feedback
Elaborated, actionable feedback is a critical component to learning (Hattie & Timperley, 2007). A key feature of technology-enhanced learning environments is that they can quickly deliver effective, personalized feedback that would otherwise be intractable beyond one-on-one or small group tutoring (VanLehn, 2011). In SMART, providing this feedback is a two-step process. The first is the ability to accurately diagnose student's comprehension issues, and the second is to deliver the feedback through multiple modalities.

In technology-enhanced learning environments, feedback is provided by constructing a model of what the learner knows (i.e., a student model or learner model) and comparing it to an expert model. The system iteratively updates the learner model in order to provide continuing support as the learner's knowledge moves towards expertise. SMART is designed in the tradition of technologies such as AKOVIA (Ifenthaler, 2014), ALA-Reader (Clariana et al., 2009), GIKS (K. Kim, 2017, 2018) and Summary Street (Wade-Stein & Kintsch, 2004), which use students' summaries of learning materials to assess their levels of understanding of the learning topic. To provide feedback, the system-generated knowledge structure (i.e., a student model) as well as its constituents are compared to a reference model (i.e., an expert model). The system then produces several similarity measures that indicate the extent to which a student's knowledge representation is more or less like the reference model (see Section 3 for detailed description of these similarity measures).

Using these similarity measures (for more details, M. Kim et al., 2019), SMART provides formative feedback information. Students receive personalized feedback that encourages them to develop a more cohesive summary focused on the key concepts and relations in the text.

Importantly, this feedback is delivered in a variety of ways. First, students can compare their summary to the expert summary used to generate the reference model. In both summaries, the concepts are shown bold, and key concepts are highlighted. Students can also view feedback in the form of a list of key concept and relations that are missing in their summary. In addition to this text-based feedback, students can view the expert's concept map, which includes the key concepts and their relations that are highlighted to separate important from less important ideas (see Figure 1). SMART automatically extracts a structure of key concepts—structured with key concepts and key relations—from the complete expert's concept map. A simplified visualization of the knowledge structure (KS) uses differently coloured nodes and links to indicate key concepts and relations that are present, absent or incorrect in the student's KS. The ability to visualize a KS is likely to help students to better understand the structural and semantic aspects of their understanding (Clariana et al., 2009; Ifenthaler, 2014; K. Kim, 2018). Thus, students have various complementary and multi-modal means of up-taking feedback.


Expert knowledge structure in SMART feedback [Colour figure can be viewed at wileyonlinelibrary.com]
In sum, SMART is founded on the theories of comprehension and learning positing (a) that students develop deep comprehension while constructing a more elaborated and integrated mental models and (b) that students' summaries can be used to infer the quality of their mental models. By comparing the student's mental model to an expert model, we can provide personalized feedback that can improve the quality of their summary.

2.5 The current study
The current study was grounded in the design research tradition that emphasizes improving design products through iterative evaluations (Collins, Joseph, & Bielaczyc, 2004; McKenney & Reeves, 2018). Building on the literature and our previous exploratory studies, we developed a beta version of SMART. The present work was an initial effort to examine SMART's potential to track and describe changes in students' mental models elicited from their summaries. We used both quantitative changes and qualitative analysis to explore how students use this feedback to improve the structure and quality of their summary in a graduate-level online course.

The feedback in SMART draws students' attention to key concepts in the reading and enables them to compare their mental model to an expert model and to compare their summary to an expert summary. We predicted that working with SMART would increase the quality of student's summaries, which would reflect improved mental models of the information in the text in terms of surface, structure and semantic dimensions. We used a mixed-methods approach to examine how interacting with SMART affected the quality of students' summaries. Visual inspection of selected knowledge representations was used to examine how students' knowledge structures shifted over time. This study was designed to address the following:

To what extent does interacting with SMART affect the nature of students' mental models? (3S parameters).
To what extent does writing and revising summaries in SMART help students to construct more expert-like mental models? (Similarity measures).
How do different students' approaches to the summary writing and revising process in SMART affect the structure of their mental model of a text? (Case study).
The findings of the present work could not only demonstrate SMART's effect on students' summary and knowledge improvement but also provide insights into the theoretical, methodological and pedagogical implications that SMART might bring into classes.

3 METHODS
3.1 Research context and participants
Data was collected across two semesters of a graduate-level asynchronous online course titled Foundations of Instructional Design and Technology (IDT). All procedures were performed following the ethical standards of the institutional and the national research committee. There were 21 and 17 students, respectively, for a total of 38 students. As shown in Table 1, students were predominantly female.


As part of normal classwork, each week, students were assigned one or more texts (e.g., textbook chapter, research article). They submitted a summary of the reading, took a quiz about the key ideas of each reading, and then participated in a discussion forum by responding to trigger questions posted by peer moderators and commenting on each other's posts. Students' weekly participation, including summary submission and postings in a discussion forum, was graded per week. In most weeks, students read and submitted their summaries via the university's learning management system. However, on weeks 4, 7 and 11, students completed the reading and summary in SMART. In these 3 weeks, participants read a total of seven texts. In SMART, students were encouraged to make multiple revisions to their summaries. In the present study, we focus on the summaries and revisions that the students wrote in these 3 weeks.

As described in Appendix, data preview showed that some students did not complete all summary assignments (e.g., Student 4 [S4] did not submit a summary of Readings 1 and 2 [R1 and R2] in week 4) and students varied in the number of summary revisions they submitted. In total, the 38 students submitted a total of 357 summaries (initial and revisions) in response to the seven readings. On average, students produced 1.34 (SD = 1.37; Range: 0–12) summaries per reading assignment and an average of 9.39 (SD = 7.20; Range: 0–47) summaries across the seven reading assignments. Twenty-eight students created more than two responses to at least one reading assignment, indicating that a majority of the students took advantage of the opportunity to revise their summary prior to the quiz and discussion.

We further examined these assignments to identify cases in which students produced more than two revisions, resulting in 66 cases with multiple attempts. Among the 66 cases, we identified 19 cases in which the student submitted an identical summary during revision(s). Removing these cases yielded a final set of 47 cases for analysis. In a quantitative phase, we employed inferential statistics to examine how the structure of students' mental models changed in terms of the 3S parameters and student/expert similarity measures from initial draft to final revision. In a qualitative phase, we used a multiple-case analysis approach to provide a richer examination into how students wrote and revised their summaries.

3.2 3S parameters and similarity measures
SMART transforms each written summary into a concept map that reflects the student's mental model of the text. SMART draws on graph theory (Wasserman & Faust, 1994) to generate concept map parameters of the 3S dimensions of a concept map.

3.2.1 3S parameters
Network analysis methods (Coronges, Stacy, & Valente, 2007; Wasserman & Faust, 1994) are used to obtain parameters that describe a concept map. These parameters were identified in previous work (M. Kim, 2015). As shown in Table 2, we aligned the six SMART parameters with the 3S dimensions. For example, the “density of a graph,” which is calculated by dividing the number of observed relations between concepts by the number of all possible pairs of concepts, indicates the extent of cohesion of a concept map, ranging from 0 to 1. Another example is the “average degree,” which indicates the average number of concept-to-concept relations, ranging from 0 to g −1 (g being the total number of concepts). A more complex knowledge structure is likely to show a higher value of average degree. Three parameters (i.e., the number of concepts, the number of relations and density) correspond to the surface dimension. The other three parameters (i.e., average degree, mean distance and diameter) are attributed to the structure of a concept map. Semantic dimension is related to particular concepts and their pair-wise relations used in a concept map.


3.2.2 3S similarity measures
We can evaluate the quality of a student's concept map by comparing it to a reference model (Coronges et al., 2007; Curtis & Davis, 2003; Goldsmith & Kraiger, 1997; M. Kim, 2015). This comparison measures the extent to which a student model is close to a reference model, ranging from 0 (completely dissimilar) to 1 (completely similar).

SMART calculates similarity based on two similarity formulas suggested by M. Kim (2015). Since most parameters are numerical, the comparison used the numerical similarity as follows:
urn:x-wiley:02664909:media:jcal12516:jcal12516-math-0001
where v1 is the value of a student model and v2 is the value of a reference model. In contrast, the conceptual similarity was used for the two similarity measures such as concept matching and propositional matching which indicate the extent to which the paired models share the same elements. The conceptual similarity draws on Tversky's (1977) similarity formula:
urn:x-wiley:02664909:media:jcal12516:jcal12516-math-0002
where A is a student model and B is a reference model. The weighting functions, α and β, were set as 0.7 and 0.3, according to the suggestion that α should be weighted higher than β in an asymmetric relation wherein a student model resembles a reference model (M. Kim, 2015).
In addition to these similarity measures, the current study added two similarity measures, Recall-C and Recall-P measures, which are provided by the SMART system. The Recall-C measure indicates the proportion of fully identical key concepts (i.e., the writer's central ideas; M. Kim et al., 2016, 2019), while the Recall-P measure indicates the extent of the overlapping of the relations of key concepts identified in both the student and the reference model (K. Kim et al., 2019).

M. Kim (2015) conducted Exploratory Factor Analysis (EFA) to identify the associations between similarity measures and 3S dimensions of knowledge structure. Referring to the associations, we have summarized the similarity measures in Table 2.

3.3 Data analysis
This mixed-methods study employed both inferential statistics and a case study methodology. To address the question of whether SMART summary revision changed the dimensional features of students' mental models, we conducted a series of one-way repeated-measures multivariate analysis of variance (MANOVAs) on the 3S parameters and similarity measures. MANOVAs were used as there are multiple indices for each of the three dimensions. To investigate whether SMART summary revision helped students to construct more expert-like mental models, we also conducted a set of parallel MANOVAs with the similarity measures as the dependent variables.

We used G*Power (Faul, Erdfelder, Buchner, & Lang, 2009) to estimate minimum sample size needed to detect a medium effect in our repeated-measures MANOVA. With the effect size = 0.31 (computed from the least partial eta squared value of 0.09 in this study) with power = 0.80, α = .05, number of groups = 1 and number of measurements = 2, G*Power indicated a minimum sample size of 42. Thus, our 47 samples were sufficient to detect effects.

To further inspect how students' mental models changed across revisions, we deployed a case study approach to examine students' different patterns of revisions in response to the SMART feedback (Merriam, 1998; Stake, 1978). We conducted visual inspections of selected students' knowledge representations that were rendered through a Social Network Analysis application (NodeXL; Hansen, Schneiderman, & Smith, 2010). We inspected these students' concept maps for (a) overall shifts in the knowledge structure and (b) how students adapted to key concepts suggested by an expert model.

4 RESULTS
4.1 3S gains
To examine how the parameters of the 3S knowledge structure changed from initial summary to final submission, we conducted a series of repeated-measures MANOVAs, one for each 3S attribute (surface, structure, semantic). As shown in Table 3, students' scores improved from initial to final submission for some aspects of all three dimensions. The multivariate test indicated significant gains for surface structures (Wilks' lambda = 0.81, p = .027) and semantic structures (Wilks' lambda = 0.57, p = .000). The univariate repeated-measures ANOVAs revealed significant changes over time (see Table 3). Notably, the analysis of the semantic dimension revealed increased numbers of Key Concepts and Key Relations. In addition, Number of Concepts and Number of Relations (surface) increased, while Density decreased, indicating a more robust network.



4.2 3S similarity gains
We were also interested in the degree to which students' summaries were more or less similar to expert summaries over time. Thus, we compared 3S similarity measures from initial summary to final submission. Similar to the previous analyses, we conducted a series of MANOVAs. As shown in Table 4, there was a significant gain in surface dimension similarity from initial to final (Wilks' lambda = 0.76, p = .007). The univariate repeated-measures ANOVAs revealed that Number of Concepts and Relations and Density of the student models became similar to the expert model, F (1, 46) = 11.17, 12.18 and 5.03 with p = .002, .001 and .030, respectively.

Students also demonstrated more similarity to experts' semantic structure at final submission (Wilks' lambda = 0.51, p = .000). Univariate ANOVAs revealed significant changes in Concept Matching, Propositional Matching, Balanced Semantic Matching, Recall-C and Recall-P. The findings suggested that the concept and their propositional relations became more similar to experts' over time.

Again, there was no effect of time for structural dimension (Wilks' lambda = 0.84, p = .051). However, the univariate repeated-measures ANOVAs demonstrated that students developed knowledge representations closer to the structure of an expert model in terms of average degree, F(1, 46) = 8.26, p = .006) and mean distance, F(1, 46) = 5.47, p = .024. One possible explanation for this is that the structural similarity was near ceiling, even in the initial summary.

To summarize the quantitative findings, students' mental models improved in terms of surface and semantic dimensions of knowledge. Their mental models also became more expert-like after revision(s).

4.3 Case study
Though MANOVA results demonstrated overall positive trends in the 3S dimensions of students' mental models, changes in the mean values of 3S indices overlook more nuanced changes in each dimension from student to student in SMART. To further explore how summary writing and revising in SMART affected students' mental models of the text, we conducted a case study of students who completed multiple revisions on a single text. The highest frequency of multiple submissions (n = 14) was observed for Reading 2. This text was about constructivist instructional approaches and was 10 pages long (~7,000 words). An initial inspection of these cases revealed that four students made no changes from initial submission to revision. Thus, those cases were excluded, leaving 10 cases from Reading 2 for in-depth review.

Table 5 shows that 7 of the 10 students demonstrated positive gains in 3S parameters and similarity measures from initial to final submission. To better understand how revisions in SMART affected students' summaries, we conducted qualitative analyses for three cases: Student 17 (who demonstrated higher scores in most similarity measures), Student 8 (who uniquely showed negative changes in similarities more than positive) and Student 10 (whose similarity pattern was neither positive or negative). We first describe the nature of the expert reference model and then compare these selected students' concept maps to this reference model over repeated revisions.


4.4 Reference model
Figure 2 depicts the reference model's knowledge representation (i.e., a concept map of the expert summary). Squares indicate key concepts predetermined by the expert. In this text, the concept map consists of several subgroups that show a linear pattern. Each linear group is formed by one or two key concepts. Key concepts tend to relate to each other in a relatively short distance, thereby serve to link subgroups together, and stand towards the centre of the network. Served by key concepts (in terms of the text's gist), the expert model formed a cohesive macrostructure (Kintsch, 1998). The local structures of the summary (i.e., a sentence or a paragraph) appear linear and goal-directed (Hay & Kinchin, 2006). The expert wrote clear propositional relations of concepts inferred by (centring on) a key concept in a part of the text that represents the expert goal-directed mental representations at a micro-level structure (van Dijk & Kintsch, 1983). Key concepts tend to connect to more surrounding concepts and thus receive more activation (Kintsch, 1998). As shown in Figure 2, key concepts worked as cohesion cues (van Dijk & Kintsch, 1983) mapping local and distal constituents in the text (i.e., relationships between linear subgroups and sentences).


4.4.1 Student 17
Figure 3 shows the concept maps generated from Student 17's summaries. Squares are key concepts overlapping with those in the reference model, and dark dots (i.e., nodes) indicate concepts the students might consider significant. The “Initial Response” map shows the concept map generated from the students' initial summary of the text. Although the student lays a good foundation, as indicated by five key concepts, only one key concept (constructivism) is positioned as significant. Four other key concepts, indicated by the lighter coloured squares, are spread apart across the network. Analysis of the “Final Response” concept map demonstrates a notable improvement in the quality of the student's summary. Most key concepts from the reference model are identified and also regarded as relatively important in the knowledge representation. There is an increase in the number of key concepts, and these concepts have moved closer to the centre of the model. Importantly, these key concepts are also interconnected with the surrounding microstructures. These changes make the final response model look similar to the reference model.

4.4.2 Student 8
In contrast to Student 17, Student 8's revisions lead to a less ideal concept map. As depicted in Figure 4, the student's initial model looks different from the reference model, forming some loosely connected linear patterns. As shown in Table 5, the student nearly doubled the length of the summary from 234 to 434 words and included more concepts (increasing from 51 to 96); however, these revisions actually decreased the number of key concepts from 2 to 1. Thus, the Final Response model (Figure 4) is a more complex concept map, but includes many unnecessary concepts and propositional relations that do not align with the reference model. A good summary should identify main ideas, while combining or omitting details (Brown & Day, 1983). Instead, this student added details that do not establish further coherence (Graesser et al., 1994).

4.4.3 Student 10
Student 10 started with a relatively high-quality concept map. As depicted in Figure 5, the initial model embeds three key concepts and forms a shape with high structural similarity measures. The final model shows that the student used all the key concepts recommended by the reference model. Thus, one might predict that this students' summary score would have increased, but it remained the same. Table 5 shows that the students increased their semantic similarity, but that the surface and structural attributes demonstrated either a decrease or no change. The concept map of the final model provides a clearer picture of what happened. As seen in Figure 5, only one key concept is centrally placed in the network. While the student identifies the key concepts, they are located off to the side of the model. This suggests that the student embedded the key concepts provided by the feedback but did not attend to key relations or consider how to more densely connect these ideas. These unsystematic revisions resulted in an increase in the semantic attributes of the model, but not the surface nor structural attributes. Thus, this case provides strong evidence for the need to consider and evaluate multiple dimensions of a learner's knowledge structure.


5 DISCUSSION
The present study demonstrated positive effects of the SMART system on the quality of student summaries of complex course materials. Importantly, SMART's evaluation architecture is grounded in theories of mental model construction (Graesser et al., 1994; Kintsch, 1988; Zwaan & Radvansky, 1998) that emphasize the need to support mental model building at multiple dimensions (i.e., surface, structure and semantic). That is, readers need help not only in identifying concepts but also in making connections across those concepts to develop a coherent representation. The features of SMART assume that the iterative process of summary revision improves students' mental models at these multiple dimensions. The findings of the current study were consistent with those assumptions.

The results indicated that students took advantage of the opportunity to write and rewrite their summaries in the SMART system. These summaries showed increases in all surface and semantic parameters from initial summary to final revision. The effect of revision was less pronounced in the structural dimension. In the structural dimension, only the average degree parameter showed a significant increase. While there were no significant increases in the other structural parameters (i.e., mean distance and diameter), there was no evidence of decrease.

Similarly, we observed improvements in all of the surface and semantic similarities as well as most structural similarities measures (all except for diameter). These findings indicate that students built a more robust and cohesive knowledge structure through revisions facilitated by SMART.

The difference in effects across the different dimensions is not wholly surprising. Contextually, the feedback that SMART provides is more targeted to surface and semantic dimensions than the structural dimension. SMART feedback shows an expected number of lexical components given in an expert's summary and directly proposes a list of specific key concepts and relations that must be posed in their mental models. That is, students are given explicit feedback on what ideas to add to their summaries. Such revisions might be relatively easy to implement (minor additions or deletions). In contrast, the student must use the graphical representation of mental models (concept map) to understand how the structure of his/her mental model differs from the expert's model. The student must then engage in relatively significant revision and restructuring of his/her summary. These more complex revisions may require additional effort and training as compared to the revisions needed to improve surface and semantic parameters.

A more in-depth examination of individual cases revealed that students' revisions were not uniform. The ways in which the shape of students' concept maps changed over time emphasize the importance of considering multiple dimensions of a reader's knowledge structure. Theories of mental models and expertise development describe how students develop qualitatively varying knowledge structure (Chi, 2008; Ericsson, 2006; M. Kim, 2015). For example, students who are only at a novice level of familiarity with the topic might use SMART's feedback to assimilate new concepts from the text and enlarge their initial understanding (an increase in the surface and semantic dimension). Given that the students in this study were in an introductory level course, most of the students are likely at this novice level. Although SMART feedback helps students to develop more expert-like knowledge, it undoubtedly takes more than a few rounds of edits in a single day to develop a rich, interconnected mental model of a field of study. In future work, SMART could be leveraged to examine how students' mental models change over longer periods. One might expect to see greater structural shifts at this larger time scale.

5.1 Implications
These findings have implications for theory, methodology and pedagogy. Theoretically, the findings are consistent with the extant body of work in reading comprehension which emphasizes that a quality mental model does not simply emerge from the inclusion of a large amount of information. Instead, a quality mental model (reflective of deeper comprehension) emerges when information in the mental model is centralized and well-connected. These findings highlight the importance of considering multiple dimensions of language when evaluating reading comprehension.

Methodologically, this study lends support to the viability of 3S parameters and 3S similarity measures as calculated in SMART for characterizing students' mental models and assessing their progress towards an expert model. Scholars can use these measures to investigate expertise development in summary via instruction. Previous studies have reported that students' mental models are observable and measurable by referring to concept maps elicited from students' lexical representations (e.g., summary; Clariana, 2010; M. Kim, 2012, 2013). However, available measures vary depending on studies, and there have been few comprehensive measurement models for reading comprehension. This study supports the value of the 3S dimensional model (M. Kim, 2015) as a means to assess students' understanding in multiple dimensions.

Pedagogically, this study offers promise for technology-enhanced learning environments like SMART. Many readers struggle to deeply comprehend texts like the ones presented in SMART. SMART was easily deployed within the context of an existing curriculum and demonstrated positive effects on students' mental model construction with no training. Through the use of natural language processing and social network analysis, the system is able to provide real-time, personalized feedback at a scale that would be difficult to match in the traditional classroom. For example, SMART was used in this course as a means to prepare for an in-class discussion. If an instructor wanted to go through a similar process of summary writing and revising with his/her class prior to discussion, the student would need to hand in the summaries and then wait for his/her instructor to read, evaluate and provide feedback to each student. This process could take several days if not weeks. With this design, the instructor would be able to cover far less content in the course. It is critical to deploy more supportive technologies for reading to learn, considering the pervasive use of text in classrooms (informational passages, textbooks, e-books, research articles). Indeed, such supports may be even more critical in read-it-on-your-own learning contexts (e.g., self-paced learning in Massive Open Online Courses, flipped classrooms, blended online courses).

An additional pedagogical benefit of SMART is that it provides formative assessment and feedback through various modalities. One possible explanation for the gains in performance, unique to SMART, is that visualization of a student's mental model as compared to an expert's mental model afforded students a deeper understanding of how to connect disparate ideas in the text. In this study, we provided multiple types of feedback, so it is not clear which, if any, modality was most effective or if these gains are due to the feedback presented in tandem. We are currently examining these questions.

5.2 Limitations and future directions
This study served as an initial investigation into the potential for SMART, focusing on the quality of the mental models as evaluated through 3S analyses. Though the results are promising, there is much work to be done. At very least, we intend to replicate the findings with other texts and text types in order to examine generalizability. A logical next step is to examine how the changes in the readers' mental model relate to learning outcomes, such as performance on a comprehension test or on the quality of in-class discussions.

The overall result of the current study showed an improvement in all three knowledge structure dimensions. Indices in the surface and semantic dimensions tended to grow in a similar direction for both their values and similarity measures. In contrast, although most structural indices also demonstrated a positive change through revision, these patterns were less inconsistent than in the other indices. Future studies should further examine how structural indices describe the changes of knowledge structure and their relationships with writing a better summary of the text. Note that the multivariate tests were insignificant, implying that structural indices may describe different characteristics of a mental model structure (Clariana, 2010; Ifenthaler, 2010; K. Kim, 2018; Pirnay-Dummer & Ifenthaler, 2011). Future work should examine how these more subtle changes might relate to other aspects of comprehension, and researchers should explore additional ways of interpreting and articulating structural changes of mental models.

One way of doing this is a cross-validation comparing 3S parameters with other text mining tools designed to assess language at multiple dimensions. For example, Crossley, Kyle and colleagues have developed a suite of NLP tools to examine dimensions such as lexical sophistication (TAALES), syntactic complexity (TAASC; Kyle, 2016), cohesion (TAACO; Crossley, Kyle, & McNamara, 2016; see also Coh-Metrix; McNamara, Graesser, McCarthy, & Cai, 2014) and content overlap (CRAT; Crossley, Kyle, Davenport, & McNamara, 2016). We would predict strong correlations between 3S and indices from these tools, but investigations of similarities and differences may yield important practical and theoretical insights, especially into the relationships between mental model structure and text structure (e.g., complexity and cohesion).

A second line of future work would be to investigate more mental model changes over multiple iterations of revision and continued practice with SMART. In this study, we examined how students' mental models changed from initial submission to final submission. Future work should look more deeply at changes from revision to revision and to explore how the students use the feedback to make these revisions.

The SMART assessment centres on the extent to which learners internalized the information from reading material. Although the 3S indices of mental models reflect the text structure (e.g., how cohesively a student writes a summary) and lexical diversity of a summary (Crossley, Kyle, Davenport, & McNamara, 2016; Li et al., 2018), SMART's feedback tends to focus on content coverage in terms of key ideas and their relations. The current SMART approach is beneficial for “writing to learn,” rather than “learning to write.” Further work could explore the effects of embedding summary writing instruction (Friend, 2001; Stevens et al., 2019) and composition feedback (Wade-Stein & Kintsch, 2004) in addition to SMART'S existing content-driven feedback.

SMART feedback may work better for reading materials that address clearly defined and structured normative knowledge (e.g., topics in the science domain). The SMART assessment process involves an expert concept map elicited from an exemplary summary. Accordingly, SMART works when an expert summary is established. Thanks to the SMART assessment algorithms, the technology is applied to writing a summary of expository texts that convey accurate and structured information to help readers learn (Meyer, Halliday, & Hasan, 1987; Craft & Ideas, 2010).

We admit that there is no one correct concept map of these texts. Expert readers can write a variety of summaries from the text. However, expertise studies have shown that experts tend to share a common knowledge base similar to the writer's intention when the summary assignment emphasizes identifying the source text's information (Graesser et al., 1994; Kintsch, 1988; Spector, 2008). Accordingly, an expert summary can represent a more optimal structure of a concept map guided by the source text's content than a novice reader.

Lastly, the focus on this study was to demonstrate SMART's potential to describe students' mental model changes while revising summaries in the context of an authentic classroom. Future design research will be needed to better understand system usability, utility and its feasibility of implementation and scalability. To do so, a systematic investigation can consider learner/user experience and perceptions in addition to efficacy measures (Sedrakyan et al., 2020). For example, the Design Implementation Framework (McCarthy, Watanabe, Dai, & McNamara, 2020; Stone et al., 2018) encourages instructional and educational technology designers to engage in multiple iterative cycles of design and implementation that consider the experiences and outcomes of multiple end users (e.g., teachers, students, researchers). Thus, this work must also consider effects across various individual differences such as gender, prior knowledge and previous technology experience. For example, our findings in this study were drawn from a sample that consisted of mostly female. Previous studies suggest gender differences in students' writing conventions and style (Gibb, Fergusson, & Horwood, 2008; Peterson, 2000). These differences may affect the way information is represented in their concept maps. Research suggests complex interactions between students' reading skill, prior knowledge and features of the to-be-read texts (O'Reilly & McNamara, 2007). Thus, it is important to consider that different students may face different challenges in reading and writing about different texts such that the quality, amount and timing of feedback are unlikely to be one-size-fits-all. More comprehensive and systematic evaluation studies will be able to explain how students with varying knowledge, experiences and interests interact with the SMART tool and thereby help integrate the technology in instructional practice and advance design knowledge about technology-supported reading comprehension.

5.3 Closing comment
This design study explored a pilot version of SMART technology, focusing on the tool's utility value to help students write a quality summary of complex reading material. The findings of both quantitative and qualitative analyses demonstrated the potential of SMART to support students' mental model building. SMART's multi-modal feedback led to improved quality of students' mental models at the surface, structure and semantic dimensions. SMART's multi-dimensional diagnosis model successfully puts theories of mental models in practice in the context of reading comprehension. Its merits for formative assessment and feedback at a scale demonstrate the value of using technology-enhanced learning environments to support various instructional settings that require students' adequate understanding of the reading materials. Such findings lay the groundwork for technology-supported mental model development.